{"parameters": {"input_shape": [128, 84, 42, 42], "filter_shape": [84, 1, 3, 3], "output_shape": [128, 84, 42, 42], "window_movement_strides": [1, 1], "padding_below_diff": [1, 1], "window_dilation_strides": [1, 1]}, "op_type": "DepthwiseConv2dNative", "tvm_func_name": "roller_DepthwiseConv2dNative__128_84_42_42___3_3_84_1___128_84_42_42_", "code": "extern \"C\" __global__ void roller_DepthwiseConv2dNative__128_84_42_42___3_3_84_1___128_84_42_42_(float* __restrict__ data, float* __restrict__ kernel, float* __restrict__ compute) {\n  float DepthwiseConv2d_local[2];\n  __shared__ float PaddedInput_shared[264];\n  __shared__ float compute_shared[9];\n  float PaddedInput_shared_local[2];\n  float compute_shared_local[1];\n  DepthwiseConv2d_local[(0)] = 0.000000e+00f;\n  DepthwiseConv2d_local[(1)] = 0.000000e+00f;\n  PaddedInput_shared[(((int)threadIdx.x))] = ((((1 <= (((((int)blockIdx.x) % 11) * 4) + (((int)threadIdx.x) / 44))) && (1 <= (((int)threadIdx.x) % 44))) && ((((int)threadIdx.x) % 44) < 43)) ? data[(((((((((int)blockIdx.x) / 11) * 1764) + ((((int)blockIdx.x) % 11) * 168)) + ((((int)threadIdx.x) / 44) * 42)) + (((int)threadIdx.x) % 44)) - 43))] : 0.000000e+00f);\n  if ((((((int)blockIdx.x) % 11) * 4) + ((((int)threadIdx.x) + 96) / 44)) < 44) {\n    PaddedInput_shared[((((int)threadIdx.x) + 96))] = (((((((((int)blockIdx.x) % 11) * 4) + ((((int)threadIdx.x) + 96) / 44)) < 43) && (1 <= ((((int)threadIdx.x) + 8) % 44))) && (((((int)threadIdx.x) + 8) % 44) < 43)) ? data[(((((((((int)blockIdx.x) / 11) * 1764) + ((((int)blockIdx.x) % 11) * 168)) + (((((int)threadIdx.x) + 96) / 44) * 42)) + ((((int)threadIdx.x) + 8) % 44)) - 43))] : 0.000000e+00f);\n  }\n  if (((int)threadIdx.x) < 72) {\n    if ((((((int)blockIdx.x) % 11) * 4) + ((((int)threadIdx.x) + 192) / 44)) < 44) {\n      PaddedInput_shared[((((int)threadIdx.x) + 192))] = (((((((((int)blockIdx.x) % 11) * 4) + ((((int)threadIdx.x) + 192) / 44)) < 43) && (1 <= ((((int)threadIdx.x) + 16) % 44))) && (((((int)threadIdx.x) + 16) % 44) < 43)) ? data[(((((((((int)blockIdx.x) / 11) * 1764) + ((((int)blockIdx.x) % 11) * 168)) + (((((int)threadIdx.x) + 192) / 44) * 42)) + ((((int)threadIdx.x) + 16) % 44)) - 43))] : 0.000000e+00f);\n    }\n  }\n  if (((int)threadIdx.x) < 9) {\n    compute_shared[(((int)threadIdx.x))] = kernel[(((((((int)blockIdx.x) % 924) / 11) * 9) + ((int)threadIdx.x)))];\n  }\n  __syncthreads();\n  for (int k_inner_outer = 0; k_inner_outer < 9; ++k_inner_outer) {\n    if (((((((int)blockIdx.x) % 11) * 4) + (((int)threadIdx.x) / 24)) + (k_inner_outer / 3)) < 44) {\n      PaddedInput_shared_local[(0)] = PaddedInput_shared[((((((((int)threadIdx.x) / 24) * 44) + ((k_inner_outer / 3) * 44)) + (((int)threadIdx.x) % 24)) + (k_inner_outer % 3)))];\n      if (((((int)threadIdx.x) % 24) + (k_inner_outer % 3)) < 20) {\n        PaddedInput_shared_local[(1)] = PaddedInput_shared[(((((((((int)threadIdx.x) / 24) * 44) + ((k_inner_outer / 3) * 44)) + (((int)threadIdx.x) % 24)) + (k_inner_outer % 3)) + 24))];\n      }\n    }\n    compute_shared_local[(0)] = compute_shared[(k_inner_outer)];\n    if ((((((int)blockIdx.x) % 11) * 4) + (((int)threadIdx.x) / 24)) < 42) {\n      DepthwiseConv2d_local[(0)] = (DepthwiseConv2d_local[(0)] + (PaddedInput_shared_local[(0)] * compute_shared_local[(0)]));\n      if ((((int)threadIdx.x) % 24) < 18) {\n        DepthwiseConv2d_local[(1)] = (DepthwiseConv2d_local[(1)] + (PaddedInput_shared_local[(1)] * compute_shared_local[(0)]));\n      }\n    }\n  }\n  if ((((((int)blockIdx.x) % 11) * 4) + (((int)threadIdx.x) / 24)) < 42) {\n    compute[((((((((int)blockIdx.x) / 11) * 1764) + ((((int)blockIdx.x) % 11) * 168)) + ((((int)threadIdx.x) / 24) * 42)) + (((int)threadIdx.x) % 24)))] = DepthwiseConv2d_local[(0)];\n    if ((((int)threadIdx.x) % 24) < 18) {\n      compute[(((((((((int)blockIdx.x) / 11) * 1764) + ((((int)blockIdx.x) % 11) * 168)) + ((((int)threadIdx.x) / 24) * 42)) + (((int)threadIdx.x) % 24)) + 24))] = DepthwiseConv2d_local[(1)];\n    }\n  }\n}\n", "gridDim": [118272, 1, 1], "blockDim": [96, 1, 1]}