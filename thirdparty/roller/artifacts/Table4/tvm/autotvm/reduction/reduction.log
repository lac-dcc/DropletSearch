(128, 512, 1024) axis: 2
[128, 512]
Time cost of this operator: 0.0003389147
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [128, 512], []),
             data: Buffer(data_2: Pointer(float32), float32, [128, 512, 1024], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2048;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf[0] = 0f32
    for (k2.outer: int32, 0, 32) {
      data_red.rf[0] = ((float32*)data_red.rf[0] + (float32*)data_2[((((blockIdx.x*32768) + (threadIdx.y*1024)) + (k2.outer*32)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, (float32*)data_red.rf[0], True, reduce_temp0, threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red_2[((blockIdx.x*32) + threadIdx.y)] = (float32*)reduce_temp0[0]
    }
  }
}


; ModuleID = 'TVMAMDGPUModule'
source_filename = "TVMAMDGPUModule"
target datalayout = "e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-ni:7"
target triple = "amdgcn-amd-amdhsa-hcc"

@shmem = private addrspace(3) global [1024 x float], align 16

; Function Attrs: nounwind
define dllexport amdgpu_kernel void @default_function_kernel0(float addrspace(1)* noalias nocapture readonly %0, float addrspace(1)* noalias nocapture %1) local_unnamed_addr #0 {
entry:
  %2 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %3 = tail call i32 @llvm.amdgcn.workitem.id.y()
  %4 = tail call i32 @llvm.amdgcn.workitem.id.x()
  %5 = shl i32 %3, 10
  %6 = shl i32 %2, 15
  %7 = add i32 %5, %4
  %8 = add i32 %7, %6
  %9 = sext i32 %8 to i64
  %10 = getelementptr inbounds float, float addrspace(1)* %0, i64 %9
  %11 = load float, float addrspace(1)* %10, align 4, !tbaa !4
  %12 = fadd float %11, 0.000000e+00
  %13 = add i32 %8, 32
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds float, float addrspace(1)* %0, i64 %14
  %16 = load float, float addrspace(1)* %15, align 4, !tbaa !4
  %17 = fadd float %12, %16
  %18 = add i32 %8, 64
  %19 = sext i32 %18 to i64
  %20 = getelementptr inbounds float, float addrspace(1)* %0, i64 %19
  %21 = load float, float addrspace(1)* %20, align 4, !tbaa !4
  %22 = fadd float %17, %21
  %23 = add i32 %8, 96
  %24 = sext i32 %23 to i64
  %25 = getelementptr inbounds float, float addrspace(1)* %0, i64 %24
  %26 = load float, float addrspace(1)* %25, align 4, !tbaa !4
  %27 = fadd float %22, %26
  %28 = add i32 %8, 128
  %29 = sext i32 %28 to i64
  %30 = getelementptr inbounds float, float addrspace(1)* %0, i64 %29
  %31 = load float, float addrspace(1)* %30, align 4, !tbaa !4
  %32 = fadd float %27, %31
  %33 = add i32 %8, 160
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds float, float addrspace(1)* %0, i64 %34
  %36 = load float, float addrspace(1)* %35, align 4, !tbaa !4
  %37 = fadd float %32, %36
  %38 = add i32 %8, 192
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds float, float addrspace(1)* %0, i64 %39
  %41 = load float, float addrspace(1)* %40, align 4, !tbaa !4
  %42 = fadd float %37, %41
  %43 = add i32 %8, 224
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds float, float addrspace(1)* %0, i64 %44
  %46 = load float, float addrspace(1)* %45, align 4, !tbaa !4
  %47 = fadd float %42, %46
  %48 = add i32 %8, 256
  %49 = sext i32 %48 to i64
  %50 = getelementptr inbounds float, float addrspace(1)* %0, i64 %49
  %51 = load float, float addrspace(1)* %50, align 4, !tbaa !4
  %52 = fadd float %47, %51
  %53 = add i32 %8, 288
  %54 = sext i32 %53 to i64
  %55 = getelementptr inbounds float, float addrspace(1)* %0, i64 %54
  %56 = load float, float addrspace(1)* %55, align 4, !tbaa !4
  %57 = fadd float %52, %56
  %58 = add i32 %8, 320
  %59 = sext i32 %58 to i64
  %60 = getelementptr inbounds float, float addrspace(1)* %0, i64 %59
  %61 = load float, float addrspace(1)* %60, align 4, !tbaa !4
  %62 = fadd float %57, %61
  %63 = add i32 %8, 352
  %64 = sext i32 %63 to i64
  %65 = getelementptr inbounds float, float addrspace(1)* %0, i64 %64
  %66 = load float, float addrspace(1)* %65, align 4, !tbaa !4
  %67 = fadd float %62, %66
  %68 = add i32 %8, 384
  %69 = sext i32 %68 to i64
  %70 = getelementptr inbounds float, float addrspace(1)* %0, i64 %69
  %71 = load float, float addrspace(1)* %70, align 4, !tbaa !4
  %72 = fadd float %67, %71
  %73 = add i32 %8, 416
  %74 = sext i32 %73 to i64
  %75 = getelementptr inbounds float, float addrspace(1)* %0, i64 %74
  %76 = load float, float addrspace(1)* %75, align 4, !tbaa !4
  %77 = fadd float %72, %76
  %78 = add i32 %8, 448
  %79 = sext i32 %78 to i64
  %80 = getelementptr inbounds float, float addrspace(1)* %0, i64 %79
  %81 = load float, float addrspace(1)* %80, align 4, !tbaa !4
  %82 = fadd float %77, %81
  %83 = add i32 %8, 480
  %84 = sext i32 %83 to i64
  %85 = getelementptr inbounds float, float addrspace(1)* %0, i64 %84
  %86 = load float, float addrspace(1)* %85, align 4, !tbaa !4
  %87 = fadd float %82, %86
  %88 = add i32 %8, 512
  %89 = sext i32 %88 to i64
  %90 = getelementptr inbounds float, float addrspace(1)* %0, i64 %89
  %91 = load float, float addrspace(1)* %90, align 4, !tbaa !4
  %92 = fadd float %87, %91
  %93 = add i32 %8, 544
  %94 = sext i32 %93 to i64
  %95 = getelementptr inbounds float, float addrspace(1)* %0, i64 %94
  %96 = load float, float addrspace(1)* %95, align 4, !tbaa !4
  %97 = fadd float %92, %96
  %98 = add i32 %8, 576
  %99 = sext i32 %98 to i64
  %100 = getelementptr inbounds float, float addrspace(1)* %0, i64 %99
  %101 = load float, float addrspace(1)* %100, align 4, !tbaa !4
  %102 = fadd float %97, %101
  %103 = add i32 %8, 608
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds float, float addrspace(1)* %0, i64 %104
  %106 = load float, float addrspace(1)* %105, align 4, !tbaa !4
  %107 = fadd float %102, %106
  %108 = add i32 %8, 640
  %109 = sext i32 %108 to i64
  %110 = getelementptr inbounds float, float addrspace(1)* %0, i64 %109
  %111 = load float, float addrspace(1)* %110, align 4, !tbaa !4
  %112 = fadd float %107, %111
  %113 = add i32 %8, 672
  %114 = sext i32 %113 to i64
  %115 = getelementptr inbounds float, float addrspace(1)* %0, i64 %114
  %116 = load float, float addrspace(1)* %115, align 4, !tbaa !4
  %117 = fadd float %112, %116
  %118 = add i32 %8, 704
  %119 = sext i32 %118 to i64
  %120 = getelementptr inbounds float, float addrspace(1)* %0, i64 %119
  %121 = load float, float addrspace(1)* %120, align 4, !tbaa !4
  %122 = fadd float %117, %121
  %123 = add i32 %8, 736
  %124 = sext i32 %123 to i64
  %125 = getelementptr inbounds float, float addrspace(1)* %0, i64 %124
  %126 = load float, float addrspace(1)* %125, align 4, !tbaa !4
  %127 = fadd float %122, %126
  %128 = add i32 %8, 768
  %129 = sext i32 %128 to i64
  %130 = getelementptr inbounds float, float addrspace(1)* %0, i64 %129
  %131 = load float, float addrspace(1)* %130, align 4, !tbaa !4
  %132 = fadd float %127, %131
  %133 = add i32 %8, 800
  %134 = sext i32 %133 to i64
  %135 = getelementptr inbounds float, float addrspace(1)* %0, i64 %134
  %136 = load float, float addrspace(1)* %135, align 4, !tbaa !4
  %137 = fadd float %132, %136
  %138 = add i32 %8, 832
  %139 = sext i32 %138 to i64
  %140 = getelementptr inbounds float, float addrspace(1)* %0, i64 %139
  %141 = load float, float addrspace(1)* %140, align 4, !tbaa !4
  %142 = fadd float %137, %141
  %143 = add i32 %8, 864
  %144 = sext i32 %143 to i64
  %145 = getelementptr inbounds float, float addrspace(1)* %0, i64 %144
  %146 = load float, float addrspace(1)* %145, align 4, !tbaa !4
  %147 = fadd float %142, %146
  %148 = add i32 %8, 896
  %149 = sext i32 %148 to i64
  %150 = getelementptr inbounds float, float addrspace(1)* %0, i64 %149
  %151 = load float, float addrspace(1)* %150, align 4, !tbaa !4
  %152 = fadd float %147, %151
  %153 = add i32 %8, 928
  %154 = sext i32 %153 to i64
  %155 = getelementptr inbounds float, float addrspace(1)* %0, i64 %154
  %156 = load float, float addrspace(1)* %155, align 4, !tbaa !4
  %157 = fadd float %152, %156
  %158 = add i32 %8, 960
  %159 = sext i32 %158 to i64
  %160 = getelementptr inbounds float, float addrspace(1)* %0, i64 %159
  %161 = load float, float addrspace(1)* %160, align 4, !tbaa !4
  %162 = fadd float %157, %161
  %163 = add i32 %8, 992
  %164 = sext i32 %163 to i64
  %165 = getelementptr inbounds float, float addrspace(1)* %0, i64 %164
  %166 = load float, float addrspace(1)* %165, align 4, !tbaa !4
  %167 = fadd float %162, %166
  tail call void @llvm.amdgcn.s.barrier()
  %168 = shl nsw i32 %3, 5
  %169 = add nsw i32 %168, %4
  %170 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %169
  store volatile float %167, float addrspace(3)* %170, align 4, !tbaa !8
  tail call void @llvm.amdgcn.s.barrier()
  %171 = icmp slt i32 %4, 16
  br i1 %171, label %if_then, label %if_end, !prof !11

if_then:                                          ; preds = %entry
  %172 = add nsw i32 %169, 16
  %173 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %172
  %174 = load volatile float, float addrspace(3)* %173, align 4, !tbaa !8
  %175 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %176 = fadd float %174, %175
  store volatile float %176, float addrspace(3)* %170, align 4, !tbaa !8
  %177 = add nsw i32 %169, 8
  %178 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %177
  %179 = load volatile float, float addrspace(3)* %178, align 4, !tbaa !8
  %180 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %181 = fadd float %179, %180
  store volatile float %181, float addrspace(3)* %170, align 4, !tbaa !8
  %182 = add nsw i32 %169, 4
  %183 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %182
  %184 = load volatile float, float addrspace(3)* %183, align 4, !tbaa !8
  %185 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %186 = fadd float %184, %185
  store volatile float %186, float addrspace(3)* %170, align 4, !tbaa !8
  %187 = add nsw i32 %169, 2
  %188 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %187
  %189 = load volatile float, float addrspace(3)* %188, align 4, !tbaa !8
  %190 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %191 = fadd float %189, %190
  store volatile float %191, float addrspace(3)* %170, align 4, !tbaa !8
  %192 = add nsw i32 %169, 1
  %193 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %192
  %194 = load volatile float, float addrspace(3)* %193, align 4, !tbaa !8
  %195 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %196 = fadd float %194, %195
  store volatile float %196, float addrspace(3)* %170, align 4, !tbaa !8
  br label %if_end

if_end:                                           ; preds = %if_then, %entry
  tail call void @llvm.amdgcn.s.barrier()
  %197 = icmp eq i32 %4, 0
  br i1 %197, label %if_then1, label %if_end2, !prof !11

if_then1:                                         ; preds = %if_end
  %198 = shl i32 %2, 5
  %199 = add nsw i32 %198, %3
  %200 = sext i32 %199 to i64
  %201 = getelementptr inbounds float, float addrspace(1)* %1, i64 %200
  %202 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %168
  %203 = load volatile float, float addrspace(3)* %202, align 16, !tbaa !8
  store float %203, float addrspace(1)* %201, align 4, !tbaa !12
  br label %if_end2

if_end2:                                          ; preds = %if_then1, %if_end
  ret void
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workgroup.id.x() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.y() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.x() #1

; Function Attrs: convergent nounwind
declare void @llvm.amdgcn.s.barrier() #2

attributes #0 = { nounwind "amdgpu-flat-work-group-size"="1,1024" "no-nans-fp-math"="true" }
attributes #1 = { nounwind readnone speculatable }
attributes #2 = { convergent nounwind }

!llvm.module.flags = !{!0, !1}
!opencl.ocl.version = !{!2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 1}
!2 = !{i32 2, i32 0}
!3 = !{!"clang version 12.0.0 (/src/external/llvm-project/clang 1100ebe275a9dcc79a9abbda902b6f10738f2f5d)"}
!4 = !{!5, !5, i64 0}
!5 = !{!"float", !6, i64 0}
!6 = !{!"0x20ae240", !7, i64 0}
!7 = !{!"tvm-tbaa"}
!8 = !{!9, !9, i64 0}
!9 = !{!"float", !10, i64 0}
!10 = !{!"0x20c6640", !7, i64 0}
!11 = !{!"branch_weights", i32 1048576, i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"0x20acde0", !7, i64 0}

(65536, 1024) axis: 1
[65536]
Time cost of this operator: 0.0003386954
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [65536], []),
             data: Buffer(data_2: Pointer(float32), float32, [65536, 1024], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2048;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf[0] = 0f32
    for (k1.outer: int32, 0, 32) {
      data_red.rf[0] = ((float32*)data_red.rf[0] + (float32*)data_2[((((blockIdx.x*32768) + (threadIdx.y*1024)) + (k1.outer*32)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, (float32*)data_red.rf[0], True, reduce_temp0, threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red_2[((blockIdx.x*32) + threadIdx.y)] = (float32*)reduce_temp0[0]
    }
  }
}


; ModuleID = 'TVMAMDGPUModule'
source_filename = "TVMAMDGPUModule"
target datalayout = "e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-ni:7"
target triple = "amdgcn-amd-amdhsa-hcc"

@shmem = private addrspace(3) global [1024 x float], align 16

; Function Attrs: nounwind
define dllexport amdgpu_kernel void @default_function_kernel0(float addrspace(1)* noalias nocapture readonly %0, float addrspace(1)* noalias nocapture %1) local_unnamed_addr #0 {
entry:
  %2 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %3 = tail call i32 @llvm.amdgcn.workitem.id.y()
  %4 = tail call i32 @llvm.amdgcn.workitem.id.x()
  %5 = shl i32 %3, 10
  %6 = shl i32 %2, 15
  %7 = add i32 %5, %4
  %8 = add i32 %7, %6
  %9 = sext i32 %8 to i64
  %10 = getelementptr inbounds float, float addrspace(1)* %0, i64 %9
  %11 = load float, float addrspace(1)* %10, align 4, !tbaa !4
  %12 = fadd float %11, 0.000000e+00
  %13 = add i32 %8, 32
  %14 = sext i32 %13 to i64
  %15 = getelementptr inbounds float, float addrspace(1)* %0, i64 %14
  %16 = load float, float addrspace(1)* %15, align 4, !tbaa !4
  %17 = fadd float %12, %16
  %18 = add i32 %8, 64
  %19 = sext i32 %18 to i64
  %20 = getelementptr inbounds float, float addrspace(1)* %0, i64 %19
  %21 = load float, float addrspace(1)* %20, align 4, !tbaa !4
  %22 = fadd float %17, %21
  %23 = add i32 %8, 96
  %24 = sext i32 %23 to i64
  %25 = getelementptr inbounds float, float addrspace(1)* %0, i64 %24
  %26 = load float, float addrspace(1)* %25, align 4, !tbaa !4
  %27 = fadd float %22, %26
  %28 = add i32 %8, 128
  %29 = sext i32 %28 to i64
  %30 = getelementptr inbounds float, float addrspace(1)* %0, i64 %29
  %31 = load float, float addrspace(1)* %30, align 4, !tbaa !4
  %32 = fadd float %27, %31
  %33 = add i32 %8, 160
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds float, float addrspace(1)* %0, i64 %34
  %36 = load float, float addrspace(1)* %35, align 4, !tbaa !4
  %37 = fadd float %32, %36
  %38 = add i32 %8, 192
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds float, float addrspace(1)* %0, i64 %39
  %41 = load float, float addrspace(1)* %40, align 4, !tbaa !4
  %42 = fadd float %37, %41
  %43 = add i32 %8, 224
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds float, float addrspace(1)* %0, i64 %44
  %46 = load float, float addrspace(1)* %45, align 4, !tbaa !4
  %47 = fadd float %42, %46
  %48 = add i32 %8, 256
  %49 = sext i32 %48 to i64
  %50 = getelementptr inbounds float, float addrspace(1)* %0, i64 %49
  %51 = load float, float addrspace(1)* %50, align 4, !tbaa !4
  %52 = fadd float %47, %51
  %53 = add i32 %8, 288
  %54 = sext i32 %53 to i64
  %55 = getelementptr inbounds float, float addrspace(1)* %0, i64 %54
  %56 = load float, float addrspace(1)* %55, align 4, !tbaa !4
  %57 = fadd float %52, %56
  %58 = add i32 %8, 320
  %59 = sext i32 %58 to i64
  %60 = getelementptr inbounds float, float addrspace(1)* %0, i64 %59
  %61 = load float, float addrspace(1)* %60, align 4, !tbaa !4
  %62 = fadd float %57, %61
  %63 = add i32 %8, 352
  %64 = sext i32 %63 to i64
  %65 = getelementptr inbounds float, float addrspace(1)* %0, i64 %64
  %66 = load float, float addrspace(1)* %65, align 4, !tbaa !4
  %67 = fadd float %62, %66
  %68 = add i32 %8, 384
  %69 = sext i32 %68 to i64
  %70 = getelementptr inbounds float, float addrspace(1)* %0, i64 %69
  %71 = load float, float addrspace(1)* %70, align 4, !tbaa !4
  %72 = fadd float %67, %71
  %73 = add i32 %8, 416
  %74 = sext i32 %73 to i64
  %75 = getelementptr inbounds float, float addrspace(1)* %0, i64 %74
  %76 = load float, float addrspace(1)* %75, align 4, !tbaa !4
  %77 = fadd float %72, %76
  %78 = add i32 %8, 448
  %79 = sext i32 %78 to i64
  %80 = getelementptr inbounds float, float addrspace(1)* %0, i64 %79
  %81 = load float, float addrspace(1)* %80, align 4, !tbaa !4
  %82 = fadd float %77, %81
  %83 = add i32 %8, 480
  %84 = sext i32 %83 to i64
  %85 = getelementptr inbounds float, float addrspace(1)* %0, i64 %84
  %86 = load float, float addrspace(1)* %85, align 4, !tbaa !4
  %87 = fadd float %82, %86
  %88 = add i32 %8, 512
  %89 = sext i32 %88 to i64
  %90 = getelementptr inbounds float, float addrspace(1)* %0, i64 %89
  %91 = load float, float addrspace(1)* %90, align 4, !tbaa !4
  %92 = fadd float %87, %91
  %93 = add i32 %8, 544
  %94 = sext i32 %93 to i64
  %95 = getelementptr inbounds float, float addrspace(1)* %0, i64 %94
  %96 = load float, float addrspace(1)* %95, align 4, !tbaa !4
  %97 = fadd float %92, %96
  %98 = add i32 %8, 576
  %99 = sext i32 %98 to i64
  %100 = getelementptr inbounds float, float addrspace(1)* %0, i64 %99
  %101 = load float, float addrspace(1)* %100, align 4, !tbaa !4
  %102 = fadd float %97, %101
  %103 = add i32 %8, 608
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds float, float addrspace(1)* %0, i64 %104
  %106 = load float, float addrspace(1)* %105, align 4, !tbaa !4
  %107 = fadd float %102, %106
  %108 = add i32 %8, 640
  %109 = sext i32 %108 to i64
  %110 = getelementptr inbounds float, float addrspace(1)* %0, i64 %109
  %111 = load float, float addrspace(1)* %110, align 4, !tbaa !4
  %112 = fadd float %107, %111
  %113 = add i32 %8, 672
  %114 = sext i32 %113 to i64
  %115 = getelementptr inbounds float, float addrspace(1)* %0, i64 %114
  %116 = load float, float addrspace(1)* %115, align 4, !tbaa !4
  %117 = fadd float %112, %116
  %118 = add i32 %8, 704
  %119 = sext i32 %118 to i64
  %120 = getelementptr inbounds float, float addrspace(1)* %0, i64 %119
  %121 = load float, float addrspace(1)* %120, align 4, !tbaa !4
  %122 = fadd float %117, %121
  %123 = add i32 %8, 736
  %124 = sext i32 %123 to i64
  %125 = getelementptr inbounds float, float addrspace(1)* %0, i64 %124
  %126 = load float, float addrspace(1)* %125, align 4, !tbaa !4
  %127 = fadd float %122, %126
  %128 = add i32 %8, 768
  %129 = sext i32 %128 to i64
  %130 = getelementptr inbounds float, float addrspace(1)* %0, i64 %129
  %131 = load float, float addrspace(1)* %130, align 4, !tbaa !4
  %132 = fadd float %127, %131
  %133 = add i32 %8, 800
  %134 = sext i32 %133 to i64
  %135 = getelementptr inbounds float, float addrspace(1)* %0, i64 %134
  %136 = load float, float addrspace(1)* %135, align 4, !tbaa !4
  %137 = fadd float %132, %136
  %138 = add i32 %8, 832
  %139 = sext i32 %138 to i64
  %140 = getelementptr inbounds float, float addrspace(1)* %0, i64 %139
  %141 = load float, float addrspace(1)* %140, align 4, !tbaa !4
  %142 = fadd float %137, %141
  %143 = add i32 %8, 864
  %144 = sext i32 %143 to i64
  %145 = getelementptr inbounds float, float addrspace(1)* %0, i64 %144
  %146 = load float, float addrspace(1)* %145, align 4, !tbaa !4
  %147 = fadd float %142, %146
  %148 = add i32 %8, 896
  %149 = sext i32 %148 to i64
  %150 = getelementptr inbounds float, float addrspace(1)* %0, i64 %149
  %151 = load float, float addrspace(1)* %150, align 4, !tbaa !4
  %152 = fadd float %147, %151
  %153 = add i32 %8, 928
  %154 = sext i32 %153 to i64
  %155 = getelementptr inbounds float, float addrspace(1)* %0, i64 %154
  %156 = load float, float addrspace(1)* %155, align 4, !tbaa !4
  %157 = fadd float %152, %156
  %158 = add i32 %8, 960
  %159 = sext i32 %158 to i64
  %160 = getelementptr inbounds float, float addrspace(1)* %0, i64 %159
  %161 = load float, float addrspace(1)* %160, align 4, !tbaa !4
  %162 = fadd float %157, %161
  %163 = add i32 %8, 992
  %164 = sext i32 %163 to i64
  %165 = getelementptr inbounds float, float addrspace(1)* %0, i64 %164
  %166 = load float, float addrspace(1)* %165, align 4, !tbaa !4
  %167 = fadd float %162, %166
  tail call void @llvm.amdgcn.s.barrier()
  %168 = shl nsw i32 %3, 5
  %169 = add nsw i32 %168, %4
  %170 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %169
  store volatile float %167, float addrspace(3)* %170, align 4, !tbaa !8
  tail call void @llvm.amdgcn.s.barrier()
  %171 = icmp slt i32 %4, 16
  br i1 %171, label %if_then, label %if_end, !prof !11

if_then:                                          ; preds = %entry
  %172 = add nsw i32 %169, 16
  %173 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %172
  %174 = load volatile float, float addrspace(3)* %173, align 4, !tbaa !8
  %175 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %176 = fadd float %174, %175
  store volatile float %176, float addrspace(3)* %170, align 4, !tbaa !8
  %177 = add nsw i32 %169, 8
  %178 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %177
  %179 = load volatile float, float addrspace(3)* %178, align 4, !tbaa !8
  %180 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %181 = fadd float %179, %180
  store volatile float %181, float addrspace(3)* %170, align 4, !tbaa !8
  %182 = add nsw i32 %169, 4
  %183 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %182
  %184 = load volatile float, float addrspace(3)* %183, align 4, !tbaa !8
  %185 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %186 = fadd float %184, %185
  store volatile float %186, float addrspace(3)* %170, align 4, !tbaa !8
  %187 = add nsw i32 %169, 2
  %188 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %187
  %189 = load volatile float, float addrspace(3)* %188, align 4, !tbaa !8
  %190 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %191 = fadd float %189, %190
  store volatile float %191, float addrspace(3)* %170, align 4, !tbaa !8
  %192 = add nsw i32 %169, 1
  %193 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %192
  %194 = load volatile float, float addrspace(3)* %193, align 4, !tbaa !8
  %195 = load volatile float, float addrspace(3)* %170, align 4, !tbaa !8
  %196 = fadd float %194, %195
  store volatile float %196, float addrspace(3)* %170, align 4, !tbaa !8
  br label %if_end

if_end:                                           ; preds = %if_then, %entry
  tail call void @llvm.amdgcn.s.barrier()
  %197 = icmp eq i32 %4, 0
  br i1 %197, label %if_then1, label %if_end2, !prof !11

if_then1:                                         ; preds = %if_end
  %198 = shl i32 %2, 5
  %199 = add nsw i32 %198, %3
  %200 = sext i32 %199 to i64
  %201 = getelementptr inbounds float, float addrspace(1)* %1, i64 %200
  %202 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %168
  %203 = load volatile float, float addrspace(3)* %202, align 16, !tbaa !8
  store float %203, float addrspace(1)* %201, align 4, !tbaa !12
  br label %if_end2

if_end2:                                          ; preds = %if_then1, %if_end
  ret void
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workgroup.id.x() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.y() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.x() #1

; Function Attrs: convergent nounwind
declare void @llvm.amdgcn.s.barrier() #2

attributes #0 = { nounwind "amdgpu-flat-work-group-size"="1,1024" "no-nans-fp-math"="true" }
attributes #1 = { nounwind readnone speculatable }
attributes #2 = { convergent nounwind }

!llvm.module.flags = !{!0, !1}
!opencl.ocl.version = !{!2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 1}
!2 = !{i32 2, i32 0}
!3 = !{!"clang version 12.0.0 (/src/external/llvm-project/clang 1100ebe275a9dcc79a9abbda902b6f10738f2f5d)"}
!4 = !{!5, !5, i64 0}
!5 = !{!"float", !6, i64 0}
!6 = !{!"0x1850f00", !7, i64 0}
!7 = !{!"tvm-tbaa"}
!8 = !{!9, !9, i64 0}
!9 = !{!"float", !10, i64 0}
!10 = !{!"0x18682e0", !7, i64 0}
!11 = !{!"branch_weights", i32 1048576, i32 1}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"0x1850ec0", !7, i64 0}

(128, 4032, 11, 11) axis: (2, 3)
[128, 4032]
Time cost of this operator: 0.0004546081
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [128, 4032], []),
             data: Buffer(data_2: Pointer(float32), float32, [128, 4032, 11, 11], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16128;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf[0] = 0f32
    for (k2.k3.fused.outer: int32, 0, 4) {
      if @tir.likely(((((k2.k3.fused.outer*32) + threadIdx.x) < 121) && (((k2.k3.fused.outer*32) + threadIdx.x) < 121)), dtype=bool) {
        data_red.rf[0] = ((float32*)data_red.rf[0] + (float32*)data_2[((((blockIdx.x*3872) + (threadIdx.y*121)) + (k2.k3.fused.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, (float32*)data_red.rf[0], True, reduce_temp0, threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red_2[((blockIdx.x*32) + threadIdx.y)] = (float32*)reduce_temp0[0]
    }
  }
}


; ModuleID = 'TVMAMDGPUModule'
source_filename = "TVMAMDGPUModule"
target datalayout = "e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-ni:7"
target triple = "amdgcn-amd-amdhsa-hcc"

@shmem = private addrspace(3) global [1024 x float], align 16

; Function Attrs: nounwind
define dllexport amdgpu_kernel void @default_function_kernel0(float addrspace(1)* noalias nocapture readonly %0, float addrspace(1)* noalias nocapture %1) local_unnamed_addr #0 {
entry:
  %2 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %3 = tail call i32 @llvm.amdgcn.workitem.id.y()
  %4 = tail call i32 @llvm.amdgcn.workitem.id.x()
  %5 = mul nsw i32 %3, 121
  %6 = mul nsw i32 %2, 3872
  %7 = icmp slt i32 %4, 121
  br i1 %7, label %if_then, label %if_end, !prof !4

if_then:                                          ; preds = %entry
  %8 = add i32 %4, %5
  %9 = add i32 %8, %6
  %10 = sext i32 %9 to i64
  %11 = getelementptr inbounds float, float addrspace(1)* %0, i64 %10
  %12 = load float, float addrspace(1)* %11, align 4, !tbaa !5
  %13 = fadd float %12, 0.000000e+00
  br label %if_end

if_end:                                           ; preds = %if_then, %entry
  %.1 = phi float [ %13, %if_then ], [ 0.000000e+00, %entry ]
  %14 = add i32 %4, 32
  %15 = icmp slt i32 %14, 121
  br i1 %15, label %if_then.1, label %if_end.1, !prof !4

if_then1:                                         ; preds = %if_end.3
  %16 = add nsw i32 %71, 16
  %17 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %16
  %18 = load volatile float, float addrspace(3)* %17, align 4, !tbaa !9
  %19 = load volatile float, float addrspace(3)* %72, align 4, !tbaa !9
  %20 = fadd float %18, %19
  store volatile float %20, float addrspace(3)* %72, align 4, !tbaa !9
  %21 = add nsw i32 %71, 8
  %22 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %21
  %23 = load volatile float, float addrspace(3)* %22, align 4, !tbaa !9
  %24 = load volatile float, float addrspace(3)* %72, align 4, !tbaa !9
  %25 = fadd float %23, %24
  store volatile float %25, float addrspace(3)* %72, align 4, !tbaa !9
  %26 = add nsw i32 %71, 4
  %27 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %26
  %28 = load volatile float, float addrspace(3)* %27, align 4, !tbaa !9
  %29 = load volatile float, float addrspace(3)* %72, align 4, !tbaa !9
  %30 = fadd float %28, %29
  store volatile float %30, float addrspace(3)* %72, align 4, !tbaa !9
  %31 = add nsw i32 %71, 2
  %32 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %31
  %33 = load volatile float, float addrspace(3)* %32, align 4, !tbaa !9
  %34 = load volatile float, float addrspace(3)* %72, align 4, !tbaa !9
  %35 = fadd float %33, %34
  store volatile float %35, float addrspace(3)* %72, align 4, !tbaa !9
  %36 = add nsw i32 %71, 1
  %37 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %36
  %38 = load volatile float, float addrspace(3)* %37, align 4, !tbaa !9
  %39 = load volatile float, float addrspace(3)* %72, align 4, !tbaa !9
  %40 = fadd float %38, %39
  store volatile float %40, float addrspace(3)* %72, align 4, !tbaa !9
  br label %if_end2

if_end2:                                          ; preds = %if_then1, %if_end.3
  tail call void @llvm.amdgcn.s.barrier()
  %41 = icmp eq i32 %4, 0
  br i1 %41, label %if_then3, label %if_end4, !prof !4

if_then3:                                         ; preds = %if_end2
  %42 = shl i32 %2, 5
  %43 = add nsw i32 %42, %3
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds float, float addrspace(1)* %1, i64 %44
  %46 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %70
  %47 = load volatile float, float addrspace(3)* %46, align 16, !tbaa !9
  store float %47, float addrspace(1)* %45, align 4, !tbaa !12
  br label %if_end4

if_end4:                                          ; preds = %if_then3, %if_end2
  ret void

if_then.1:                                        ; preds = %if_end
  %48 = add i32 %14, %5
  %49 = add i32 %48, %6
  %50 = sext i32 %49 to i64
  %51 = getelementptr inbounds float, float addrspace(1)* %0, i64 %50
  %52 = load float, float addrspace(1)* %51, align 4, !tbaa !5
  %53 = fadd float %.1, %52
  br label %if_end.1

if_end.1:                                         ; preds = %if_then.1, %if_end
  %.1.1 = phi float [ %53, %if_then.1 ], [ %.1, %if_end ]
  %54 = add i32 %4, 64
  %55 = icmp slt i32 %54, 121
  br i1 %55, label %if_then.2, label %if_end.2, !prof !4

if_then.2:                                        ; preds = %if_end.1
  %56 = add i32 %54, %5
  %57 = add i32 %56, %6
  %58 = sext i32 %57 to i64
  %59 = getelementptr inbounds float, float addrspace(1)* %0, i64 %58
  %60 = load float, float addrspace(1)* %59, align 4, !tbaa !5
  %61 = fadd float %.1.1, %60
  br label %if_end.2

if_end.2:                                         ; preds = %if_then.2, %if_end.1
  %.1.2 = phi float [ %61, %if_then.2 ], [ %.1.1, %if_end.1 ]
  %62 = add i32 %4, 96
  %63 = icmp slt i32 %62, 121
  br i1 %63, label %if_then.3, label %if_end.3, !prof !4

if_then.3:                                        ; preds = %if_end.2
  %64 = add i32 %62, %5
  %65 = add i32 %64, %6
  %66 = sext i32 %65 to i64
  %67 = getelementptr inbounds float, float addrspace(1)* %0, i64 %66
  %68 = load float, float addrspace(1)* %67, align 4, !tbaa !5
  %69 = fadd float %.1.2, %68
  br label %if_end.3

if_end.3:                                         ; preds = %if_then.3, %if_end.2
  %.1.3 = phi float [ %69, %if_then.3 ], [ %.1.2, %if_end.2 ]
  tail call void @llvm.amdgcn.s.barrier()
  %70 = shl nsw i32 %3, 5
  %71 = add nsw i32 %70, %4
  %72 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %71
  store volatile float %.1.3, float addrspace(3)* %72, align 4, !tbaa !9
  tail call void @llvm.amdgcn.s.barrier()
  %73 = icmp slt i32 %4, 16
  br i1 %73, label %if_then1, label %if_end2, !prof !4
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workgroup.id.x() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.y() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.x() #1

; Function Attrs: convergent nounwind
declare void @llvm.amdgcn.s.barrier() #2

attributes #0 = { nounwind "amdgpu-flat-work-group-size"="1,1024" "no-nans-fp-math"="true" }
attributes #1 = { nounwind readnone speculatable }
attributes #2 = { convergent nounwind }

!llvm.module.flags = !{!0, !1}
!opencl.ocl.version = !{!2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 1}
!2 = !{i32 2, i32 0}
!3 = !{!"clang version 12.0.0 (/src/external/llvm-project/clang 1100ebe275a9dcc79a9abbda902b6f10738f2f5d)"}
!4 = !{!"branch_weights", i32 1048576, i32 1}
!5 = !{!6, !6, i64 0}
!6 = !{!"float", !7, i64 0}
!7 = !{!"0x2fb9d70", !8, i64 0}
!8 = !{!"tvm-tbaa"}
!9 = !{!10, !10, i64 0}
!10 = !{!"float", !11, i64 0}
!11 = !{!"0x2fd04f0", !8, i64 0}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"0x2fb67c0", !8, i64 0}

(128, 2048, 7, 7) axis: (2, 3)
[128, 2048]
Time cost of this operator: 0.0001416311
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [128, 2048], []),
             data: Buffer(data_2: Pointer(float32), float32, [128, 2048, 7, 7], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf[0] = 0f32
    for (k2.k3.fused.outer: int32, 0, 2) {
      if @tir.likely(((((k2.k3.fused.outer*32) + threadIdx.x) < 49) && (((k2.k3.fused.outer*32) + threadIdx.x) < 49)), dtype=bool) {
        data_red.rf[0] = ((float32*)data_red.rf[0] + (float32*)data_2[((((blockIdx.x*1568) + (threadIdx.y*49)) + (k2.k3.fused.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, (float32*)data_red.rf[0], True, reduce_temp0, threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red_2[((blockIdx.x*32) + threadIdx.y)] = (float32*)reduce_temp0[0]
    }
  }
}


; ModuleID = 'TVMAMDGPUModule'
source_filename = "TVMAMDGPUModule"
target datalayout = "e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-ni:7"
target triple = "amdgcn-amd-amdhsa-hcc"

@shmem = private addrspace(3) global [1024 x float], align 16

; Function Attrs: nounwind
define dllexport amdgpu_kernel void @default_function_kernel0(float addrspace(1)* noalias nocapture readonly %0, float addrspace(1)* noalias nocapture %1) local_unnamed_addr #0 {
entry:
  %2 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %3 = tail call i32 @llvm.amdgcn.workitem.id.y()
  %4 = tail call i32 @llvm.amdgcn.workitem.id.x()
  %5 = mul nsw i32 %3, 49
  %6 = mul nsw i32 %2, 1568
  %7 = icmp slt i32 %4, 49
  br i1 %7, label %if_then, label %if_end, !prof !4

if_then:                                          ; preds = %entry
  %8 = add i32 %4, %5
  %9 = add i32 %8, %6
  %10 = sext i32 %9 to i64
  %11 = getelementptr inbounds float, float addrspace(1)* %0, i64 %10
  %12 = load float, float addrspace(1)* %11, align 4, !tbaa !5
  %13 = fadd float %12, 0.000000e+00
  br label %if_end

if_end:                                           ; preds = %if_then, %entry
  %.1 = phi float [ %13, %if_then ], [ 0.000000e+00, %entry ]
  %14 = add i32 %4, 32
  %15 = icmp slt i32 %14, 49
  br i1 %15, label %if_then.1, label %if_end.1, !prof !4

if_then1:                                         ; preds = %if_end.1
  %16 = add nsw i32 %55, 16
  %17 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %16
  %18 = load volatile float, float addrspace(3)* %17, align 4, !tbaa !9
  %19 = load volatile float, float addrspace(3)* %56, align 4, !tbaa !9
  %20 = fadd float %18, %19
  store volatile float %20, float addrspace(3)* %56, align 4, !tbaa !9
  %21 = add nsw i32 %55, 8
  %22 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %21
  %23 = load volatile float, float addrspace(3)* %22, align 4, !tbaa !9
  %24 = load volatile float, float addrspace(3)* %56, align 4, !tbaa !9
  %25 = fadd float %23, %24
  store volatile float %25, float addrspace(3)* %56, align 4, !tbaa !9
  %26 = add nsw i32 %55, 4
  %27 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %26
  %28 = load volatile float, float addrspace(3)* %27, align 4, !tbaa !9
  %29 = load volatile float, float addrspace(3)* %56, align 4, !tbaa !9
  %30 = fadd float %28, %29
  store volatile float %30, float addrspace(3)* %56, align 4, !tbaa !9
  %31 = add nsw i32 %55, 2
  %32 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %31
  %33 = load volatile float, float addrspace(3)* %32, align 4, !tbaa !9
  %34 = load volatile float, float addrspace(3)* %56, align 4, !tbaa !9
  %35 = fadd float %33, %34
  store volatile float %35, float addrspace(3)* %56, align 4, !tbaa !9
  %36 = add nsw i32 %55, 1
  %37 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %36
  %38 = load volatile float, float addrspace(3)* %37, align 4, !tbaa !9
  %39 = load volatile float, float addrspace(3)* %56, align 4, !tbaa !9
  %40 = fadd float %38, %39
  store volatile float %40, float addrspace(3)* %56, align 4, !tbaa !9
  br label %if_end2

if_end2:                                          ; preds = %if_then1, %if_end.1
  tail call void @llvm.amdgcn.s.barrier()
  %41 = icmp eq i32 %4, 0
  br i1 %41, label %if_then3, label %if_end4, !prof !4

if_then3:                                         ; preds = %if_end2
  %42 = shl i32 %2, 5
  %43 = add nsw i32 %42, %3
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds float, float addrspace(1)* %1, i64 %44
  %46 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %54
  %47 = load volatile float, float addrspace(3)* %46, align 16, !tbaa !9
  store float %47, float addrspace(1)* %45, align 4, !tbaa !12
  br label %if_end4

if_end4:                                          ; preds = %if_then3, %if_end2
  ret void

if_then.1:                                        ; preds = %if_end
  %48 = add i32 %14, %5
  %49 = add i32 %48, %6
  %50 = sext i32 %49 to i64
  %51 = getelementptr inbounds float, float addrspace(1)* %0, i64 %50
  %52 = load float, float addrspace(1)* %51, align 4, !tbaa !5
  %53 = fadd float %.1, %52
  br label %if_end.1

if_end.1:                                         ; preds = %if_then.1, %if_end
  %.1.1 = phi float [ %53, %if_then.1 ], [ %.1, %if_end ]
  tail call void @llvm.amdgcn.s.barrier()
  %54 = shl nsw i32 %3, 5
  %55 = add nsw i32 %54, %4
  %56 = getelementptr inbounds [1024 x float], [1024 x float] addrspace(3)* @shmem, i32 0, i32 %55
  store volatile float %.1.1, float addrspace(3)* %56, align 4, !tbaa !9
  tail call void @llvm.amdgcn.s.barrier()
  %57 = icmp slt i32 %4, 16
  br i1 %57, label %if_then1, label %if_end2, !prof !4
}

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workgroup.id.x() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.y() #1

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.amdgcn.workitem.id.x() #1

; Function Attrs: convergent nounwind
declare void @llvm.amdgcn.s.barrier() #2

attributes #0 = { nounwind "amdgpu-flat-work-group-size"="1,1024" "no-nans-fp-math"="true" }
attributes #1 = { nounwind readnone speculatable }
attributes #2 = { convergent nounwind }

!llvm.module.flags = !{!0, !1}
!opencl.ocl.version = !{!2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 1}
!2 = !{i32 2, i32 0}
!3 = !{!"clang version 12.0.0 (/src/external/llvm-project/clang 1100ebe275a9dcc79a9abbda902b6f10738f2f5d)"}
!4 = !{!"branch_weights", i32 1048576, i32 1}
!5 = !{!6, !6, i64 0}
!6 = !{!"float", !7, i64 0}
!7 = !{!"0x1cd4c90", !8, i64 0}
!8 = !{!"tvm-tbaa"}
!9 = !{!10, !10, i64 0}
!10 = !{!"float", !11, i64 0}
!11 = !{!"0x1ceb540", !8, i64 0}
!12 = !{!13, !13, i64 0}
!13 = !{!"float", !14, i64 0}
!14 = !{!"0x1cd17c0", !8, i64 0}

