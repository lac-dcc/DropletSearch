(128, 512, 1024) axis: 2
[128, 512]
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [65536], []),
             data: Buffer(data_2: Pointer(float32), float32, [67108864], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2048;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf_1: Buffer(data_red.rf, float32, [1], [], scope="local", align=4)[0] = 0f32
    for (k2.outer: int32, 0, 32) {
      data_red.rf_1[0] = (data_red.rf_1[0] + data[((((blockIdx.x*32768) + (threadIdx.y*1024)) + (k2.outer*32)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, data_red.rf_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red[((blockIdx.x*32) + threadIdx.y)] = reduce_temp0_1[0]
    }
  }
}



#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)
#define __shfl_sync(mask, var, lane, width) \
        __shfl((var), (lane), (width))

#define __shfl_down_sync(mask, var, offset, width) \
        __shfl_down((var), (offset), (width))

#define __shfl_up_sync(mask, var, offset, width) \
        __shfl_up((var), (offset), (width))
#endif


#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) default_function_kernel0(float* __restrict__ data, float* __restrict__ data_red) {
  float data_red_rf[1];
  float red_buf0[1];
  data_red_rf[0] = 0.000000e+00f;
  for (int k2_outer = 0; k2_outer < 32; ++k2_outer) {
    data_red_rf[0] = (data_red_rf[0] + data[((((((int)blockIdx.x) * 32768) + (((int)threadIdx.y) * 1024)) + (k2_outer * 32)) + ((int)threadIdx.x))]);
  }
  uint mask[1];
  float t0[1];
  red_buf0[0] = data_red_rf[0];
  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);
  if (((int)threadIdx.x) == 0) {
    data_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.y))] = red_buf0[0];
  }
}


best runtime: 1.5884752000
compilation time: 3.045133113861084
(65536, 1024) axis: 1
[65536]
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [65536], []),
             data: Buffer(data_2: Pointer(float32), float32, [67108864], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2048;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf_1: Buffer(data_red.rf, float32, [1], [], scope="local", align=4)[0] = 0f32
    for (k1.outer: int32, 0, 32) {
      data_red.rf_1[0] = (data_red.rf_1[0] + data[((((blockIdx.x*32768) + (threadIdx.y*1024)) + (k1.outer*32)) + threadIdx.x)])
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, data_red.rf_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red[((blockIdx.x*32) + threadIdx.y)] = reduce_temp0_1[0]
    }
  }
}



#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)
#define __shfl_sync(mask, var, lane, width) \
        __shfl((var), (lane), (width))

#define __shfl_down_sync(mask, var, offset, width) \
        __shfl_down((var), (offset), (width))

#define __shfl_up_sync(mask, var, offset, width) \
        __shfl_up((var), (offset), (width))
#endif


#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) default_function_kernel0(float* __restrict__ data, float* __restrict__ data_red) {
  float data_red_rf[1];
  float red_buf0[1];
  data_red_rf[0] = 0.000000e+00f;
  for (int k1_outer = 0; k1_outer < 32; ++k1_outer) {
    data_red_rf[0] = (data_red_rf[0] + data[((((((int)blockIdx.x) * 32768) + (((int)threadIdx.y) * 1024)) + (k1_outer * 32)) + ((int)threadIdx.x))]);
  }
  uint mask[1];
  float t0[1];
  red_buf0[0] = data_red_rf[0];
  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);
  if (((int)threadIdx.x) == 0) {
    data_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.y))] = red_buf0[0];
  }
}


best runtime: 1.5882151000
compilation time: 2.968754768371582
(128, 4032, 11, 11) axis: (2, 3)
[128, 4032]
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [516096], []),
             data: Buffer(data_2: Pointer(float32), float32, [62447616], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 16128;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf_1: Buffer(data_red.rf, float32, [1], [], scope="local", align=4)[0] = 0f32
    for (k2.k3.fused.outer: int32, 0, 4) {
      if @tir.likely(((((k2.k3.fused.outer*32) + threadIdx.x) < 121) && (((k2.k3.fused.outer*32) + threadIdx.x) < 121)), dtype=bool) {
        data_red.rf_1[0] = (data_red.rf_1[0] + data[((((blockIdx.x*3872) + (threadIdx.y*121)) + (k2.k3.fused.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, data_red.rf_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red[((blockIdx.x*32) + threadIdx.y)] = reduce_temp0_1[0]
    }
  }
}



#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)
#define __shfl_sync(mask, var, lane, width) \
        __shfl((var), (lane), (width))

#define __shfl_down_sync(mask, var, offset, width) \
        __shfl_down((var), (offset), (width))

#define __shfl_up_sync(mask, var, offset, width) \
        __shfl_up((var), (offset), (width))
#endif


#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) default_function_kernel0(float* __restrict__ data, float* __restrict__ data_red) {
  float data_red_rf[1];
  float red_buf0[1];
  data_red_rf[0] = 0.000000e+00f;
  for (int k2_k3_fused_outer = 0; k2_k3_fused_outer < 4; ++k2_k3_fused_outer) {
    if ((((k2_k3_fused_outer * 32) + ((int)threadIdx.x)) < 121) && (((k2_k3_fused_outer * 32) + ((int)threadIdx.x)) < 121)) {
      data_red_rf[0] = (data_red_rf[0] + data[((((((int)blockIdx.x) * 3872) + (((int)threadIdx.y) * 121)) + (k2_k3_fused_outer * 32)) + ((int)threadIdx.x))]);
    }
  }
  uint mask[1];
  float t0[1];
  red_buf0[0] = data_red_rf[0];
  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);
  if (((int)threadIdx.x) == 0) {
    data_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.y))] = red_buf0[0];
  }
}


best runtime: 2.0358150000
compilation time: 3.405298948287964
(128, 2048, 7, 7) axis: (2, 3)
[128, 2048]
Lowered TIR:
@main = primfn(data_1: handle, data_red_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {data_red: Buffer(data_red_2: Pointer(float32), float32, [262144], []),
             data: Buffer(data_2: Pointer(float32), float32, [12845056], [])}
  buffer_map = {data_1: data, data_red_1: data_red} {
  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 8192;
  allocate(data_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;
  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;
  attr [IterVar(threadIdx.y: int32, [0:32], "ThreadIndex", "threadIdx.y")] "thread_extent" = 32;
  attr [IterVar(threadIdx.x: int32, [0:32], "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
    data_red.rf_1: Buffer(data_red.rf, float32, [1], [], scope="local", align=4)[0] = 0f32
    for (k2.k3.fused.outer: int32, 0, 2) {
      if @tir.likely(((((k2.k3.fused.outer*32) + threadIdx.x) < 49) && (((k2.k3.fused.outer*32) + threadIdx.x) < 49)), dtype=bool) {
        data_red.rf_1[0] = (data_red.rf_1[0] + data[((((blockIdx.x*1568) + (threadIdx.y*49)) + (k2.k3.fused.outer*32)) + threadIdx.x)])
      }
    }
    attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
    @tir.tvm_thread_allreduce(1u32, data_red.rf_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope="local")[0], threadIdx.x, dtype=handle)
    if (threadIdx.x == 0) {
      data_red[((blockIdx.x*32) + threadIdx.y)] = reduce_temp0_1[0]
    }
  }
}



#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)
#define __shfl_sync(mask, var, lane, width) \
        __shfl((var), (lane), (width))

#define __shfl_down_sync(mask, var, offset, width) \
        __shfl_down((var), (offset), (width))

#define __shfl_up_sync(mask, var, offset, width) \
        __shfl_up((var), (offset), (width))
#endif


#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif
extern "C" __global__ void __launch_bounds__(1024) default_function_kernel0(float* __restrict__ data, float* __restrict__ data_red) {
  float data_red_rf[1];
  float red_buf0[1];
  data_red_rf[0] = 0.000000e+00f;
  for (int k2_k3_fused_outer = 0; k2_k3_fused_outer < 2; ++k2_k3_fused_outer) {
    if ((((k2_k3_fused_outer * 32) + ((int)threadIdx.x)) < 49) && (((k2_k3_fused_outer * 32) + ((int)threadIdx.x)) < 49)) {
      data_red_rf[0] = (data_red_rf[0] + data[((((((int)blockIdx.x) * 1568) + (((int)threadIdx.y) * 49)) + (k2_k3_fused_outer * 32)) + ((int)threadIdx.x))]);
    }
  }
  uint mask[1];
  float t0[1];
  red_buf0[0] = data_red_rf[0];
  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);
  red_buf0[0] = (red_buf0[0] + t0[0]);
  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);
  if (((int)threadIdx.x) == 0) {
    data_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.y))] = red_buf0[0];
  }
}


best runtime: 0.5915091000
compilation time: 1.224546194076538
