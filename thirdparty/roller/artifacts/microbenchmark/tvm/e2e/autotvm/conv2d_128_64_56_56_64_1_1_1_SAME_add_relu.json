{"parameters": {"input_shape": [128, 64, 56, 56], "filter_shape": [64, 64, 1, 1], "output_shape": [128, 64, 56, 56], "window_movement_strides": [1, 1], "padding_below_diff": [0, 0], "window_dilation_strides": [1, 1]}, "op_type": "Fused_Convolution_Add_Relu", "tvm_func_name": "conv2d_128_64_56_56_64_1_1_1_SAME_add_relu", "code": "extern \"C\" __global__ void conv2d_128_64_56_56_64_1_1_1_SAME_add_relu(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ compute, float* __restrict__ input2) {\n  float compute1[32];\n  __shared__ float pad_temp_shared[448];\n  __shared__ float placeholder_shared[256];\n  #pragma unroll\n  for (int yy_init = 0; yy_init < 2; ++yy_init) {\n    #pragma unroll\n    for (int vthread_s = 0; vthread_s < 16; ++vthread_s) {\n      compute1[(((vthread_s * 2) + yy_init))] = 0.000000e+00f;\n    }\n  }\n  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n    __syncthreads();\n    #pragma unroll\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n      pad_temp_shared[((((((int)threadIdx.z) * 112) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))] = placeholder[(((((((((int)blockIdx.z) * 200704) + (rc_outer * 12544)) + (((int)threadIdx.z) * 3136)) + (((int)blockIdx.y) * 112)) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))];\n    }\n    #pragma unroll\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {\n      if (((((int)threadIdx.z) * 16) + (((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) >> 2)) < 64) {\n        if ((((((int)threadIdx.z) * 64) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) < 256) {\n          if (((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) < 64) {\n            placeholder_shared[((((((int)threadIdx.z) * 64) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1))] = placeholder1[(((((((int)threadIdx.z) * 1024) + ((((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) >> 2) * 64)) + (rc_outer * 4)) + (((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) & 3)))];\n          }\n        }\n      }\n    }\n    __syncthreads();\n    #pragma unroll\n    for (int rc_inner = 0; rc_inner < 4; ++rc_inner) {\n      #pragma unroll\n      for (int yy = 0; yy < 2; ++yy) {\n        #pragma unroll\n        for (int vthread_s1 = 0; vthread_s1 < 16; ++vthread_s1) {\n          compute1[(((vthread_s1 * 2) + yy))] = (compute1[(((vthread_s1 * 2) + yy))] + (pad_temp_shared[((((rc_inner * 112) + (yy * 56)) + ((int)threadIdx.x)))] * placeholder_shared[((((vthread_s1 * 16) + (((int)threadIdx.z) * 4)) + rc_inner))]));\n        }\n      }\n    }\n  }\n  #pragma unroll\n  for (int i2_inner_inner_inner = 0; i2_inner_inner_inner < 2; ++i2_inner_inner_inner) {\n    #pragma unroll\n    for (int vthread_s2 = 0; vthread_s2 < 16; ++vthread_s2) {\n      compute[(((((((((int)blockIdx.z) * 200704) + (vthread_s2 * 12544)) + (((int)threadIdx.z) * 3136)) + (((int)blockIdx.y) * 112)) + (i2_inner_inner_inner * 56)) + ((int)threadIdx.x)))] = max((compute1[(((vthread_s2 * 2) + i2_inner_inner_inner))] + input2[(((((((((int)blockIdx.z) * 200704) + (vthread_s2 * 12544)) + (((int)threadIdx.z) * 3136)) + (((int)blockIdx.y) * 112)) + (i2_inner_inner_inner * 56)) + ((int)threadIdx.x)))]), 0.000000e+00f);\n    }\n  }\n}\n", "gridDim": [1, 28, 128], "blockDim": [56, 1, 4]}