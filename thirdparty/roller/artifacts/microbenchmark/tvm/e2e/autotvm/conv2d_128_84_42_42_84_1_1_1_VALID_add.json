{"parameters": {"input_shape": [128, 84, 42, 42], "filter_shape": [84, 84, 1, 1], "output_shape": [128, 84, 42, 42], "window_movement_strides": [1, 1], "padding_below_diff": [0, 0], "window_dilation_strides": [1, 1]}, "op_type": "Fused_Convolution_Add", "tvm_func_name": "conv2d_128_84_42_42_84_1_1_1_VALID_add", "code": "extern \"C\" __global__ void conv2d_128_84_42_42_84_1_1_1_VALID_add(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ input2) {\n  float compute[56];\n  __shared__ float pad_temp_shared[504];\n  __shared__ float placeholder_shared[504];\n  for (int ff_init = 0; ff_init < 14; ++ff_init) {\n    for (int yy_init = 0; yy_init < 2; ++yy_init) {\n      compute[(((ff_init * 2) + yy_init))] = 0.000000e+00f;\n      compute[((((ff_init * 2) + yy_init) + 28))] = 0.000000e+00f;\n    }\n  }\n  for (int rc_outer = 0; rc_outer < 14; ++rc_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n      pad_temp_shared[((((((int)threadIdx.z) * 84) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))] = placeholder[(((((((((int)blockIdx.z) * 148176) + (rc_outer * 10584)) + (((int)threadIdx.z) * 1764)) + (((int)blockIdx.y) * 84)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))];\n    }\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {\n      placeholder_shared[((((((int)threadIdx.z) * 84) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1))] = placeholder1[(((((((int)threadIdx.z) * 1176) + ((((((int)threadIdx.x) * 4) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) / 6) * 84)) + (rc_outer * 6)) + (((((int)threadIdx.x) * 4) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) % 6)))];\n    }\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 6; ++rc_inner) {\n      for (int ff = 0; ff < 14; ++ff) {\n        for (int yy = 0; yy < 2; ++yy) {\n          compute[(((ff * 2) + yy))] = (compute[(((ff * 2) + yy))] + (pad_temp_shared[((((rc_inner * 84) + (yy * 42)) + ((int)threadIdx.x)))] * placeholder_shared[((((((int)threadIdx.z) * 84) + (ff * 6)) + rc_inner))]));\n          compute[((((ff * 2) + yy) + 28))] = (compute[((((ff * 2) + yy) + 28))] + (pad_temp_shared[(((((rc_inner * 84) + (yy * 42)) + ((int)threadIdx.x)) + 21))] * placeholder_shared[((((((int)threadIdx.z) * 84) + (ff * 6)) + rc_inner))]));\n        }\n      }\n    }\n  }\n  for (int ax1_inner_inner_inner = 0; ax1_inner_inner_inner < 14; ++ax1_inner_inner_inner) {\n    for (int ax2_inner_inner_inner = 0; ax2_inner_inner_inner < 2; ++ax2_inner_inner_inner) {\n      T_add[(((((((((int)blockIdx.z) * 148176) + (((int)threadIdx.z) * 24696)) + (ax1_inner_inner_inner * 1764)) + (((int)blockIdx.y) * 84)) + (ax2_inner_inner_inner * 42)) + ((int)threadIdx.x)))] = (compute[(((ax1_inner_inner_inner * 2) + ax2_inner_inner_inner))] + input2[(((((((((int)blockIdx.z) * 148176) + (((int)threadIdx.z) * 24696)) + (ax1_inner_inner_inner * 1764)) + (((int)blockIdx.y) * 84)) + (ax2_inner_inner_inner * 42)) + ((int)threadIdx.x)))]);\n      T_add[((((((((((int)blockIdx.z) * 148176) + (((int)threadIdx.z) * 24696)) + (ax1_inner_inner_inner * 1764)) + (((int)blockIdx.y) * 84)) + (ax2_inner_inner_inner * 42)) + ((int)threadIdx.x)) + 21))] = (compute[((((ax1_inner_inner_inner * 2) + ax2_inner_inner_inner) + 28))] + input2[((((((((((int)blockIdx.z) * 148176) + (((int)threadIdx.z) * 24696)) + (ax1_inner_inner_inner * 1764)) + (((int)blockIdx.y) * 84)) + (ax2_inner_inner_inner * 42)) + ((int)threadIdx.x)) + 21))]);\n    }\n  }\n}\n", "gridDim": [1, 21, 128], "blockDim": [21, 1, 6]}