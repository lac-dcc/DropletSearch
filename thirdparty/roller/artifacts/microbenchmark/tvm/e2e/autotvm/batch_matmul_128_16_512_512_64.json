{"parameters": {"arg0_shape": [128, 16, 512, 512], "arg1_shape": [128, 16, 512, 64], "out_shape": [128, 16, 512, 64], "transpose_A": false, "transpose_B": false}, "op_type": "BatchMatMul", "tvm_func_name": "batch_matmul_128_16_512_512_64", "code": "extern \"C\" __global__ void batch_matmul_128_16_512_512_64(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_batch_matmul_NN) {\n  float T_batch_matmul_NN_local[32];\n  __shared__ float placeholder_shared[4096];\n  __shared__ float placeholder_d_shared[2048];\n  float placeholder_shared_local[8];\n  float placeholder_d_shared_local[4];\n  for (int i_c_init = 0; i_c_init < 8; ++i_c_init) {\n    for (int j_c_init = 0; j_c_init < 4; ++j_c_init) {\n      T_batch_matmul_NN_local[(((i_c_init * 4) + j_c_init))] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer = 0; k_outer < 16; ++k_outer) {\n    __syncthreads();\n    #pragma unroll\n    for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n      #pragma unroll\n      for (int ax2_inner = 0; ax2_inner < 2; ++ax2_inner) {\n        placeholder_shared[(((((((int)threadIdx.y) * 256) + (ax1_inner * 32)) + (((int)threadIdx.x) * 2)) + ax2_inner))] = placeholder[((((((((((int)blockIdx.z) * 262144) + (((int)blockIdx.y) * 65536)) + (((int)threadIdx.y) * 4096)) + (ax1_inner * 512)) + (k_outer * 32)) + (((int)threadIdx.x) * 2)) + ax2_inner))];\n      }\n    }\n    #pragma unroll\n    for (int ax1_inner1 = 0; ax1_inner1 < 2; ++ax1_inner1) {\n      #pragma unroll\n      for (int ax2_inner1 = 0; ax2_inner1 < 4; ++ax2_inner1) {\n        placeholder_d_shared[(((((((int)threadIdx.y) * 128) + (ax1_inner1 * 64)) + (((int)threadIdx.x) * 4)) + ax2_inner1))] = placeholder1[(((((((((int)blockIdx.z) * 32768) + (k_outer * 2048)) + (((int)threadIdx.y) * 128)) + (ax1_inner1 * 64)) + (((int)threadIdx.x) * 4)) + ax2_inner1))];\n      }\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 32; ++k_inner) {\n      #pragma unroll\n      for (int ax1 = 0; ax1 < 8; ++ax1) {\n        placeholder_shared_local[(ax1)] = placeholder_shared[((((((int)threadIdx.y) * 256) + (ax1 * 32)) + k_inner))];\n      }\n      #pragma unroll\n      for (int ax2 = 0; ax2 < 4; ++ax2) {\n        placeholder_d_shared_local[(ax2)] = placeholder_d_shared[((((k_inner * 64) + (((int)threadIdx.x) * 4)) + ax2))];\n      }\n      for (int i_c = 0; i_c < 8; ++i_c) {\n        #pragma unroll\n        for (int j_c = 0; j_c < 4; ++j_c) {\n          T_batch_matmul_NN_local[(((i_c * 4) + j_c))] = (T_batch_matmul_NN_local[(((i_c * 4) + j_c))] + (placeholder_shared_local[(i_c)] * placeholder_d_shared_local[(j_c)]));\n        }\n      }\n    }\n  }\n  for (int i_inner_inner = 0; i_inner_inner < 8; ++i_inner_inner) {\n    #pragma unroll\n    for (int j_inner_inner = 0; j_inner_inner < 4; ++j_inner_inner) {\n      T_batch_matmul_NN[(((((((((int)blockIdx.z) * 32768) + (((int)blockIdx.y) * 8192)) + (((int)threadIdx.y) * 512)) + (i_inner_inner * 64)) + (((int)threadIdx.x) * 4)) + j_inner_inner))] = T_batch_matmul_NN_local[(((i_inner_inner * 4) + j_inner_inner))];\n    }\n  }\n}\n", "gridDim": [1, 4, 2048], "blockDim": [16, 16, 1]}