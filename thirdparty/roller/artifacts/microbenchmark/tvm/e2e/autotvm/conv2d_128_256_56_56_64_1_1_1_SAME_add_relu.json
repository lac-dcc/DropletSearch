{"parameters": {"input_shape": [128, 256, 56, 56], "filter_shape": [64, 256, 1, 1], "output_shape": [128, 64, 56, 56], "window_movement_strides": [1, 1], "padding_below_diff": [0, 0], "window_dilation_strides": [1, 1]}, "op_type": "Fused_Convolution_Add_Relu", "tvm_func_name": "conv2d_128_256_56_56_64_1_1_1_SAME_add_relu", "code": "extern \"C\" __global__ void conv2d_128_256_56_56_64_1_1_1_SAME_add_relu(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ compute, float* __restrict__ input2) {\n  float compute1[32];\n  __shared__ float pad_temp_shared[896];\n  __shared__ float placeholder_shared[128];\n  #pragma unroll\n  for (int ff_init = 0; ff_init < 16; ++ff_init) {\n    compute1[(ff_init)] = 0.000000e+00f;\n    compute1[((ff_init + 16))] = 0.000000e+00f;\n  }\n  for (int rc_outer = 0; rc_outer < 64; ++rc_outer) {\n    __syncthreads();\n    #pragma unroll\n    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 4; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n      pad_temp_shared[(((((((int)threadIdx.z) * 448) + (((int)threadIdx.y) * 224)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))] = placeholder[(((((((((((int)blockIdx.z) >> 1) * 802816) + (rc_outer * 12544)) + (((int)threadIdx.z) * 6272)) + (((int)threadIdx.y) * 3136)) + (((int)blockIdx.y) * 224)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))];\n    }\n    if ((((((int)threadIdx.z) * 16) + (((int)threadIdx.y) * 8)) + (((int)threadIdx.x) >> 2)) < 32) {\n      if ((((((int)threadIdx.z) * 64) + (((int)threadIdx.y) * 32)) + ((int)threadIdx.x)) < 128) {\n        if (((((int)threadIdx.y) * 32) + ((int)threadIdx.x)) < 64) {\n          if (((int)threadIdx.x) < 32) {\n            placeholder_shared[((((((int)threadIdx.z) * 64) + (((int)threadIdx.y) * 32)) + ((int)threadIdx.x)))] = placeholder1[((((((((((int)blockIdx.z) & 1) * 8192) + (((int)threadIdx.z) * 4096)) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) >> 2) * 256)) + (rc_outer * 4)) + (((int)threadIdx.x) & 3)))];\n          }\n        }\n      }\n    }\n    __syncthreads();\n    #pragma unroll\n    for (int rc_inner = 0; rc_inner < 4; ++rc_inner) {\n      #pragma unroll\n      for (int ff = 0; ff < 16; ++ff) {\n        compute1[(ff)] = (compute1[(ff)] + (pad_temp_shared[((((rc_inner * 224) + (((int)threadIdx.y) * 56)) + ((int)threadIdx.x)))] * placeholder_shared[((((((int)threadIdx.z) * 64) + (ff * 4)) + rc_inner))]));\n        compute1[((ff + 16))] = (compute1[((ff + 16))] + (pad_temp_shared[(((((rc_inner * 224) + (((int)threadIdx.y) * 56)) + ((int)threadIdx.x)) + 112))] * placeholder_shared[((((((int)threadIdx.z) * 64) + (ff * 4)) + rc_inner))]));\n      }\n    }\n  }\n  #pragma unroll\n  for (int i1_inner_inner_inner = 0; i1_inner_inner_inner < 16; ++i1_inner_inner_inner) {\n    compute[(((((((((int)blockIdx.z) * 100352) + (((int)threadIdx.z) * 50176)) + (i1_inner_inner_inner * 3136)) + (((int)blockIdx.y) * 224)) + (((int)threadIdx.y) * 56)) + ((int)threadIdx.x)))] = max((compute1[(i1_inner_inner_inner)] + input2[(((((((((int)blockIdx.z) * 100352) + (((int)threadIdx.z) * 50176)) + (i1_inner_inner_inner * 3136)) + (((int)blockIdx.y) * 224)) + (((int)threadIdx.y) * 56)) + ((int)threadIdx.x)))]), 0.000000e+00f);\n    compute[((((((((((int)blockIdx.z) * 100352) + (((int)threadIdx.z) * 50176)) + (i1_inner_inner_inner * 3136)) + (((int)blockIdx.y) * 224)) + (((int)threadIdx.y) * 56)) + ((int)threadIdx.x)) + 112))] = max((compute1[((i1_inner_inner_inner + 16))] + input2[((((((((((int)blockIdx.z) * 100352) + (((int)threadIdx.z) * 50176)) + (i1_inner_inner_inner * 3136)) + (((int)blockIdx.y) * 224)) + (((int)threadIdx.y) * 56)) + ((int)threadIdx.x)) + 112))]), 0.000000e+00f);\n  }\n}\n", "gridDim": [1, 14, 256], "blockDim": [56, 2, 2]}