.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.T
==================================================
No: 161	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 8192)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 2)) + j_inner) + 64)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.49, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for b_c.3 (0,4)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              for k.2 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,8)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 162	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
lockIdx.x) * 64)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 8192)] = T_batch_matmul_NN_local[((((b_inner * 256) + (i_inner * 32)) + j_inner) + 1536)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.74, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for j_c.3 (0,8)
          for b_c.4 (0,6)
            for i_c.4 (0,8)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,8)
          for j.3 (0,32)
            T_batch_matmul_NN = ...

==================================================
No: 163	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 32) + (i_inner * 8)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,8)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for k.1 (0,8)
        for b_c.3 (0,3)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 164	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x) & 31) * 512)) + (i_inner * 128)) + j_inner) + 98368)] = T_batch_matmul_NN_local[((((b_inner * 256) + (i_inner * 64)) + j_inner) + 1536)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:14.45, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for b_c.3 (0,2)
        for i_c.3 (0,4)
          for j_c.3 (0,16)
            for k.2 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,4)
        for j.3 (0,64)
          T_batch_matmul_NN = ...

==================================================
No: 165	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (b_inner * 16384)) + (((int)blockIdx.x) * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 131072)] = T_batch_matmul_NN_local[(((b_inner * 2) + j_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for j_c.3 (0,2)
              for k.2 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 166	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 128)) + ((((int)threadIdx.x) & 3) * 32)) + j_inner) + 139264)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 32)) + j_inner) + 1280)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for b_c.3 (0,4)
        for i_c.3 (0,2)
          for j_c.4 (0,32)
            T_batch_matmul_NN.local = ...
    for b.3 (0,4)
      for i.3 (0,2)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 167	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x) & 127) >> 2) * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 4128)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
        for j_c.3 (0,2)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 168	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 3) * 32)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 8208)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 2)) + j_inner) + 192)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.49, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for b_c.3 (0,4)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,8)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 169	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p0.shared = ...
      for k.1 (0,2)
        for b_c.3 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,8)
              for j_c.4 (0,16)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,4)
        for j.3 (0,128)
          T_batch_matmul_NN = ...

==================================================
No: 170	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
Idx.x) & 63) >> 5) * 8192)) + ((((int)threadIdx.x) & 63) * 128)) + ((((int)blockIdx.x) & 31) * 4)) + j_inner)] = T_batch_matmul_NN_local[((b_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.65, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for b_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 171	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 128)) + ((((int)threadIdx.x) & 31) * 4)) + j_inner) + 6144)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 4)) + j_inner) + 1152)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.56, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,2)
            for b_c.4 (0,12)
              for i_c.4 (0,4)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for i.3 (0,8)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 172	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 2) * 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 31) * 4)) + (((int)threadIdx.x) & 3)) + 2048)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,4)
            for b_c.4 (0,3)
              T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 173	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 49184)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 174	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 128)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 8288)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 2)) + j_inner) + 448)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for b_c.4 (0,2)
                for i_c.4 (0,16)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,16)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 175	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x) >> 1) * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 1) * 4)) + j_inner) + 8)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,2)
          for j_c.3 (0,4)
            for b_c.4 (0,3)
              T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 176	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 8)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 2)) + j_inner) + 48)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.81, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,8)
            for b_c.4 (0,3)
              for i_c.4 (0,8)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 177	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 1) * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 4144)] = T_batch_matmul_NN_local[(((b_inner * 8) + j_inner) + 672)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,2)
            for b_c.4 (0,12)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 178	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
5) * 1024)) + (i_inner * 128)) + ((((int)blockIdx.x) & 3) * 32)) + (((int)threadIdx.x) & 31)) + 49152)] = T_batch_matmul_NN_local[(((b_inner * 8) + i_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.54, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,2)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,3)
            for i_c.3 (0,4)
              for k.2 (0,16)
                for i_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 179	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 128)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 106560)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 224)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for j_c.3 (0,4)
            for b_c.4 (0,2)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 180	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 128)) + ((((int)blockIdx.x) & 3) * 32)) + j_inner) + 4096)] = T_batch_matmul_NN_local[((((b_inner * 256) + (i_inner * 32)) + j_inner) + 768)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:8.42, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for b_c.4 (0,3)
                for i_c.4 (0,2)
                  for j_c.4 (0,8)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          for j.3 (0,32)
            T_batch_matmul_NN = ...

==================================================
No: 181	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 6) * 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 1) * 64)) + (((int)threadIdx.x) & 63)) + 73728)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 40)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,2)
            for b_c.4 (0,2)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 182	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t)threadIdx.x) & 63) >> 4) * 2048)) + (i_inner * 128)) + ((((int)blockIdx.x) & 1) * 64)) + (((int)threadIdx.x) & 15)) + 8240)] = T_batch_matmul_NN_local[(i_inner + 112)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,8)
          for k.2 (0,2)
            for i_c.4 (0,16)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 183	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 4) * 1024)) + (i_inner * 128)) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) & 15)) + 172032)] = T_batch_matmul_NN_local[(((b_inner * 8) + i_inner) + 176)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:3.34, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,4)
            for k.2 (0,4)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 184	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) & 63) >> 2) * 1024)) + ((((int)blockIdx.x) & 3) * 32)) + (((int)threadIdx.x) * 2)) + j_inner) + 896)] = T_batch_matmul_NN_local[(((b_inner * 2) + j_inner) + 28)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,2)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 185	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (((((int)threadIdx.x) & 7) >> 1) * 512)) + (i_inner * 128)) + ((((int)threadIdx.x) & 1) * 64)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 64) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,24)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for j_c.4 (0,32)
              T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 186	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
adIdx.x) & 31) >> 1) * 128)) + ((((int)blockIdx.x) & 3) * 32)) + ((((int)threadIdx.x) & 1) * 16)) + j_inner)] = T_batch_matmul_NN_local[((b_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    for k.0 (0,2)
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,8)
        for b_c.3 (0,2)
          for j_c.3 (0,8)
            for k.2 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 187	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
> 1) * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 4096)] = T_batch_matmul_NN_local[(((b_inner * 32) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,8)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for j_c.3 (0,8)
          for b_c.4 (0,12)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 188	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 1) * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 1) * 4)) + j_inner) + 6152)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 84)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for j_c.3 (0,4)
          for b_c.4 (0,3)
            T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 189	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,8)
            for i_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 190	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)blockIdx.x) & 3) * 4096)) + ((((int)threadIdx.x) >> 4) * 128)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 18496)] = T_batch_matmul_NN_local[(j_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 191	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 1) * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 2048)] = T_batch_matmul_NN_local[(((b_inner * 32) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.65, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for j_c.3 (0,4)
          for k.2 (0,16)
            for b_c.4 (0,2)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 192	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 8192)) + ((((int)threadIdx.x) >> 1) * 128)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 6208)] = T_batch_matmul_NN_local[(((b_inner * 32) + j_inner) + 2688)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:8.57, Tstamp:1688646958.85)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,8)
            for b_c.4 (0,12)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 33.62 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 193	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 96)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 96) + j_inner) + 3072)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,512)
        for ax0@ax1@ax2@.0.0 (0,576)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,6)
            for i_c.4 (0,32)
              for j_c.4 (0,48)
                T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,96)
          T_batch_matmul_NN = ...

==================================================
No: 194	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((int)threadIdx.x) >> 6) * 12288)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) & 63)) + 192)] = T_batch_matmul_NN_local[(i_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.45, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 195	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) & 3) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,1536)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for j_c.3 (0,2)
          for i_c.4 (0,4)
            for j_c.4 (0,12)
              T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 196	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) & 3) * 12)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.92, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,6)
              for i_c.4 (0,4)
                for j_c.4 (0,6)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,12)
          T_batch_matmul_NN = ...

==================================================
No: 197	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 6144)) + (i_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 3) * 16)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.49, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,1024)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,3)
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 198	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
072) + (i_inner * 768)) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) & 3) * 3)) + j_inner) + 84)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 84)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,2)
            for j_c.3 (0,3)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 199	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) % 12) * 4)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.52, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            vectorize ax0@ax1@ax2@.1 (0,16)
              p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,32)
            for i_c.4 (0,8)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 200	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[(((((((int)threadIdx.x) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 48)) + j_inner) + 49184)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 640)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:4.43, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,288)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,768)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for k.2 (0,3)
                for i_c.4 (0,2)
                  for j_c.4 (0,4)
                    T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 201	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
1536)];
      T_batch_matmul_NN[((((i_inner * 768) + (((int)threadIdx.x) * 12)) + j_inner) + 49536)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 2304)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,4)
        for i_c.3 (0,64)
          for j_c.3 (0,6)
            for k.2 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 202	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 63) * 4)) + j_inner) + 49152)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,8)
            for i_c.4 (0,4)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 203	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) % 12) * 4)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.65, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,22)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,8)
              for i_c.4 (0,8)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 204	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (((int)threadIdx.x) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 48) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.58, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,288)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,24)
            for i_c.4 (0,4)
              for j_c.4 (0,24)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,48)
          T_batch_matmul_NN = ...

==================================================
No: 205	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
readIdx.x) >> 2) * 12288) + (i_inner * 768)) + ((((int)threadIdx.x) & 3) * 96)) + j_inner) + 49536)] = T_batch_matmul_NN_local[(((i_inner * 96) + j_inner) + 4608)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,1536)
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for i_c.3 (0,16)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for j_c.4 (0,48)
              T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,96)
        T_batch_matmul_NN = ...

==================================================
No: 206	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + j_inner) + 64)] = T_batch_matmul_NN_local[(((i_inner * 64) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:2.53, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,2)
            for j_c.3 (0,32)
              for k.2 (0,3)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,64)
          T_batch_matmul_NN = ...

==================================================
No: 207	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
r * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 15) * 16)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.36, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,2)
            for i_c.4 (0,2)
              for j_c.4 (0,16)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 208	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 48) * 4)) + j_inner) + 192)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,512)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p0.shared = ...
        for k.1 (0,6)
          for j_c.3 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 209	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 6144)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 15) * 16)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,8)
          for k.2 (0,2)
            for j_c.4 (0,16)
              T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 210	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
3072)];
      T_batch_matmul_NN[((((i_inner * 768) + (((int)threadIdx.x) * 24)) + j_inner) + 49536)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 4608)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,384)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,4)
        for i_c.3 (0,64)
          for j_c.3 (0,6)
            for k.2 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 211	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x) >> 1) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 8)) + ((((int)threadIdx.x) & 1) * 4)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for k.1 (0,48)
        for i_c.4 (0,2)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 212	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 6) * 64)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(j_inner + 64)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.12, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,2)
            for j_c.4 (0,32)
              T_batch_matmul_NN.local = ...
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 213	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 48) * 4)) + j_inner) + 192)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      for k.0 (0,512)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,6)
          for j_c.3 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 214	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 31) * 6)) + j_inner) + 49152)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,512)
        for ax0@ax1@ax2@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,3)
          for i_c.3 (0,16)
            for j_c.3 (0,2)
              for k.2 (0,2)
                for i_c.4 (0,2)
                  for j_c.4 (0,3)
                    T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 215	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
52) + ((((int)threadIdx.x) >> 6) * 12288)) + (i_inner * 768)) + ((((int)threadIdx.x) & 63) * 12)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 12) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for i_c.3 (0,2)
        for k.2 (0,8)
          for i_c.4 (0,8)
            for j_c.4 (0,12)
              T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 216	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[((((((((int)threadIdx.x) / 6) * 768) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) % 6) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(j_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,16)
            T_batch_matmul_NN.local = ...
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 217	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[(((((i_inner * 768) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 512)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,22)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,128)
            for j_c.3 (0,2)
              for k.2 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,128)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 218	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.12, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 219	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 31) * 4)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.12, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,2)
            for k.2 (0,3)
              for i_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 220	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[((((((((int)threadIdx.x) >> 1) * 768) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 128)] = T_batch_matmul_NN_local[(j_inner + 64)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,16)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,32)
            for k.2 (0,3)
              T_batch_matmul_NN.local = ...
      for j.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 221	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) >> 2) * 1536)) + (i_inner * 768)) + ((((int)threadIdx.x) & 3) * 192)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 192) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,144)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,2)
            for j_c.3 (0,32)
              for k.2 (0,3)
                for j_c.4 (0,6)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,192)
          T_batch_matmul_NN = ...

==================================================
No: 222	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[(((((i_inner * 768) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) * 32)) + j_inner) + 256)] = T_batch_matmul_NN_local[(((i_inner * 32) + j_inner) + 8192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,1024)
        for ax0@ax1@ax2@.0.0 (0,288)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p0.shared = ...
        for k.1 (0,3)
          for i_c.3 (0,128)
            for j_c.4 (0,32)
              T_batch_matmul_NN.local = ...
      for i.3 (0,128)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 223	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) >> 4) * 12288)) + (i_inner * 768)) + ((((int)threadIdx.x) & 15) * 24)) + j_inner) + 384)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.43, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,288)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for k.1 (0,12)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for i_c.4 (0,4)
                for j_c.4 (0,6)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

==================================================
No: 224	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) % 6) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688646983.05)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,16)
            for k.2 (0,2)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 225	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) / 12) * 12288) + (i_inner * 768)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner) + 49728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 1792)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.31, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,1536)
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for i_c.4 (0,8)
            for j_c.4 (0,16)
              T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 226	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[((((((((int)threadIdx.x) >> 1) * 768) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) & 1) * 64)) + j_inner) + 256)] = T_batch_matmul_NN_local[(j_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,16)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,32)
            for k.2 (0,3)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 227	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)threadIdx.x) / 24) * 3072)) + (i_inner * 768)) + ((((int)threadIdx.x) % 24) * 8)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,8)
            for k.2 (0,2)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 228	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)threadIdx.x) >> 5) * 6144) + (i_inner * 768)) + ((((int)threadIdx.x) & 31) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
threadIdx.x b.2@i.2@j.2@ (0,512)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,1024)
    for ax0@ax1@ax2@.0.0 (0,5)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p1.shared = ...
    threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for k.1 (0,3)
      for i_c.4 (0,8)
        for j_c.4 (0,24)
          T_batch_matmul_NN.local = ...
  for i.3 (0,8)
    for j.3 (0,24)
      T_batch_matmul_NN = ...

==================================================
No: 229	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[j_inner];
    T_batch_matmul_NN[((((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 192)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(j_inner + 192)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,3)
            for j_c.4 (0,64)
              T_batch_matmul_NN.local = ...
      for j.3 (0,192)
        T_batch_matmul_NN = ...

==================================================
No: 230	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 31) * 24)) + ((((int)threadIdx.x) & 1) * 3)) + j_inner) + 24594)] = T_batch_matmul_NN_local[(j_inner + 21)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for k.1 (0,3)
          for k.2 (0,8)
            for j_c.4 (0,3)
              T_batch_matmul_NN.local = ...
      for j.3 (0,3)
        T_batch_matmul_NN = ...

==================================================
No: 231	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 63) * 4)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.16, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,16)
              for i_c.4 (0,8)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 232	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) % 3) * 32)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 32) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,11)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            vectorize ax0@ax1@ax2@.1 (0,8)
              p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,32)
            for i_c.4 (0,2)
              for j_c.4 (0,32)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 233	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 / 12) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 3) * 16)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,48)
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,32)
        for j_c.3 (0,4)
          for k.2 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 234	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...

    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 1) * 768) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 16)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.43, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,4)
        for j_c.3 (0,16)
          for k.2 (0,6)
            T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 235	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 6) * 49152) + ((((int)threadIdx.x) / 12) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,8)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 236	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) & 3) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.37, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,12)
                T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 237	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 48)) + ((((int)threadIdx.x) & 1) * 12)) + j_inner) + 73752)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 168)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.72, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,12)
              for k.2 (0,32)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,12)
          T_batch_matmul_NN = ...

==================================================
No: 238	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[((((((int)threadIdx.x) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.3 (0,16)
          for j_c.3 (0,8)
            for k.2 (0,6)
              T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 239	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[((((((((int)blockIdx.x) >> 2) * 1536) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) * 24)) + j_inner) + 768)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,6)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 240	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) >> 6) * 49152) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 241	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 5) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 31) * 6)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 6) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,256)
      for ax0@ax1@ax2@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,3)
        for i_c.3 (0,32)
          for k.2 (0,4)
            for i_c.4 (0,2)
              for j_c.4 (0,6)
                T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 242	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)threadIdx.x) >> 5) * 49152) + (i_inner * 768)) + ((((int)threadIdx.x) & 31) * 6)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 1152)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:2.86, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,256)
      for ax0@ax1@ax2@.0.0 (0,144)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for j_c.3 (0,6)
        for k.2 (0,12)
          for i_c.4 (0,64)
            T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 243	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) & 7) * 24)) + j_inner) + 24768)] = T_batch_matmul_NN_local[(j_inner + 72)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,8)
            for k.2 (0,3)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 244	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) >> 6) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) & 63) * 4)) + j_inner) + 25088)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 160)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,8)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,4)
            for k.2 (0,2)
              for i_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 245	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN_local[j_inner];
    T_batch_matmul_NN[((((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + j_inner) + 384)] = T_batch_matmul_NN_local[(j_inner + 384)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,384)
            for k.2 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,384)
        T_batch_matmul_NN = ...

==================================================
No: 246	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) * 12288) + ((((int)threadIdx.x) / 48) * 768)) + ((((int)threadIdx.x) % 48) * 4)) + j_inner) + 576)] = T_batch_matmul_NN_local[(j_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,4)
            T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 247	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
152) + ((((int)threadIdx.x) >> 6) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) & 63) * 12)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 12) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p0.shared = ...
      for k.1 (0,8)
        for i_c.4 (0,8)
          for j_c.4 (0,12)
            T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 248	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) * 8)) + j_inner) + 24608)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 1280)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:6.21, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,1152)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,1536)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,4)
            for k.2 (0,6)
              for i_c.4 (0,32)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 249	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN[((((((((int)threadIdx.x) >> 3) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) & 7)) + 8)] = T_batch_matmul_NN_local[(i_inner + 8)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,2)
            for k.2 (0,16)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 250	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 15) * 16)) + j_inner) + 49152)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 512)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.46, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,512)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.2 (0,6)
          for i_c.4 (0,32)
            for j_c.4 (0,16)
              T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 251	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) % 12) * 4)) + j_inner) + 144)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p0.shared = ...
        for k.2 (0,16)
          for i_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 252	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 2) * 3072) + (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 3) * 64)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 64) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,24)
        for j_c.3 (0,32)
          for i_c.4 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 253	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[((((((((int)threadIdx.x) >> 1) * 768) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 1) * 96)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(j_inner + 288)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,32)
            for k.2 (0,4)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for j.3 (0,96)
        T_batch_matmul_NN = ...

==================================================
No: 254	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 48) * 2)) + j_inner) + 73824)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,12)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for i_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 255	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) % 6) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,16)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 256	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688646991.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 15.08 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.T
==================================================
No: 257	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[(((((((int)threadIdx.x) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + j_inner) + 73740)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 168)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.72, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,32)
            for i_c.4 (0,2)
              for j_c.4 (0,6)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,12)
          T_batch_matmul_NN = ...

==================================================
No: 258	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[(((((i_inner * 768) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) * 4)) + j_inner) + 192)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 512)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for i_c.3 (0,128)
          for k.2 (0,3)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,128)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 259	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) / 48) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) % 48)) + 144)] = T_batch_matmul_NN_local[(i_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,3)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 260	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for k.2 (0,16)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 261	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) % 24) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.74, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,8)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for k.2 (0,48)
            for i_c.4 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 262	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) & 1) * 48)) + j_inner) + 36864)] = T_batch_matmul_NN_local[(j_inner + 144)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.62, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,12)
            for k.2 (0,4)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for j.3 (0,48)
        T_batch_matmul_NN = ...

==================================================
No: 263	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) >> 1) * 3072)) + (i_inner * 768)) + ((((int)threadIdx.x) & 1) * 96)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 96) + j_inner) + 1152)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,32)
              for k.2 (0,4)
                for i_c.4 (0,2)
                  for j_c.4 (0,3)
                    T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,96)
          T_batch_matmul_NN = ...

==================================================
No: 264	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
072) + (i_inner * 768)) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 64)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,3)
            for i_c.4 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 265	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[(((((((int)threadIdx.x) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 768)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,384)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,768)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,12)
          for k.2 (0,4)
            for i_c.4 (0,16)
              for j_c.4 (0,16)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 266	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) / 192) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) % 192) * 4)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.42, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for i_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 267	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) & 7) * 48)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 48) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,288)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for j_c.3 (0,6)
        for k.2 (0,12)
          for i_c.4 (0,2)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,48)
        T_batch_matmul_NN = ...

==================================================
No: 268	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) & 15) * 24)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,2)
              for j_c.4 (0,12)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

==================================================
No: 269	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) / 6) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 6) * 32)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 32) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,24)
          for i_c.4 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 270	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 271	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) / 192) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) % 192) * 2)) + j_inner) + 24960)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.41, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for k.2 (0,4)
                for i_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 272	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 1) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + ((((int)threadIdx.x) & 1) * 12)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 12) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.51, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,24)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for j_c.3 (0,2)
        for k.2 (0,32)
          for i_c.4 (0,2)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 273	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
     T_batch_matmul_NN[((((i_inner * 768) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,192)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p0.shared = ...
      for k.1 (0,24)
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for i_c.4 (0,64)
              T_batch_matmul_NN.local = ...
    for i.3 (0,128)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 274	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 2) * 49152) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 275	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
threadIdx.x b.2@i.2@j.2@ (0,64)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,64)
    for ax0@ax1@ax2@.0.0 (0,144)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,24)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p0.shared = ...
    for k.1 (0,3)
      for i_c.3 (0,8)
        for j_c.3 (0,32)
          for k.2 (0,4)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
  for i.3 (0,8)
    for j.3 (0,192)
      T_batch_matmul_NN = ...

==================================================
No: 276	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 5) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 24) * 32)) + (((int)threadIdx.x) & 31)) + 6144)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,12)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 277	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x) >> 2) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + ((((int)threadIdx.x) & 3) * 6)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 6) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,24)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.2 (0,32)
        for i_c.4 (0,2)
          for j_c.4 (0,6)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 278	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.3 (0,8)
          for k.2 (0,3)
            T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 279	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[((((((((int)threadIdx.x) >> 4) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) & 15)) + 73728)] = T_batch_matmul_NN_local[(i_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,12)
          for k.2 (0,4)
            for i_c.4 (0,16)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 280	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 1) * 24)) + j_inner) + 12432)] = T_batch_matmul_NN_local[(j_inner + 168)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,24)
            T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 281	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) * 6144) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)threadIdx.x) % 24) * 8)) + j_inner) + 576)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 282	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 64)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
        for j_c.3 (0,4)
          for k.2 (0,4)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 283	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) * 6144) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)threadIdx.x) % 24) * 8)) + j_inner) + 576)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 284	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
/ 12) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,6)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 285	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 49248)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 448)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.3 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 286	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) / 96) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) % 96)) + 96)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 287	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[((((((int)threadIdx.x) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for i_c.3 (0,16)
        for j_c.3 (0,8)
          for k.2 (0,12)
            T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 288	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8) * 49152) + ((((int)threadIdx.x) >> 4) * 12288)) + (i_inner * 768)) + ((((int)blockIdx.x) % 48) * 16)) + (((int)threadIdx.x) & 15))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647026.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.2 (0,2)
        for i_c.4 (0,16)
          T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 289	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
76) + ((((int)threadIdx.x) >> 5) * 12288)) + (i_inner * 768)) + ((((int)threadIdx.x) & 31) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.36, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for i_c.4 (0,16)
          for j_c.4 (0,24)
            T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 290	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) % 48) * 4)) + j_inner) + 49344)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.50, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for i_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 291	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + j_inner) + 160)] = T_batch_matmul_NN_local[(((i_inner * 32) + j_inner) + 320)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for i_c.4 (0,2)
                for j_c.4 (0,16)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 292	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,4)
          for k.2 (0,8)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 293	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 96)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for k.1 (0,6)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for k.2 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 294	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + ((((int)threadIdx.x) & 3) * 6)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 36)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.2 (0,32)
          for i_c.4 (0,2)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 295	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) % 48) * 4)) + j_inner) + 73920)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 224)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.39, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p0.shared = ...
        for j_c.3 (0,2)
          for i_c.4 (0,8)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 296	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 2) * 6144) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) * 96)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 96) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:2.02, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,2304)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,12)
        for j_c.3 (0,4)
          for k.2 (0,2)
            for i_c.4 (0,8)
              for j_c.4 (0,24)
                T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,96)
        T_batch_matmul_NN = ...

==================================================
No: 297	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) & 31) * 24)) + ((((int)threadIdx.x) % 3) * 8)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,384)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,8)
          for k.2 (0,24)
            for i_c.4 (0,2)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 298	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 36992)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 352)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for i_c.4 (0,8)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 299	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 12288)) + (i_inner * 768)) + ((((int)blockIdx.x) % 24) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      for k.2 (0,2)
        for i_c.4 (0,16)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 300	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 3) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) & 7) * 16)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for i_c.4 (0,4)
                for j_c.4 (0,8)
                  T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 301	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + (((int)blockIdx.x) * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 49152)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.3 (0,4)
              for k.2 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 302	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 7) * 3)) + j_inner) + 168)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 42)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,3)
            for k.2 (0,4)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 303	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 1) * 24)) + j_inner) + 12432)] = T_batch_matmul_NN_local[(j_inner + 168)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for j_c.3 (0,24)
            T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 304	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 305	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 6) * 64)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 64) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,48)
      for ax0@ax1@ax2@.0.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,11)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p0.shared = ...
      for j_c.3 (0,32)
        for k.2 (0,16)
          for i_c.4 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 306	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) & 3) * 24)) + j_inner) + 288)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 144)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:3.30, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,3)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for k.2 (0,4)
                for j_c.4 (0,12)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

==================================================
No: 307	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) >> 4) * 1536) + ((((int)threadIdx.x) / 48) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) % 48))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
        p0.shared = ...
      for k.1 (0,24)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 308	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
4576) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 6) * 4)) + j_inner) + 24)] = T_batch_matmul_NN_local[(j_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,12)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 309	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 310	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 1) * 24)) + j_inner) + 12432)] = T_batch_matmul_NN_local[(j_inner + 168)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,24)
            for k.2 (0,4)
              T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 311	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 7) * 16)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,4)
          for k.2 (0,4)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 312	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 12) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) % 12) * 32)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 32) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:3.71, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,16)
              for j_c.4 (0,16)
                T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 313	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 2) * 3072) + (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 3) * 48)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 48) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,8)
          for k.2 (0,8)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,48)
        T_batch_matmul_NN = ...

==================================================
No: 314	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 4)) + j_inner) + 256)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 315	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
adIdx.x) >> 2) * 1536)) + (i_inner * 768)) + ((((int)threadIdx.x) & 3) * 192)) + j_inner) + 36864)] = T_batch_matmul_NN_local[(((i_inner * 192) + j_inner) + 1152)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,144)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,8)
              for k.2 (0,3)
                for j_c.4 (0,24)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,192)
          T_batch_matmul_NN = ...

==================================================
No: 316	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 3) * 48)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 48) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.13, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,384)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,4)
            for j_c.3 (0,48)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,48)
          T_batch_matmul_NN = ...

==================================================
No: 317	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[(((((((int)threadIdx.x) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 128) + j_inner) + 256)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.76, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,16)
          for j_c.3 (0,16)
            for i_c.4 (0,2)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,128)
          T_batch_matmul_NN = ...

==================================================
No: 318	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)threadIdx.x) >> 5) * 3072)) + (i_inner * 768)) + ((((int)threadIdx.x) & 31) * 6)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 72)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,6)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 319	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ ((((int)threadIdx.x) >> 4) * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 15) * 8)) + j_inner) + 10752)] = T_batch_matmul_NN_local[(j_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for j_c.3 (0,4)
            for k.2 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 320	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[((((((((int)blockIdx.x) / 192) * 49152) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + j_inner) + 2)] = T_batch_matmul_NN_local[(j_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647032.10)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,8)
          for k.2 (0,3)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,2)
        T_batch_matmul_NN = ...

Time elapsed for measurement: 22.83 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 321	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 3584)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 224)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,8)
          for b_c.3 (0,2)
            for i_c.3 (0,4)
              for j_c.3 (0,2)
                for k.2 (0,2)
                  for b_c.4 (0,2)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 322	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
3) * 2048)) + ((((int)threadIdx.x) >> 1) * 64)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 16432)] = T_batch_matmul_NN_local[(((b_inner * 8) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for j_c.3 (0,8)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 323	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)blockIdx.x) >> 3) * 256)) + ((((int)threadIdx.x) >> 3) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) & 7))] = T_batch_matmul_NN_local[b_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for b_c.3 (0,3)
        for k.2 (0,2)
          for b_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for b.3 (0,12)
      T_batch_matmul_NN = ...

==================================================
No: 324	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (((int)blockIdx.x) * 2048)) + (((((int)threadIdx.x) & 63) >> 1) * 64)) + ((((int)threadIdx.x) & 1) * 16)) + j_inner) + 32)] = T_batch_matmul_NN_local[(j_inner + 16)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
        for j_c.3 (0,8)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 325	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 & 255) >> 1) * 64)) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 65552)] = T_batch_matmul_NN_local[(((b_inner * 8) + j_inner) + 80)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            vectorize ax0@ax1@ax2@.1 (0,8)
              p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,4)
            for b_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 326	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 26112)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 336)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.70, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,3)
            for i_c.3 (0,4)
              for k.2 (0,2)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,4)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 327	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x) & 7) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 1) * 32)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 128) + (i_inner * 32)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.50, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  threadIdx.x b.2@i.2@j.2@ (0,24)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          p0.shared = ...
      for b_c.3 (0,2)
        for j_c.3 (0,8)
          for k.2 (0,4)
            for i_c.4 (0,4)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,4)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 328	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 4) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 15)) + 1024)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,32)
              p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,4)
              for b_c.4 (0,12)
                for i_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for i.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 329	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
28)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 1024)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 24)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,6)
            for i_c.3 (0,2)
              for j_c.3 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,2)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 330	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
28)) + (i_inner * 64)) + ((((int)threadIdx.x) & 1) * 16)) + j_inner) + 32)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 16)) + j_inner) + 64)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,2)
            for j_c.3 (0,16)
              for b_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,2)
          for j.3 (0,16)
            T_batch_matmul_NN = ...

==================================================
No: 331	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 65664)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 40)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for k.2 (0,2)
                for b_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,2)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 332	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 17408)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 8)) + j_inner) + 96)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.72, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,8)
            for b_c.4 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,8)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,2)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 333	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
3) >> 5) * 4096)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 128) + (i_inner * 2)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for i_c.3 (0,64)
        for k.2 (0,4)
          for b_c.4 (0,3)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for i.3 (0,64)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 334	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner) + 12)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 335	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)threadIdx.x) & 15) >> 1) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 31) * 2)) + (((int)threadIdx.x) & 1)) + 2048)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,8)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 336	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 8192)) + ((((int)threadIdx.x) >> 6) * 512)) + (i_inner * 64)) + (((int)threadIdx.x) & 63)) + 30720)] = T_batch_matmul_NN_local[(((b_inner * 8) + i_inner) + 168)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,8)
            for k.2 (0,2)
              for b_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 337	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 6160)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 2)) + j_inner) + 1344)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,3)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for b_c.4 (0,2)
                for i_c.4 (0,8)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,16)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 338	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 128) + (i_inner * 2)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.74, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,64)
          for k.2 (0,2)
            for b_c.4 (0,3)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for i.3 (0,64)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 339	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x) & 3) * 2048)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 17920)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 448)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.80, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,512)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,8)
            for i_c.4 (0,8)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 340	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(int)threadIdx.x) & 7) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 7) * 8)) + j_inner) + 26628)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.44, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,24)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 341	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(int)threadIdx.x) >> 1) * 64)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner)] = T_batch_matmul_NN_local[((b_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,8)
        for j_c.3 (0,4)
          for b_c.4 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,4)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 342	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 65536)] = T_batch_matmul_NN_local[((((b_inner * 128) + (i_inner * 8)) + j_inner) + 1024)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:6.01, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,4)
          for i_c.3 (0,4)
            for j_c.3 (0,8)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,16)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 343	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) & 31) >> 2) * 256)) + (i_inner * 64)) + ((((int)threadIdx.x) & 3) * 16)) + j_inner) + 2048)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,16)
              for k.2 (0,16)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 344	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6)) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 1536)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 96)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.73, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for i_c.3 (0,4)
              for k.2 (0,4)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 345	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
15) >> 2) * 2048)) + (((int)threadIdx.x) * 64)) + ((((int)blockIdx.x) & 3) * 16)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((b_inner * 16) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for j_c.3 (0,16)
          for k.2 (0,4)
            for b_c.4 (0,3)
              T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 346	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
> 5) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 31)) + 3584)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,8)
          for b_c.3 (0,2)
            for i_c.3 (0,4)
              for k.2 (0,2)
                for b_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 347	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 32768)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 32)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,4)
            for k.2 (0,2)
              for b_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 348	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)threadIdx.x) & 7) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + j_inner) + 7168)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 224)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.46, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,24)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,8)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 349	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 17408)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 8)) + j_inner) + 384)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,8)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 350	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
kIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 1) * 16)) + j_inner) + 2048)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 16)) + j_inner) + 128)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,16)
            for b_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,16)
            T_batch_matmul_NN = ...

==================================================
No: 351	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)threadIdx.x) >> 3) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) & 7))] = T_batch_matmul_NN_local[((b_inner * 2) + i_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,3)
          for k.2 (0,2)
            for b_c.4 (0,4)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,12)
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 352	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (((((int)threadIdx.x) & 63) >> 1) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) & 1)) + 6)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647059.64)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,4)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 353	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 1) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 1) * 16)) + j_inner) + 20480)] = T_batch_matmul_NN_local[(((b_inner * 16) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.60, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for b_c.3 (0,2)
          for k.2 (0,8)
            for j_c.4 (0,16)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 354	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(int)threadIdx.x) & 15) >> 3) * 512)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 7)) + 33800)] = T_batch_matmul_NN_local[(i_inner + 88)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 355	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 4096)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 4)) + j_inner) + 16)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.56, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,2)
            for i_c.3 (0,2)
              for j_c.3 (0,2)
                for k.2 (0,4)
                  for j_c.4 (0,2)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,2)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 356	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x) >> 3) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 7)) + 8)] = T_batch_matmul_NN_local[(((b_inner * 2) + i_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for b_c.3 (0,6)
          for i_c.3 (0,2)
            for k.2 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 357	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 4096)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 2)) + j_inner) + 128)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.02, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
      for b_c.3 (0,2)
        for i_c.3 (0,32)
          for k.2 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,32)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 358	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 256)) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 32)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 96)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.55, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,4)
            for i_c.3 (0,2)
              for j_c.3 (0,2)
                for k.2 (0,2)
                  for b_c.4 (0,3)
                    for i_c.4 (0,2)
                      T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 359	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 8)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 32)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for b_c.4 (0,4)
                for i_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 360	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
lockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 1024)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 2)) + j_inner) + 96)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,3)
            for i_c.3 (0,8)
              for k.2 (0,4)
                for i_c.4 (0,2)
                  for j_c.4 (0,2)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,16)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 361	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (((((int)blockIdx.x) & 15) >> 2) * 2048)) + (((int)threadIdx.x) * 64)) + ((((int)blockIdx.x) & 3) * 16)) + j_inner) + 8204)] = T_batch_matmul_NN_local[(j_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for j_c.3 (0,4)
          for k.2 (0,16)
            T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 362	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 65568)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 4)) + j_inner) + 160)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.41, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for b_c.3 (0,4)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,2)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 363	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 8192) + (i_inner * 64)) + (((int)threadIdx.x) * 2)) + j_inner) + 77824)] = T_batch_matmul_NN_local[((((b_inner * 128) + (i_inner * 2)) + j_inner) + 2688)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for b_c.3 (0,3)
        for j_c.3 (0,2)
          for k.2 (0,4)
            for i_c.4 (0,64)
              T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for i.3 (0,64)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 364	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((int)threadIdx.x) >> 6) * 16384)) + (b_inner * 8192)) + ((((int)blockIdx.x) & 63) * 128)) + (((int)threadIdx.x) & 63)) + 64)] = T_batch_matmul_NN_local[(b_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,16)
          for k.2 (0,2)
            for b_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 365	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + ((((int)threadIdx.x) >> 4) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 15)) + 2048)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      for k.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 366	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 81920)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 40)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,8)
              p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 367	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 24)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 48)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.44, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,16)
            for b_c.4 (0,2)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 368	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 3584)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 224)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for k.2 (0,4)
                for b_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 369	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 3) * 512)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 7)) + 1032)] = T_batch_matmul_NN_local[(((b_inner * 8) + i_inner) + 72)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,2)
            for b_c.4 (0,3)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 370	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
15) >> 2) * 2048)) + (((int)threadIdx.x) * 64)) + ((((int)blockIdx.x) & 3) * 16)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((b_inner * 16) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for j_c.3 (0,16)
          for k.2 (0,4)
            for b_c.4 (0,3)
              T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 371	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 4) * 49152) + (b_inner * 8192)) + (i_inner * 64)) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) & 15))] = T_batch_matmul_NN_local[((b_inner * 128) + i_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,128)
        for k.2 (0,4)
          for b_c.4 (0,6)
            T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,128)
        T_batch_matmul_NN = ...

==================================================
No: 372	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 512)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 2)) + j_inner) + 48)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.39, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,3)
            for i_c.3 (0,8)
              for k.2 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 373	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 4) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 1536)] = T_batch_matmul_NN_local[(((b_inner * 2) + j_inner) + 36)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,6)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 374	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 32)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 4)) + j_inner) + 384)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,12)
          for i_c.3 (0,8)
            for j_c.3 (0,4)
              for k.2 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for i.3 (0,8)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 375	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)blockIdx.x) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 4096)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 2)) + j_inner) + 128)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.75, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,32)
            for k.2 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,32)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 376	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x) & 15) * 512)) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 32768)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 377	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 >> 4) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 15)) + 1024)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for i_c.3 (0,2)
              for k.2 (0,4)
                for i_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 378	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
r * 64)) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 256) + (i_inner * 4)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for b_c.3 (0,3)
        for j_c.3 (0,2)
          for b_c.4 (0,2)
            for i_c.4 (0,64)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,64)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 379	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 1032)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,4)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 380	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 64)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 32768)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 32)) + j_inner) + 256)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,2)
          for j.3 (0,32)
            T_batch_matmul_NN = ...

==================================================
No: 381	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 768)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 72)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,6)
            for k.2 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,2)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 382	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 65536)] = T_batch_matmul_NN_local[((((b_inner * 128) + (i_inner * 8)) + j_inner) + 1024)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:5.90, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,4)
          for i_c.3 (0,4)
            for j_c.3 (0,8)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,16)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 383	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(int)threadIdx.x) >> 3) * 512)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 2048)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,2)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,8)
            for j_c.3 (0,4)
              for k.2 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 384	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dx.x) >> 3) * 256)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 32) + (i_inner * 8)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647067.97)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,8)
              for b_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,4)
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 16.58 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 385	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 386	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 387	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 388	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,128)
    T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 389	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 390	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,3072)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,64)
    T_broadcast_to = ...

==================================================
No: 391	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
1f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 128) + ax3_outer)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,128)
    T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for k.0 (0,128)
    T_softmax_maxelem = ...
  for ax3.0 (0,128)
    T_broadcast_to = ...

==================================================
No: 392	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 393	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,16384)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,12)
    T_broadcast_to = ...

==================================================
No: 394	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,49152)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,4)
    T_broadcast_to = ...

==================================================
No: 395	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.402823e+38f;
  for (int k = 0; k < 128; ++k) {
    T_softmax_maxelem[((int)blockIdx.x)] = max(T_softmax_maxelem[((int)blockIdx.x)], p0[((((int)blockIdx.x) * 128) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 396	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 16)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_maxelem = ...
  for ax3.0 (0,8)
    threadIdx.x ax3.1 (0,16)
      T_broadcast_to = ...

==================================================
No: 397	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.402823e+38f;
  for (int k = 0; k < 128; ++k) {
    T_softmax_maxelem[((int)blockIdx.x)] = max(T_softmax_maxelem[((int)blockIdx.x)], p0[((((int)blockIdx.x) * 128) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 398	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 399	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 8)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_maxelem = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 400	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 401	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 402	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 403	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,96)
  threadIdx.x i0@i1@i2@.1 (0,16)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 404	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_maxelem = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 405	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 406	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.402823e+38f;
  for (int k = 0; k < 128; ++k) {
    T_softmax_maxelem[((int)blockIdx.x)] = max(T_softmax_maxelem[((int)blockIdx.x)], p0[((((int)blockIdx.x) * 128) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 407	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 408	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,49152)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,4)
    T_broadcast_to = ...

==================================================
No: 409	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 8)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_maxelem = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 410	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,768)
  threadIdx.x i0@i1@i2@.1 (0,2)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 411	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_expsum = ...
  for ax3.0 (0,8)
    threadIdx.x ax3.1 (0,16)
      T_broadcast_to = ...

==================================================
No: 412	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 413	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 64)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 414	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,49152)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,4)
    T_broadcast_to = ...

==================================================
No: 415	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
e-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 4)])) / T_softmax_expsum[(((int)blockIdx.x) >> 4)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,768)
  threadIdx.x i0@i1@i2@.1 (0,2)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,24576)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,8)
    T_broadcast_to = ...

==================================================
No: 416	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647092.91)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 417	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 64)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 418	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 419	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 8)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_maxelem = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 420	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 32)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 421	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 64)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 422	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 2)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_maxelem = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 423	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 32)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 424	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
e-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 6)])) / T_softmax_expsum[(((int)blockIdx.x) >> 6)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,96)
  threadIdx.x i0@i1@i2@.1 (0,16)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,768)
  threadIdx.x i0@i1@i2@.1 (0,2)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,98304)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,2)
    T_broadcast_to = ...

==================================================
No: 425	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 426	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,24576)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,8)
    T_broadcast_to = ...

==================================================
No: 427	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,96)
  threadIdx.x i0@i1@i2@.1 (0,16)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 428	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 red_buf0[0], 1, 32);
  red_buf0[0] = max(red_buf0[0], t0[0]);
  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);
  T_softmax_maxelem[((int)blockIdx.x)] = red_buf0[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,3072)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,64)
    T_broadcast_to = ...

==================================================
No: 429	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 430	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_maxelem = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 431	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,24576)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,8)
    T_broadcast_to = ...

==================================================
No: 432	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 433	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 4)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_maxelem = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 434	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 64)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 435	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,98304)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,2)
    T_broadcast_to = ...

==================================================
No: 436	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 16
  for k.0 (0,128)
    T_softmax_expsum = ...
  for ax3.0 (0,128)
    T_broadcast_to = ...

==================================================
No: 437	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
e-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 5)])) / T_softmax_expsum[(((int)blockIdx.x) >> 5)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,49152)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,4)
    T_broadcast_to = ...

==================================================
No: 438	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,196608)
  T_broadcast_to = ...

==================================================
No: 439	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 2) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,768)
  threadIdx.x i0@i1@i2@.1 (0,2)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 440	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 441	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 16)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_maxelem = ...
  for ax3.0 (0,8)
    threadIdx.x ax3.1 (0,16)
      T_broadcast_to = ...

==================================================
No: 442	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 16)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,8)
    threadIdx.x k.1 (0,16)
      T_softmax_maxelem = ...
  for ax3.0 (0,8)
    threadIdx.x ax3.1 (0,16)
      T_broadcast_to = ...

==================================================
No: 443	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 444	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 2048) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,768)
  threadIdx.x i0@i1@i2@.1 (0,2)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,96)
  threadIdx.x i0@i1@i2@.1 (0,16)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 445	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 64)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_expsum = ...
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
  for ax3.0 (0,2)
    threadIdx.x ax3.1 (0,64)
      T_broadcast_to = ...

==================================================
No: 446	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((int)blockIdx.x) >> 2)])) / T_softmax_expsum[(((int)blockIdx.x) >> 2)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 447	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 32)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 1024
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 448	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647095.42)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

Time elapsed for measurement: 5.05 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 449	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 450	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 451	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 452	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 453	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 454	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 455	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 456	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 457	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 458	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 459	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 460	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 461	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 462	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 463	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 464	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 465	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 466	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 467	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 468	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 469	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 470	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 471	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 472	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 473	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 474	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 475	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 476	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 477	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 478	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 479	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647112.45)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 480	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
] = red_buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (p0_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647112.45)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  p0_red auto_unroll: 512
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      p0_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 481	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 482	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
] = red_buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (p0_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  p0_red auto_unroll: 16
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      p0_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 483	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 484	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 485	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 486	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 487	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 488	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 489	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 490	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 491	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 492	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 493	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 494	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 495	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 496	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 497	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 498	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 499	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 500	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 501	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 502	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 503	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 504	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 505	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 506	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 507	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 508	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 509	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 510	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 511	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
] = red_buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (p0_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647114.87)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  p0_red auto_unroll: 64
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      p0_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 512	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647114.87)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

Time elapsed for measurement: 4.88 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 513	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
e+00f, min(9.000000e+00f, (T_matmul_NT[ax1_inner] + p2[((((int)threadIdx.x) * 4) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
threadIdx.x ax0.2@ax1.2@ (0,192)
  T_matmul_NT auto_unroll: 16
  for k.0 (0,768)
    for ax0@ax1@.0.0 (0,4)
      threadIdx.x ax0@ax1@.0.1 (0,192)
        p1.shared = ...
    threadIdx.x ax0@ax1@.0.1 (0,192)
      p0.shared = ...
    for j.3 (0,4)
      T_matmul_NT = ...
  for ax1.3 (0,4)
    T_fast_tanh = ...

==================================================
No: 514	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner + 20)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 4)) + ax1_inner) + 320)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,6)
    threadIdx.x ax0.2@ax1.2@ (0,16)
      for k.0 (0,768)
        for ax0@ax1@.0.0 (0,24)
          threadIdx.x ax0@ax1@.0.1 (0,16)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,16)
          p0.shared = ...
        for j.3 (0,4)
          T_matmul_NT = ...
      for ax1.3 (0,4)
        T_fast_tanh = ...

==================================================
No: 515	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+00f, min(9.000000e+00f, (T_matmul_NT[ax1_inner] + p2[((((int)threadIdx.x) * 16) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.39, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
threadIdx.x ax0.2@ax1.2@ (0,48)
  T_matmul_NT auto_unroll: 1024
  for k.0 (0,64)
    for ax0@ax1@.0.0 (0,192)
      threadIdx.x ax0@ax1@.0.1 (0,48)
        p1.shared = ...
    threadIdx.x ax0@ax1@.0.1 (0,48)
      p0.shared = ...
    for k.1 (0,2)
      for k.2 (0,6)
        for j.4 (0,16)
          T_matmul_NT = ...
  for ax1.3 (0,16)
    T_fast_tanh = ...

==================================================
No: 516	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
f, min(9.000000e+00f, (T_matmul_NT[7] + p2[(((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) + 42)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,16)
  vthread ax0.1@ax1.1@ (0,8)
    threadIdx.x ax0.2@ax1.2@ (0,6)
      T_matmul_NT auto_unroll: 64
      for k.0 (0,32)
        for ax0@ax1@.0.0 (0,192)
          threadIdx.x ax0@ax1@.0.1 (0,6)
            p1.shared = ...
        for ax0@ax1@.0.0 (0,4)
          threadIdx.x ax0@ax1@.0.1 (0,6)
            p0.shared = ...
        for k.1 (0,3)
          for k.2 (0,8)
            T_matmul_NT = ...
      T_fast_tanh = ...

==================================================
No: 517	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
1_inner + 4)] + p2[((((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 2)) + ax1_inner) + 16)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,32)
  vthread ax0.1@ax1.1@ (0,3)
    threadIdx.x ax0.2@ax1.2@ (0,4)
      for k.0 (0,256)
        for ax0@ax1@.0.0 (0,18)
          threadIdx.x ax0@ax1@.0.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,4)
          vectorize ax0@ax1@.1 (0,3)
            p0.shared = ...
        for k.2 (0,3)
          for j.4 (0,2)
            T_matmul_NT = ...
      for ax1.3 (0,2)
        T_fast_tanh = ...

==================================================
No: 518	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 4)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 4)) + ax1_inner) + 192)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,48)
      T_matmul_NT auto_unroll: 64
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,64)
          threadIdx.x ax0@ax1@.0.1 (0,48)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,48)
          vectorize ax0@ax1@.1 (0,6)
            p0.shared = ...
        for j.3 (0,4)
          for k.2 (0,8)
            T_matmul_NT = ...
      for ax1.3 (0,4)
        T_fast_tanh = ...

==================================================
No: 519	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,16)
      for ax0@ax1@.0.0 (0,192)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,4)
          p0.shared = ...
      for k.2 (0,48)
        for j.4 (0,4)
          T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 520	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0000e+00f, (T_matmul_NT[(ax1_inner + 16)] + p2[(((((int)threadIdx.x) * 8) + ax1_inner) + 512)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,3)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@.0.0 (0,24)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for j.3 (0,4)
        for j.4 (0,2)
          T_matmul_NT = ...
    for ax1.3 (0,8)
      T_fast_tanh = ...

==================================================
No: 521	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 96) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,8)
  threadIdx.x ax0.2@ax1.2@ (0,96)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,192)
      threadIdx.x ax0@ax1@.0.1 (0,96)
        vectorize ax0@ax1@.1 (0,16)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,96)
        p0.shared = ...
      for k.1 (0,4)
        T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 522	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 2)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,12)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 512
    for k.0 (0,192)
      for ax0@ax1@.0.0 (0,8)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        vectorize ax0@ax1@.1 (0,4)
          p0.shared = ...
      for k.1 (0,2)
        for j.3 (0,2)
          for k.2 (0,2)
            T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 523	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 6)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    T_matmul_NT auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@.0.0 (0,72)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          vectorize ax0@ax1@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        p0.shared = ...
      for k.1 (0,3)
        for j.3 (0,2)
          for k.2 (0,8)
            for j.4 (0,3)
              T_matmul_NT = ...
    for ax1.3 (0,6)
      T_fast_tanh = ...

==================================================
No: 524	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,3)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    for k.0 (0,256)
      for ax0@ax1@.0.0 (0,12)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        p0.shared = ...
      for k.1 (0,3)
        for j.3 (0,2)
          for j.4 (0,2)
            T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 525	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,96)
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,48)
        threadIdx.x ax0@ax1@.0.1 (0,96)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,96)
        vectorize ax0@ax1@.1 (0,6)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,3)
          for j.4 (0,4)
            T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 526	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 96) + (((int)threadIdx.x) * 2)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,8)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    T_matmul_NT auto_unroll: 1024
    for k.0 (0,128)
      for ax0@ax1@.0.0 (0,4)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          vectorize ax0@ax1@.1 (0,3)
            p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        p0.shared = ...
      for k.1 (0,3)
        for k.2 (0,2)
          for j.4 (0,2)
            T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 527	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 4)] + p2[((((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + ax1_inner) + 128)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.44, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  vthread ax0.1@ax1.1@ (0,3)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      T_matmul_NT auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@.0.0 (0,144)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          vectorize ax0@ax1@.1 (0,2)
            p0.shared = ...
        for k.1 (0,24)
          for j.4 (0,2)
            T_matmul_NT = ...
      for ax1.3 (0,2)
        T_fast_tanh = ...

==================================================
No: 528	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, (T_matmul_NT[(ax1_inner + 12)] + p2[(((((int)threadIdx.x) * 12) + ax1_inner) + 384)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    for k.0 (0,128)
      for ax0@ax1@.0.0 (0,36)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          vectorize ax0@ax1@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,3)
          for j.4 (0,12)
            T_matmul_NT = ...
    for ax1.3 (0,12)
      T_fast_tanh = ...

==================================================
No: 529	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 min(9.000000e+00f, (T_matmul_NT[3] + p2[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 288)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,4)
    threadIdx.x ax0.2@ax1.2@ (0,96)
      for k.0 (0,48)
        for ax0@ax1@.0.0 (0,16)
          threadIdx.x ax0@ax1@.0.1 (0,96)
            vectorize ax0@ax1@.1 (0,4)
              p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,96)
          vectorize ax0@ax1@.1 (0,8)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,8)
            T_matmul_NT = ...
      T_fast_tanh = ...

==================================================
No: 530	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 8)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,48)
      for ax0@ax1@.0.0 (0,128)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,4)
          for j.4 (0,8)
            T_matmul_NT = ...
    for ax1.3 (0,8)
      T_fast_tanh = ...

==================================================
No: 531	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0000e+00f, (T_matmul_NT[(ax1_inner + 22)] + p2[(((((int)threadIdx.x) * 2) + ax1_inner) + 704)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,12)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@.0.0 (0,192)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.1 (0,8)
        for j.4 (0,2)
          T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 532	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 min(9.000000e+00f, (T_matmul_NT[5] + p2[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 320)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,6)
    threadIdx.x ax0.2@ax1.2@ (0,64)
      T_matmul_NT auto_unroll: 64
      for k.0 (0,192)
        for ax0@ax1@.0.0 (0,24)
          threadIdx.x ax0@ax1@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,64)
          vectorize ax0@ax1@.1 (0,2)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_matmul_NT = ...
      T_fast_tanh = ...

==================================================
No: 533	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 8)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    T_matmul_NT auto_unroll: 1024
    for k.0 (0,384)
      for ax0@ax1@.0.0 (0,16)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.2 (0,2)
        for j.4 (0,8)
          T_matmul_NT = ...
    for ax1.3 (0,8)
      T_fast_tanh = ...

==================================================
No: 534	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,12)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,192)
      for ax0@ax1@.0.0 (0,4)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        vectorize ax0@ax1@.1 (0,4)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,2)
          T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 535	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 6)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 6)) + ax1_inner) + 192)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      T_matmul_NT auto_unroll: 64
      for k.0 (0,192)
        for ax0@ax1@.0.0 (0,48)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          vectorize ax0@ax1@.1 (0,2)
            p0.shared = ...
        for k.2 (0,4)
          for j.4 (0,6)
            T_matmul_NT = ...
      for ax1.3 (0,6)
        T_fast_tanh = ...

==================================================
No: 536	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
, min(9.000000e+00f, (T_matmul_NT[3] + p2[(((((int)blockIdx.x) * 128) + ((int)threadIdx.x)) + 96)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,6)
  vthread ax0.1@ax1.1@ (0,4)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      T_matmul_NT auto_unroll: 16
      for k.0 (0,384)
        for ax0@ax1@.0.0 (0,8)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p0.shared = ...
        for k.2 (0,2)
          T_matmul_NT = ...
      T_fast_tanh = ...

==================================================
No: 537	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0000e+00f, (T_matmul_NT[(ax1_inner + 10)] + p2[(((((int)threadIdx.x) * 2) + ax1_inner) + 640)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,6)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    for k.0 (0,256)
      for ax0@ax1@.0.0 (0,36)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        vectorize ax0@ax1@.1 (0,3)
          p0.shared = ...
      for j.3 (0,2)
        for k.2 (0,3)
          T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 538	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner + 6)] + p2[((((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + ax1_inner) + 96)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,16)
      T_matmul_NT auto_unroll: 512
      for k.0 (0,48)
        for ax0@ax1@.0.0 (0,192)
          threadIdx.x ax0@ax1@.0.1 (0,16)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,16)
          p0.shared = ...
        for k.1 (0,8)
          for j.3 (0,3)
            for k.2 (0,2)
              for j.4 (0,2)
                T_matmul_NT = ...
      for ax1.3 (0,6)
        T_fast_tanh = ...

==================================================
No: 539	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+00f, min(9.000000e+00f, (T_matmul_NT[ax1_inner] + p2[((((int)threadIdx.x) * 16) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
threadIdx.x ax0.2@ax1.2@ (0,48)
  T_matmul_NT auto_unroll: 1024
  for k.0 (0,64)
    for ax0@ax1@.0.0 (0,96)
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,2)
          p1.shared = ...
    threadIdx.x ax0@ax1@.0.1 (0,48)
      p0.shared = ...
    for k.1 (0,4)
      for k.2 (0,3)
        for j.4 (0,16)
          T_matmul_NT = ...
  for ax1.3 (0,16)
    T_fast_tanh = ...

==================================================
No: 540	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,48)
      for ax0@ax1@.0.0 (0,48)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          vectorize ax0@ax1@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for k.1 (0,8)
        for j.3 (0,6)
          for k.2 (0,2)
            for j.4 (0,2)
              T_matmul_NT = ...
    for ax1.3 (0,12)
      T_fast_tanh = ...

==================================================
No: 541	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0000e+00f, (T_matmul_NT[(ax1_inner + 14)] + p2[(((((int)threadIdx.x) * 2) + ax1_inner) + 672)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,8)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,192)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,6)
          for j.4 (0,2)
            T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 542	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 3)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 3)) + ax1_inner) + 192)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,64)
      T_matmul_NT auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@.0.0 (0,18)
          threadIdx.x ax0@ax1@.0.1 (0,64)
            vectorize ax0@ax1@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p0.shared = ...
        for k.1 (0,3)
          for j.3 (0,3)
            for k.2 (0,2)
              T_matmul_NT = ...
      for ax1.3 (0,3)
        T_fast_tanh = ...

==================================================
No: 543	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,12)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,6)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          vectorize ax0@ax1@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,6)
          T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 544	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
f, min(9.000000e+00f, (T_matmul_NT[1] + p2[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) + 32)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647139.12)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,12)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      for k.0 (0,192)
        for ax0@ax1@.0.0 (0,8)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_matmul_NT = ...
      T_fast_tanh = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 545	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+00f, min(9.000000e+00f, (T_matmul_NT[ax1_inner] + p2[((((int)threadIdx.x) * 12) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
threadIdx.x ax0.2@ax1.2@ (0,64)
  T_matmul_NT auto_unroll: 64
  for k.0 (0,768)
    threadIdx.x ax0@ax1@.0.1 (0,64)
      vectorize ax0@ax1@.1 (0,48)
        p1.shared = ...
    threadIdx.x ax0@ax1@.0.1 (0,64)
      p0.shared = ...
    for j.3 (0,2)
      for j.4 (0,6)
        T_matmul_NT = ...
  for ax1.3 (0,12)
    T_fast_tanh = ...

==================================================
No: 546	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner + 18)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 6)) + ax1_inner) + 288)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.41, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,4)
    threadIdx.x ax0.2@ax1.2@ (0,16)
      T_matmul_NT auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,48)
          threadIdx.x ax0@ax1@.0.1 (0,16)
            vectorize ax0@ax1@.1 (0,4)
              p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,16)
          p0.shared = ...
        for j.3 (0,6)
          for k.2 (0,8)
            T_matmul_NT = ...
      for ax1.3 (0,6)
        T_fast_tanh = ...

==================================================
No: 547	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 2)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,12)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 512
    for k.0 (0,96)
      for ax0@ax1@.0.0 (0,16)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.1 (0,2)
        for j.3 (0,2)
          for k.2 (0,4)
            T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 548	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,72)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for k.1 (0,12)
        for j.4 (0,6)
          T_matmul_NT = ...
    for ax1.3 (0,6)
      T_fast_tanh = ...

==================================================
No: 549	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 48) + (((int)threadIdx.x) * 24)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,16)
  threadIdx.x ax0.2@ax1.2@ (0,2)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,4)
      for ax0@ax1@.0.0 (0,4608)
        threadIdx.x ax0@ax1@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@.0.0 (0,96)
        threadIdx.x ax0@ax1@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,192)
        for j.3 (0,24)
          T_matmul_NT = ...
    for ax1.3 (0,24)
      T_fast_tanh = ...

==================================================
No: 550	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 48) + (((int)threadIdx.x) * 12)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,16)
  threadIdx.x ax0.2@ax1.2@ (0,4)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,48)
      for ax0@ax1@.0.0 (0,192)
        threadIdx.x ax0@ax1@.0.1 (0,4)
          p1.shared = ...
      for ax0@ax1@.0.0 (0,4)
        threadIdx.x ax0@ax1@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,8)
        for j.3 (0,6)
          for k.2 (0,2)
            for j.4 (0,2)
              T_matmul_NT = ...
    for ax1.3 (0,12)
      T_fast_tanh = ...

==================================================
No: 551	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 6)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 2)) + ax1_inner) + 288)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,4)
    threadIdx.x ax0.2@ax1.2@ (0,48)
      T_matmul_NT auto_unroll: 16
      for k.0 (0,384)
        for ax0@ax1@.0.0 (0,16)
          threadIdx.x ax0@ax1@.0.1 (0,48)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,48)
          vectorize ax0@ax1@.1 (0,2)
            p0.shared = ...
        for k.2 (0,2)
          for j.4 (0,2)
            T_matmul_NT = ...
      for ax1.3 (0,2)
        T_fast_tanh = ...

==================================================
No: 552	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,64)
  threadIdx.x ax0.2@ax1.2@ (0,3)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,768)
      for ax0@ax1@.0.0 (0,4)
        threadIdx.x ax0@ax1@.0.1 (0,3)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,3)
        p0.shared = ...
      for j.4 (0,4)
        T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 553	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 2)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,24)
  threadIdx.x ax0.2@ax1.2@ (0,16)
    T_matmul_NT auto_unroll: 1024
    for k.0 (0,24)
      for ax0@ax1@.0.0 (0,16)
        threadIdx.x ax0@ax1@.0.1 (0,16)
          vectorize ax0@ax1@.1 (0,4)
            p1.shared = ...
      for ax0@ax1@.0.0 (0,2)
        threadIdx.x ax0@ax1@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,32)
        for j.3 (0,2)
          T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 554	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner + 3)] + p2[((((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 3)) + ax1_inner) + 96)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      T_matmul_NT auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,48)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          vectorize ax0@ax1@.1 (0,2)
            p0.shared = ...
        for k.1 (0,4)
          for j.3 (0,3)
            for k.2 (0,2)
              T_matmul_NT = ...
      for ax1.3 (0,3)
        T_fast_tanh = ...

==================================================
No: 555	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 8)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,3)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,48)
      for ax0@ax1@.0.0 (0,128)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,8)
          for j.4 (0,8)
            T_matmul_NT = ...
    for ax1.3 (0,8)
      T_fast_tanh = ...

==================================================
No: 556	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,24)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,24)
      for ax0@ax1@.0.0 (0,32)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        vectorize ax0@ax1@.1 (0,4)
          p0.shared = ...
      for k.1 (0,32)
        T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 557	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,384)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,384)
      for ax0@ax1@.0.0 (0,2)
        threadIdx.x ax0@ax1@.0.1 (0,384)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,384)
        p0.shared = ...
      for k.2 (0,2)
        T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 558	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0000e+00f, (T_matmul_NT[(ax1_inner + 14)] + p2[(((((int)threadIdx.x) * 2) + ax1_inner) + 672)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,8)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,48)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          vectorize ax0@ax1@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,3)
          for j.4 (0,2)
            T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 559	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,6)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,12)
      for ax0@ax1@.0.0 (0,256)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@.0.0 (0,2)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p0.shared = ...
      for j.3 (0,4)
        for k.2 (0,64)
          T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 560	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,3)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,48)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        vectorize ax0@ax1@.1 (0,6)
          p0.shared = ...
      for k.1 (0,3)
        for k.2 (0,4)
          for j.4 (0,4)
            T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 561	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 min(9.000000e+00f, (T_matmul_NT[3] + p2[(((((int)blockIdx.x) * 384) + ((int)threadIdx.x)) + 288)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,4)
    threadIdx.x ax0.2@ax1.2@ (0,96)
      T_matmul_NT auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,32)
          threadIdx.x ax0@ax1@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,96)
          p0.shared = ...
        for k.2 (0,8)
          T_matmul_NT = ...
      T_fast_tanh = ...

==================================================
No: 562	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 6)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    T_matmul_NT auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@.0.0 (0,144)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        p0.shared = ...
      for k.1 (0,3)
        for j.3 (0,2)
          for k.2 (0,8)
            for j.4 (0,3)
              T_matmul_NT = ...
    for ax1.3 (0,6)
      T_fast_tanh = ...

==================================================
No: 563	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    for k.0 (0,64)
      for ax0@ax1@.0.0 (0,72)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.1 (0,4)
        for j.3 (0,2)
          for k.2 (0,3)
            for j.4 (0,3)
              T_matmul_NT = ...
    for ax1.3 (0,6)
      T_fast_tanh = ...

==================================================
No: 564	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,192)
  threadIdx.x ax0.2@ax1.2@ (0,4)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,192)
      threadIdx.x ax0@ax1@.0.1 (0,4)
        vectorize ax0@ax1@.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,4)
        vectorize ax0@ax1@.1 (0,2)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,2)
          T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 565	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 4)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 4)) + ax1_inner) + 192)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,48)
      T_matmul_NT auto_unroll: 64
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,11)
          threadIdx.x ax0@ax1@.0.1 (0,48)
            vectorize ax0@ax1@.1 (0,6)
              p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,48)
          p0.shared = ...
        for j.3 (0,4)
          for k.2 (0,8)
            T_matmul_NT = ...
      for ax1.3 (0,4)
        T_fast_tanh = ...

==================================================
No: 566	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 4)] + p2[((((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 2)) + ax1_inner) + 128)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,4)
  vthread ax0.1@ax1.1@ (0,3)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      for k.0 (0,16)
        for ax0@ax1@.0.0 (0,288)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          vectorize ax0@ax1@.1 (0,24)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,24)
            for j.4 (0,2)
              T_matmul_NT = ...
      for ax1.3 (0,2)
        T_fast_tanh = ...

==================================================
No: 567	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
00000e+00f, (T_matmul_NT[(ax1_inner + 6)] + p2[(((((int)threadIdx.x) * 6) + ax1_inner) + 384)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,64)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,128)
      for ax0@ax1@.0.0 (0,72)
        threadIdx.x ax0@ax1@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,64)
        p0.shared = ...
      for j.3 (0,6)
        for k.2 (0,6)
          T_matmul_NT = ...
    for ax1.3 (0,6)
      T_fast_tanh = ...

==================================================
No: 568	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 2)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,12)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@.0.0 (0,2)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for j.4 (0,2)
        T_matmul_NT = ...
    for ax1.3 (0,2)
      T_fast_tanh = ...

==================================================
No: 569	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,16)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,256)
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,3)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        vectorize ax0@ax1@.1 (0,3)
          p0.shared = ...
      for k.2 (0,3)
        T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 570	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ner + 48)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 48)) + ax1_inner) + 192)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.75, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,4)
      T_matmul_NT auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,768)
          threadIdx.x ax0@ax1@.0.1 (0,4)
            p1.shared = ...
        for ax0@ax1@.0.0 (0,2)
          threadIdx.x ax0@ax1@.0.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,4)
            for j.4 (0,48)
              T_matmul_NT = ...
      for ax1.3 (0,48)
        T_fast_tanh = ...

==================================================
No: 571	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, min(9.000000e+00f, (T_matmul_NT[0] + p2[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,16)
  threadIdx.x ax0.2@ax1.2@ (0,48)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@.0.0 (0,24)
        threadIdx.x ax0@ax1@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,48)
        p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,3)
          T_matmul_NT = ...
    T_fast_tanh = ...

==================================================
No: 572	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
000e+00f, (T_matmul_NT[(ax1_inner + 12)] + p2[(((((int)threadIdx.x) * 12) + ax1_inner) + 384)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
vthread ax0.1@ax1.1@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    for k.0 (0,128)
      for ax0@ax1@.0.0 (0,144)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,3)
          for j.4 (0,12)
            T_matmul_NT = ...
    for ax1.3 (0,12)
      T_fast_tanh = ...

==================================================
No: 573	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 12) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,64)
  threadIdx.x ax0.2@ax1.2@ (0,3)
    T_matmul_NT auto_unroll: 16
    for k.0 (0,96)
      for ax0@ax1@.0.0 (0,32)
        threadIdx.x ax0@ax1@.0.1 (0,3)
          p1.shared = ...
      for ax0@ax1@.0.0 (0,3)
        threadIdx.x ax0@ax1@.0.1 (0,3)
          p0.shared = ...
      for k.2 (0,8)
        for j.4 (0,4)
          T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 574	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 4)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  threadIdx.x ax0.2@ax1.2@ (0,96)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,192)
      for ax0@ax1@.0.0 (0,16)
        threadIdx.x ax0@ax1@.0.1 (0,96)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,96)
        p0.shared = ...
      for k.2 (0,4)
        for j.4 (0,4)
          T_matmul_NT = ...
    for ax1.3 (0,4)
      T_fast_tanh = ...

==================================================
No: 575	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NT[ax1_inner] + p2[(((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 8)) + ax1_inner)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,3)
  threadIdx.x ax0.2@ax1.2@ (0,32)
    T_matmul_NT auto_unroll: 64
    for k.0 (0,96)
      for ax0@ax1@.0.0 (0,64)
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@.0.1 (0,32)
        p0.shared = ...
      for k.1 (0,8)
        for j.3 (0,4)
          for j.4 (0,2)
            T_matmul_NT = ...
    for ax1.3 (0,8)
      T_fast_tanh = ...

==================================================
No: 576	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner + 6)] + p2[((((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 6)) + ax1_inner) + 192)])))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647142.09)
==================================================
Placeholder: p0, p1, p2
blockIdx.x ax0.0@ax1.0@ (0,2)
  vthread ax0.1@ax1.1@ (0,2)
    threadIdx.x ax0.2@ax1.2@ (0,32)
      T_matmul_NT auto_unroll: 16
      for k.0 (0,96)
        for ax0@ax1@.0.0 (0,96)
          threadIdx.x ax0@ax1@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@.0.1 (0,32)
          p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,4)
            for j.4 (0,6)
              T_matmul_NT = ...
      for ax1.3 (0,6)
        T_fast_tanh = ...

Time elapsed for measurement: 5.68 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 577	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) / 96) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) % 96)) + 96)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,24)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 578	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + (((int)threadIdx.x) * 2)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,16)
            for j_c.3 (0,2)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 579	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,16)
            for j_c.3 (0,4)
              for k.2 (0,16)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 580	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
44) + (i_inner * 768)) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) & 15) * 3)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,16)
          for i_c.3 (0,8)
            for j_c.4 (0,3)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 581	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
52) + ((((int)threadIdx.x) >> 6) * 24576)) + (i_inner * 768)) + ((((int)threadIdx.x) & 63) * 12)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 12) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,4)
          for i_c.4 (0,8)
            for j_c.4 (0,3)
              T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 582	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 1) * 24576) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 32)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 32) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:3.04, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,12)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,16)
              for j_c.4 (0,16)
                T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 583	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)threadIdx.x) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + j_inner) + 12)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.73, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,32)
            for i_c.4 (0,8)
              for j_c.4 (0,6)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,12)
          T_batch_matmul_NN = ...

==================================================
No: 584	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[((((((((int)blockIdx.x) / 24) * 6144) + (i_inner * 768)) + ((((int)blockIdx.x) % 24) * 32)) + ((int)threadIdx.x)) + 4608)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,24)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 585	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN[((((((((int)threadIdx.x) / 6) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + (((int)threadIdx.x) % 6)) + 18)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,11)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,16)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 586	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 1) * 32)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 32) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.60, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for j_c.3 (0,16)
          for k.2 (0,24)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 587	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 12288)) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 48) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
        p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,4)
          for i_c.4 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 588	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[(((((((int)blockIdx.x) >> 4) * 6144) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,288)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,24)
        for j_c.3 (0,48)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
    for j.3 (0,48)
      T_batch_matmul_NN = ...

==================================================
No: 589	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[((((((((int)threadIdx.x) >> 5) * 768) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 31) * 6)) + j_inner) + 86016)] = T_batch_matmul_NN_local[(j_inner + 42)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,5)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,3)
            for k.2 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 590	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 4) * 1536) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 12) * 4)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,24)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          p0.shared = ...
      for k.2 (0,24)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 591	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((((int)blockIdx.x) * 12288) + (i_inner * 768)) + (((int)threadIdx.x) * 384)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(((i_inner * 384) + j_inner) + 3072)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,4608)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for k.1 (0,12)
          for i_c.3 (0,8)
            for j_c.3 (0,32)
              for j_c.4 (0,12)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,384)
          T_batch_matmul_NN = ...

==================================================
No: 592	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[(((((((int)blockIdx.x) * 24576) + (i_inner * 768)) + (((int)threadIdx.x) * 12)) + j_inner) + 12288)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,144)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for k.2 (0,6)
                for i_c.4 (0,4)
                  for j_c.4 (0,3)
                    T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,12)
          T_batch_matmul_NN = ...

==================================================
No: 593	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 64) + j_inner) + 256)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,16)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for j_c.3 (0,64)
          for k.2 (0,4)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,64)
          T_batch_matmul_NN = ...

==================================================
No: 594	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,4)
          for k.2 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 595	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) & 3) * 48)) + j_inner) + 192)] = T_batch_matmul_NN_local[(((i_inner * 48) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,24)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,48)
          T_batch_matmul_NN = ...

==================================================
No: 596	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 4) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 24) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 597	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 6) * 4)) + j_inner) + 24)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,12)
            for i_c.4 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 598	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 64)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,16)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 599	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.39, Tstamp:1688647165.16)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 600	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)];
      T_batch_matmul_NN[((((((int)blockIdx.x) * 24576) + (i_inner * 768)) + j_inner) + 12864)] = T_batch_matmul_NN_local[(((i_inner * 192) + j_inner) + 21504)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,256)
      for ax0@ax1@ax2@.0.0 (0,2304)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for k.1 (0,3)
        for i_c.3 (0,8)
          for j_c.3 (0,192)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,192)
        T_batch_matmul_NN = ...

==================================================
No: 601	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + (((int)blockIdx.x) * 24)) + ((((int)threadIdx.x) & 3) * 6)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 36)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.2 (0,32)
          for i_c.4 (0,2)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 602	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 127) * 2)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,6)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 603	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) & 15) * 8)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.82, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,8)
            for k.2 (0,24)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 604	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) / 48) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) % 48)) + 144)] = T_batch_matmul_NN_local[(i_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.15, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,3)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 605	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) & 7) * 6)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 6) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,4)
          for j_c.4 (0,6)
            T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 606	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
    T_batch_matmul_NN[(((((((int)blockIdx.x) >> 3) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) & 7) * 96)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,1152)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,8)
        for j_c.3 (0,3)
          for k.2 (0,3)
            for j_c.4 (0,32)
              T_batch_matmul_NN.local = ...
    for j.3 (0,96)
      T_batch_matmul_NN = ...

==================================================
No: 607	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 127) * 2)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,6)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 608	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 73728)] = T_batch_matmul_NN_local[(j_inner + 768)];
    T_batch_matmul_NN[(((((int)threadIdx.x) * 128) + j_inner) + 86016)] = T_batch_matmul_NN_local[(j_inner + 896)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647165.17)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p0.shared = ...
      for k.1 (0,6)
        for j_c.3 (0,4)
          for j_c.4 (0,32)
            T_batch_matmul_NN.local = ...
    for j.3 (0,128)
      T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 609	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 96)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,4)
            for k.2 (0,4)
              for i_c.4 (0,32)
                T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 610	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 15) * 8)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.56, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,8)
              for k.2 (0,6)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 611	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) * 24)) + j_inner) + 144)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 4608)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,8)
            for i_c.4 (0,8)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for i.3 (0,64)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

==================================================
No: 612	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)threadIdx.x) / 48) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) % 48) * 8)) + j_inner) + 384)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,8)
            for j_c.3 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 613	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 64)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.4 (0,8)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 614	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 192) * 6144) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 615	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.82, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,8)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for k.2 (0,48)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 616	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 768)) + ((((int)blockIdx.x) & 7) * 96)) + (((int)threadIdx.x) * 6)) + j_inner) + 12336)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 288)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.49, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,1152)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,96)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,3)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 617	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t)threadIdx.x) / 48) * 12288) + (i_inner * 768)) + ((((int)threadIdx.x) % 48) * 8)) + j_inner) + 384)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,8)
        for j_c.3 (0,4)
          for k.2 (0,4)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 618	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) >> 4) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) & 15) * 12)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 12) + j_inner) + 288)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,2)
            for j_c.3 (0,12)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,12)
          T_batch_matmul_NN = ...

==================================================
No: 619	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[(((((((int)blockIdx.x) * 24576) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner) + 576)] = T_batch_matmul_NN_local[(j_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,16)
            for k.2 (0,6)
              T_batch_matmul_NN.local = ...
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 620	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) & 7) * 6)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(j_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,2)
            for k.2 (0,4)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 621	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
48) * 49152) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 48) * 16)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 16) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,2)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 622	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 128) + j_inner) + 256)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.89, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,16)
            for i_c.4 (0,2)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,128)
          T_batch_matmul_NN = ...

==================================================
No: 623	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 31) * 4)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,8)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 624	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) & 31) * 24)) + ((((int)threadIdx.x) % 12) * 2)) + j_inner) + 36864)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,3)
          for i_c.3 (0,4)
            for k.2 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 625	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)threadIdx.x) % 24) * 4)) + j_inner) + 672)] = T_batch_matmul_NN_local[(j_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 626	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN_local[(i_inner + 64)];
    T_batch_matmul_NN[((((i_inner * 768) + (((int)blockIdx.x) * 256)) + ((int)threadIdx.x)) + 73728)] = T_batch_matmul_NN_local[(i_inner + 96)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,6)
          for i_c.3 (0,4)
            for i_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 627	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 9248)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for j_c.3 (0,4)
          for k.2 (0,4)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 628	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
24576)) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 15) * 12)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 12) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,4)
          for i_c.4 (0,8)
            for j_c.4 (0,3)
              T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 629	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) % 48) * 4)) + j_inner) + 73920)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for k.2 (0,3)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 630	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for k.2 (0,16)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 631	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) * 4)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for k.2 (0,12)
                for i_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 632	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) / 12) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,24)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.3 (0,16)
              for k.2 (0,6)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 633	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 4) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 24) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 634	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
h_matmul_NN_local[i_inner];
    T_batch_matmul_NN[((((i_inner * 768) + (((int)blockIdx.x) * 192)) + ((int)threadIdx.x)) + 96)] = T_batch_matmul_NN_local[(i_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            vectorize ax0@ax1@ax2@.1 (0,6)
              p0.shared = ...
        for i_c.3 (0,128)
          for k.2 (0,24)
            T_batch_matmul_NN.local = ...
      for i.3 (0,128)
        T_batch_matmul_NN = ...

==================================================
No: 635	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
144) + (i_inner * 768)) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) & 7) * 6)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,6)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,6)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 636	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 7) * 24)) + j_inner) + 49152)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,24)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

==================================================
No: 637	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) >> 4) * 1536) + (i_inner * 768)) + ((((int)threadIdx.x) & 15) * 48)) + j_inner) + 49152)] = T_batch_matmul_NN_local[(((i_inner * 48) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.16, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for j_c.3 (0,8)
            for k.2 (0,6)
              for j_c.4 (0,6)
                T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,48)
        T_batch_matmul_NN = ...

==================================================
No: 638	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x) / 3) * 6144) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,4)
          for k.2 (0,24)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 639	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 40)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,24)
          for i_c.3 (0,2)
            for k.2 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 640	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
r * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner) + 24768)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647169.22)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,22)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,16)
          for j_c.3 (0,2)
            for i_c.4 (0,8)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 9.16 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 641	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (T_multiply_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647175.70)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  T_multiply_red auto_unroll: 16
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      T_multiply_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 642	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (T_multiply_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647175.70)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  T_multiply_red auto_unroll: 512
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      T_multiply_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 643	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (T_multiply_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647175.70)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  T_multiply_red auto_unroll: 64
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      T_multiply_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 644	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647175.70)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 645	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647175.70)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 646	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 647	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 648	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 649	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 650	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 651	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 652	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 653	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 654	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 655	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 656	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 657	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 658	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 659	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 660	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 661	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 662	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 663	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 664	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 665	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 666	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 667	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 668	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 669	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 670	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 671	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 672	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647175.71)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 673	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 674	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 675	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 676	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 677	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 678	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 679	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 680	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 681	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 682	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 683	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 684	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 685	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 686	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 687	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 688	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 689	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 690	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 691	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 692	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 693	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 694	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 695	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 696	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 697	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 698	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 699	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 700	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 701	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 702	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 703	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 704	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647178.16)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

Time elapsed for measurement: 4.94 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 705	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 8) * 1536) + ((((int)blockIdx.x) & 255) * 3)) + ((int)threadIdx.x)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,3)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 706	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
68) + ((int)blockIdx.x)) + 73728)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((int)threadIdx.x) * 768) + ((int)blockIdx.x)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 707	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 708	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[(((((((int)blockIdx.x) * 1536) + (i_inner * 768)) + (((int)threadIdx.x) * 24)) + j_inner) + 672)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 336)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.88, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,768)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,6)
            for k.2 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

==================================================
No: 709	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 128) + j_inner) + 256)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for j_c.3 (0,64)
          for i_c.4 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,128)
          T_batch_matmul_NN = ...

==================================================
No: 710	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)threadIdx.x) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) * 6)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 36)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for i_c.4 (0,2)
          for j_c.4 (0,6)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 711	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) & 1) * 64)) + j_inner) + 256)] = T_batch_matmul_NN_local[(((i_inner * 64) + j_inner) + 1024)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:3.06, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,8)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,64)
          T_batch_matmul_NN = ...

==================================================
No: 712	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 96)) + j_inner) + 192)] = T_batch_matmul_NN_local[(((i_inner * 96) + j_inner) + 768)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:5.53, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,384)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,4)
            for i_c.4 (0,8)
              for j_c.4 (0,24)
                T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,96)
          T_batch_matmul_NN = ...

==================================================
No: 713	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 128) + j_inner) + 256)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for j_c.3 (0,16)
          for i_c.4 (0,2)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,128)
          T_batch_matmul_NN = ...

==================================================
No: 714	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 1) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 1)) + 6146)] = T_batch_matmul_NN_local[(i_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,4)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 715	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN[((((((((int)blockIdx.x) >> 4) * 1536) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) * 6)) + j_inner) + 768)] = T_batch_matmul_NN_local[(j_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 716	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[(((((((int)blockIdx.x) * 3072) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)threadIdx.x) & 7) * 24)) + j_inner) + 2112)] = T_batch_matmul_NN_local[(j_inner + 168)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
        for j_c.3 (0,12)
          for k.2 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 717	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[(((((((int)blockIdx.x) >> 4) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for j_c.3 (0,12)
        for k.2 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for j.3 (0,48)
      T_batch_matmul_NN = ...

==================================================
No: 718	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(int j_inner = 0; j_inner < 16; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 48) + (((int)threadIdx.x) * 16)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  threadIdx.x b.2@i.2@j.2@ (0,3)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
          p0.shared = ...
      for k.1 (0,3)
        for j_c.3 (0,16)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 719	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int j_inner = 0; j_inner < 32; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 32)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,12)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
        p0.shared = ...
      for j_c.3 (0,2)
        for k.2 (0,4)
          for j_c.4 (0,16)
            T_batch_matmul_NN.local = ...
    for j.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 720	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 24576) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 721	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 722	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)blockIdx.x) / 192) * 6144) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 4608)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for k.1 (0,3)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 723	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 24576) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 724	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[10];
  T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + (((int)blockIdx.x) * 96)) + (((int)threadIdx.x) & 7)) + 88)] = T_batch_matmul_NN_local[11];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 725	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 726	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 3) * 24576) + ((((int)threadIdx.x) >> 8) * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) & 255)) + 21504)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for k.1 (0,12)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 727	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 4) * 1536) + ((((int)blockIdx.x) & 15) * 48)) + ((int)threadIdx.x)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            vectorize ax0@ax1@ax2@.1 (0,6)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 728	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 729	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 3) * 6144) + ((((int)blockIdx.x) & 7) * 96)) + ((int)threadIdx.x)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,4)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 730	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 731	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 732	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,48)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 733	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 734	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)blockIdx.x) / 48) * 24576) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 48) * 16)) + (((int)threadIdx.x) & 3)) + 12)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            vectorize ax0@ax1@ax2@.1 (0,16)
              p0.shared = ...
        for k.1 (0,8)
          for k.2 (0,12)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 735	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 736	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 3) * 6144) + ((((int)blockIdx.x) % 3) * 256)) + ((int)threadIdx.x)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647203.83)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
        for k.1 (0,12)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 737	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.2 (0,24)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 738	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,6)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 739	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN_local[2];
  T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) & 7)) + 24)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p0.shared = ...
        for k.2 (0,48)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 740	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 384) * 24576) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1)) + 21504)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.1 (0,24)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 741	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 3) * 6144) + ((((int)blockIdx.x) & 7) * 96)) + ((int)threadIdx.x)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 742	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 24576) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,12)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 743	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 744	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 6) * 24576) + ((((int)threadIdx.x) >> 7) * 768)) + ((((int)blockIdx.x) % 6) * 128)) + (((int)threadIdx.x) & 127)) + 21504)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for k.1 (0,24)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 745	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 3) * 6144) + ((((int)blockIdx.x) & 7) * 96)) + ((int)threadIdx.x)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 746	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 747	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)blockIdx.x) / 48) * 24576) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 48) * 16)) + (((int)threadIdx.x) & 3)) + 12)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,8)
          for k.2 (0,12)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 748	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 749	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 750	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)blockIdx.x) / 48) * 24576) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 48) * 16)) + (((int)threadIdx.x) & 3)) + 12)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,6)
          for k.2 (0,16)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 751	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 752	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN_local[2];
  T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) & 7)) + 24)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 753	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 24576) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 754	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) >> 2) * 768) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) & 3)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,48)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 755	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 756	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)blockIdx.x) >> 4) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) % 24)) + 24)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p0.shared = ...
        for k.2 (0,24)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 757	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 3) * 24576) + ((((int)threadIdx.x) >> 8) * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) & 255)) + 21504)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,6)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 758	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 24576) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 21504)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 759	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 760	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((((((int)blockIdx.x) / 24) * 6144) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) % 24) * 32)) + (((int)threadIdx.x) & 7)) + 24)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 761	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,48)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 762	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 24576) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 763	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 764	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 4) * 1536) + ((((int)blockIdx.x) & 15) * 48)) + ((int)threadIdx.x)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 765	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 48) * 768) + (((int)blockIdx.x) * 48)) + (((int)threadIdx.x) % 48)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p0.shared = ...
        for k.1 (0,8)
          for k.2 (0,3)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 766	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[((((((((int)threadIdx.x) / 96) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) * 192)) + (((int)threadIdx.x) % 96)) + 49248)] = T_batch_matmul_NN_local[(i_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,24)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 767	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) * 12288) + (i_inner * 768)) + (((int)threadIdx.x) * 6)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 288)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,16)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 768	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 49280)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 640)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647206.42)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,16)
            for j_c.3 (0,4)
              for k.2 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 10.20 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 769	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
] = red_buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (p0_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      p0_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 770	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
] = red_buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (p0_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  p0_red auto_unroll: 1024
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      p0_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 771	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 772	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 773	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 774	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 775	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 776	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 777	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 778	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 779	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 780	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 781	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 782	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 783	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 784	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 785	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 786	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 787	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 788	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 789	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 790	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 791	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 792	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 793	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 794	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 795	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 796	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 797	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 798	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 799	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 800	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647212.61)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 801	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 802	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 803	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 804	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 805	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 806	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 807	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 808	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 809	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 810	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 811	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 812	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 813	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 814	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 815	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 816	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 817	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 818	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 819	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 820	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 821	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 822	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 823	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 824	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 825	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 826	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 827	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 828	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 829	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 830	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 831	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 832	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647215.05)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

Time elapsed for measurement: 4.85 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 833	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
]));
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[23] * p1_shared[23]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647236.45)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,32)
    for ax0@ax1@ax2@.0.0 (0,6)
      vectorize ax0@ax1@ax2@.1 (0,4)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,24)
      p0.shared = ...
    for k.1 (0,24)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 834	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647236.45)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 835	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647236.45)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 836	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,11)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 837	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN_local[0] + (p0_shared[767] * p1_shared[(((int)threadIdx.x) + 3068)]));
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.22, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for ax0@ax1@ax2@.0.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p0.shared = ...
    for k.1 (0,384)
      for k.2 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 838	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0] + (p0_shared[23] * p1_shared[(((int)threadIdx.x) + 92)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 839	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
23)] * p1_shared[23]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,24)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 840	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
47)] * p1_shared[47]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,24)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 841	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + 1)] * p1_shared[(((k_outer_inner * 8) + ((int)threadIdx.x)) + 4)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for ax0@ax1@ax2@.0.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p0.shared = ...
    for k.1 (0,384)
      for k.2 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 842	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0] + (p0_shared[23] * p1_shared[(((int)threadIdx.x) + 92)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 843	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0] + (p0_shared[23] * p1_shared[(((int)threadIdx.x) + 92)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 844	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 845	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)blockIdx.x) / 3) * 3072) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 128) + j_inner) + 512)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,256)
        p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,4)
        p0.shared = ...
      for j_c.3 (0,64)
        for i_c.4 (0,4)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,128)
        T_batch_matmul_NN = ...

==================================================
No: 846	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 847	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_shared[23]));
  }
  T_batch_matmul_NN[(((int)blockIdx.x) * 2)] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        p0.shared = ...
      for k.1 (0,12)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 848	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 849	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 1)] * p1_shared[1]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 850	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN_local[0] + (p0_shared[3] * p1_shared[(((int)threadIdx.x) + 288)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 96) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 851	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 384) * 24576) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,6)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,64)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 852	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,24)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 853	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 192) * 6144) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 854	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 384) * 24576) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,12)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 855	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ared[k_inner]));
    }
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    for k.0 (0,12)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.2 (0,64)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 856	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 6144) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,48)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 857	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 858	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 859	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 384) * 24576) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    for k.0 (0,12)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 860	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 6144) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,48)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 861	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 862	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) >> 5) * 12288) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 31) * 24)) + (((int)threadIdx.x) % 24))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,24)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 863	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 864	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647236.46)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,24)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 865	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
23)] * p1_shared[23]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 866	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.67, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 867	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[(((((((int)blockIdx.x) >> 2) * 1536) + ((((int)threadIdx.x) / 192) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + (((int)threadIdx.x) % 192))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 868	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 869	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,16)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 870	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,24)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 871	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) >> 6) * 12288) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  threadIdx.x b.2@i.2@j.2@ (0,192)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 872	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,16)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 873	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) >> 5) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 31) * 24)) + (((int)threadIdx.x) % 24))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        p0.shared = ...
      for k.2 (0,24)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 874	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ner)]));
      }
    }
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    for k.0 (0,12)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 875	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p0.shared = ...
      for k.2 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 876	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 877	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 12288) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 878	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN_local[0] + (p0_shared[7] * p1_shared[(((int)threadIdx.x) + 336)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 879	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) >> 6) * 12288) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  threadIdx.x b.2@i.2@j.2@ (0,192)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 880	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_local[0] + (p0_shared[23] * p1_shared[(((int)threadIdx.x) + 8832)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 384) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 881	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,24)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 882	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
5)] * p1_shared[95]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,8)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
      for k.1 (0,24)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 883	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) >> 4) * 1536) + ((((int)threadIdx.x) / 48) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) % 48))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
        p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 884	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 6144) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,48)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 885	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) >> 4) * 12288) + ((((int)threadIdx.x) / 48) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) % 48))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 886	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 24576) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 887	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 6144) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,48)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 888	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NN[(((((((int)blockIdx.x) / 96) * 1536) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) % 96) * 8)) + (((int)threadIdx.x) & 7))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 889	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 192) * 6144) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 890	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) >> 5) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 31) * 24)) + (((int)threadIdx.x) % 24))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 891	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 192) * 49152) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for k.1 (0,24)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 892	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 384) * 24576) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,12)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,32)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 893	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,3)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 894	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 15) * 8)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,2)
            for i_c.4 (0,16)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 895	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,4)
              for j_c.4 (0,16)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 896	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 3) * 16)) + j_inner) + 192)] = T_batch_matmul_NN_local[(j_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647239.11)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,8)
            for k.2 (0,3)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for j.3 (0,16)
        T_batch_matmul_NN = ...

Time elapsed for measurement: 5.87 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 897	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
buf0[0];
  __syncthreads();
  if (((int)threadIdx.x) < 1) {
    T_divide[(((int)blockIdx.x) + ((int)threadIdx.x))] = (T_multiply_red[((int)threadIdx.x)] * 1.302083e-03f);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2.0@ (0,128)
  for k2.0 (0,24)
    threadIdx.x k2.1 (0,32)
      T_multiply_red = ...
  threadIdx.x ax2.1 (0,32)
    T_divide = ...

==================================================
No: 898	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 899	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 900	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 901	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 902	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 903	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 904	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 905	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 906	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 907	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 908	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 909	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 910	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 911	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 912	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 913	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 914	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 915	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 916	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 917	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 918	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 919	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 920	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 921	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 922	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 923	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 924	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 925	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 926	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 927	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 928	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647245.75)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 929	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647248.21)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 930	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647248.21)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 931	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 932	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 933	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 934	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 935	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 936	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 937	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 938	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 939	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 940	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 941	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 942	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 943	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 944	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 945	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 946	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 947	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 948	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 949	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 950	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 951	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 952	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 953	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 954	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 955	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 956	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 957	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 958	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 959	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 960	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647248.22)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

Time elapsed for measurement: 4.99 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 961	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 48) * 2)) + j_inner) + 295008)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 962	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
    T_batch_matmul_NN[((((i_inner * 3072) + (((int)blockIdx.x) * 768)) + (((int)threadIdx.x) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,8)
        for j_c.3 (0,2)
          for i_c.4 (0,16)
            T_batch_matmul_NN.local = ...
    for i.3 (0,128)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 963	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
    T_batch_matmul_NN[((((i_inner * 3072) + (((int)blockIdx.x) * 768)) + (((int)threadIdx.x) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.36, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,8)
        for j_c.3 (0,2)
          for i_c.4 (0,16)
            T_batch_matmul_NN.local = ...
    for i.3 (0,128)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 964	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN_local[2];
  T_batch_matmul_NN[(((((((int)threadIdx.x) >> 5) * 3072) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) & 31)) + 294912)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p0.shared = ...
        for k.1 (0,6)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 965	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)threadIdx.x) >> 1) * 3072) + (((int)blockIdx.x) * 16)) + (((int)threadIdx.x) & 1)) + 14)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.50, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,12)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 966	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + ((((int)threadIdx.x) >> 7) * 6144)) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,24)
          T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 967	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + ((((int)threadIdx.x) >> 7) * 6144)) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,6)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 968	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN_local[j_inner];
    T_batch_matmul_NN[((((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 24)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,6)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for j_c.3 (0,6)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 969	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[((((((((int)threadIdx.x) >> 2) * 3072) + (((int)blockIdx.x) * 48)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 196640)] = T_batch_matmul_NN_local[(j_inner + 20)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for j_c.3 (0,2)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 970	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 3072)) + (((int)blockIdx.x) * 512)) + ((((int)threadIdx.x) & 31) * 4)) + j_inner) + 384)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for k.2 (0,3)
              for i_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 971	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[((((((((int)blockIdx.x) / 12) * 98304) + (((int)threadIdx.x) * 3072)) + ((((int)blockIdx.x) % 12) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(j_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
        for j_c.3 (0,2)
          for j_c.4 (0,64)
            T_batch_matmul_NN.local = ...
      for j.3 (0,128)
        T_batch_matmul_NN = ...

==================================================
No: 972	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) / 12) * 6144) + (i_inner * 3072)) + (((int)blockIdx.x) * 24)) + ((((int)threadIdx.x) % 12) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,24)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,4)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 973	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN[((((((((int)threadIdx.x) >> 2) * 6144) + (i_inner * 3072)) + (((int)blockIdx.x) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,384)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,6)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 974	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 1) * 12288) + (i_inner * 3072)) + ((((int)blockIdx.x) & 1) * 1536)) + (((int)threadIdx.x) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  threadIdx.x b.2@i.2@j.2@ (0,192)
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.4 (0,4)
        for j_c.4 (0,8)
          T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 975	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
304)) + (i_inner * 3072)) + ((((int)blockIdx.x) % 3) * 1024)) + ((((int)threadIdx.x) & 31) * 32)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 32) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p0.shared = ...
      for i_c.3 (0,32)
        for j_c.3 (0,4)
          for j_c.4 (0,8)
            T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 976	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,256)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for k.1 (0,3)
          for i_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 977	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,256)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for k.2 (0,3)
          for i_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 978	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) >> 4) * 3072)) + ((((int)blockIdx.x) % 12) * 256)) + ((((int)threadIdx.x) & 15) * 8)) + j_inner) + 128)] = T_batch_matmul_NN_local[(j_inner + 8)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
        for j_c.3 (0,2)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 979	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 3072)) + (((int)blockIdx.x) * 512)) + ((((int)threadIdx.x) & 31) * 4)) + j_inner) + 384)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 980	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[(((((((int)threadIdx.x) * 6144) + (i_inner * 3072)) + (((int)blockIdx.x) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 128) + j_inner) + 256)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for j_c.4 (0,64)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,128)
          T_batch_matmul_NN = ...

==================================================
No: 981	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN_local[j_inner];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 3072) + (((int)blockIdx.x) * 12)) + j_inner) + 196608)] = T_batch_matmul_NN_local[(j_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,64)
          for j_c.3 (0,2)
            for j_c.4 (0,6)
              T_batch_matmul_NN.local = ...
      for j.3 (0,12)
        T_batch_matmul_NN = ...

==================================================
No: 982	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[((((((int)threadIdx.x) * 6144) + (i_inner * 3072)) + (((int)blockIdx.x) * 256)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 256) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,4)
          for j_c.4 (0,64)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,256)
        T_batch_matmul_NN = ...

==================================================
No: 983	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NN_local[j_inner];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 3072) + (((int)blockIdx.x) * 256)) + j_inner) + 128)] = T_batch_matmul_NN_local[(j_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for j_c.3 (0,2)
          for j_c.4 (0,64)
            T_batch_matmul_NN.local = ...
      for j.3 (0,128)
        T_batch_matmul_NN = ...

==================================================
No: 984	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) >> 1) * 24576) + (i_inner * 3072)) + (((int)blockIdx.x) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,24)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,8)
        for j_c.3 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 985	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) >> 1) * 24576) + (i_inner * 3072)) + (((int)blockIdx.x) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,24)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,8)
        for j_c.3 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 986	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) >> 1) * 12288) + (i_inner * 3072)) + (((int)blockIdx.x) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.16, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,24)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 987	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 3072)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) & 127) * 3)) + j_inner) + 294912)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 144)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
        for i_c.4 (0,16)
          for j_c.4 (0,3)
            T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 988	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 3072)) + ((((int)blockIdx.x) & 7) * 384)) + ((((int)threadIdx.x) % 24) * 4)) + j_inner) + 288)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 989	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) >> 7) * 98304) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 8)) + j_inner) + 2048)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 512)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,8)
          for j_c.3 (0,8)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 990	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + (((int)blockIdx.x) * 512)) + ((((int)threadIdx.x) & 31) * 4)) + j_inner) + 196992)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for j_c.3 (0,2)
          for i_c.4 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 991	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 96) * 196608) + (i_inner * 3072)) + (((int)blockIdx.x) * 768)) + ((((int)threadIdx.x) % 96) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,192)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,4)
          for j_c.3 (0,4)
            for i_c.4 (0,16)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 992	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 3072)) + (((int)blockIdx.x) * 1536)) + ((((int)threadIdx.x) & 31) * 24)) + j_inner) + 768)] = T_batch_matmul_NN_local[(((i_inner * 24) + j_inner) + 384)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.44, Tstamp:1688647269.66)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              for j_c.4 (0,12)
                T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,24)
          T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 993	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 48) * 2)) + j_inner) + 295008)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647272.19)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,384)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 994	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) >> 7) * 98304) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 8)) + j_inner) + 2048)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 512)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647272.19)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,8)
          for j_c.3 (0,8)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 995	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)blockIdx.x) * 49152) + ((((int)threadIdx.x) >> 8) * 12288)) + (i_inner * 3072)) + (((int)threadIdx.x) & 255)) + 2816)] = T_batch_matmul_NN_local[(i_inner + 44)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647272.19)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        for k.1 (0,3)
          for i_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 996	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)threadIdx.x) >> 7) * 196608) + (i_inner * 3072)) + (((int)blockIdx.x) * 512)) + (((int)threadIdx.x) & 127)) + 384)] = T_batch_matmul_NN_local[(i_inner + 192)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647272.19)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
        for i_c.4 (0,64)
          T_batch_matmul_NN.local = ...
      for i.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 997	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) >> 7) * 196608) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 6)) + j_inner) + 2304)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 1152)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,256)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for k.1 (0,3)
        for i_c.4 (0,64)
          for j_c.4 (0,6)
            T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 998	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[((((((((int)threadIdx.x) >> 7) * 49152) + (i_inner * 3072)) + (((int)blockIdx.x) * 512)) + (((int)threadIdx.x) & 127)) + 384)] = T_batch_matmul_NN_local[(i_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for k.1 (0,3)
          for i_c.4 (0,16)
            T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 999	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 3072)) + ((((int)blockIdx.x) % 3) * 1024)) + ((((int)threadIdx.x) & 127) * 4)) + j_inner) + 512)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 128)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.46, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,4)
          for k.2 (0,4)
            for i_c.4 (0,8)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1000	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 3072)) + ((((int)blockIdx.x) & 1) * 1536)) + ((((int)threadIdx.x) & 63) * 4)) + j_inner) + 1280)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 320)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,4)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1001	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + ((((int)threadIdx.x) >> 7) * 6144)) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p0.shared = ...
      for j_c.3 (0,24)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 1002	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + ((((int)threadIdx.x) >> 7) * 6144)) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 24)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 24) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      for j_c.3 (0,24)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 1003	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for i_c.4 (0,2)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1004	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 3072)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 196800)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
        for j_c.3 (0,2)
          for i_c.4 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1005	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for i_c.4 (0,2)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1006	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
        for i_c.4 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1007	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for i_c.4 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1008	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for i_c.3 (0,4)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1009	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 7) * 6)) + j_inner) + 196752)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 168)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.12, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,3)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 1010	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 7) * 6)) + j_inner) + 196752)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 168)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,3)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 1011	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) % 48) * 2)) + j_inner) + 295008)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
        for i_c.3 (0,4)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1012	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
        for i_c.4 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1013	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) >> 7) * 6144)) + (i_inner * 3072)) + ((((int)threadIdx.x) & 127) * 6)) + j_inner) + 2304)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 36)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,6)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 1014	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 3072)) + (((int)blockIdx.x) * 192)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.4 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1015	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[((((((((int)threadIdx.x) >> 5) * 6144) + (i_inner * 3072)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.x) & 31)) + 196640)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,18)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for k.2 (0,6)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1016	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 3072)) + ((((int)blockIdx.x) & 3) * 768)) + ((((int)threadIdx.x) & 255) * 3)) + j_inner) + 36864)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 18)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,3)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 1017	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 3072)) + ((((int)blockIdx.x) & 1) * 1536)) + ((((int)threadIdx.x) & 127) * 3)) + j_inner) + 1152)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 72)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,3)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 1018	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 3072)) + ((((int)blockIdx.x) % 6) * 512)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner) + 256)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 32)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
        for j_c.3 (0,8)
          for i_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1019	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 3072)) + ((((int)blockIdx.x) & 1) * 1536)) + ((((int)threadIdx.x) & 127) * 3)) + j_inner) + 1152)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 72)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,3)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 1020	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 3072)) + ((((int)blockIdx.x) & 3) * 768)) + ((((int)threadIdx.x) & 255) * 3)) + j_inner) + 36864)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 18)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,3)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 1021	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 3072)) + ((((int)blockIdx.x) & 3) * 768)) + ((((int)threadIdx.x) & 63) * 3)) + j_inner) + 576)] = T_batch_matmul_NN_local[(((i_inner * 3) + j_inner) + 72)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for i_c.3 (0,4)
          for j_c.3 (0,3)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,3)
          T_batch_matmul_NN = ...

==================================================
No: 1022	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 3072)) + ((((int)blockIdx.x) % 24) * 128)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 112)] = T_batch_matmul_NN_local[(((i_inner * 8) + j_inner) + 224)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,12)
              for i_c.4 (0,2)
                for j_c.4 (0,8)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1023	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[(((((((int)blockIdx.x) * 12288) + (i_inner * 3072)) + (((int)threadIdx.x) * 48)) + j_inner) + 1536)] = T_batch_matmul_NN_local[(((i_inner * 48) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,256)
        for ax0@ax1@ax2@.0.0 (0,288)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
        for k.1 (0,3)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,24)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,48)
          T_batch_matmul_NN = ...

==================================================
No: 1024	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 3072)) + ((((int)blockIdx.x) & 7) * 384)) + (((int)threadIdx.x) * 6)) + j_inner) + 172032)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 336)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647272.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,144)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for i_c.4 (0,8)
                for j_c.4 (0,3)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 5.32 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1025	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 & 63) >> 1) * 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((int)threadIdx.x)) + 256)] = T_batch_matmul_NN_local[(((b_inner * 2) + i_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
        for b_c.3 (0,2)
          for b_c.4 (0,3)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1026	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((int)blockIdx.x) >> 6) * 512) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 4)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        p0.shared = ...
      for k.1 (0,4)
        for j_c.3 (0,2)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1027	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 8)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,2)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1028	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 8)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,2)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1029	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[j_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) >> 8) * 32768) + ((((int)blockIdx.x) & 255) * 64)) + j_inner) + 16384)] = T_batch_matmul_NN_local[(j_inner + 64)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.53, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,512)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      for k.1 (0,4)
        for j_c.3 (0,2)
          for j_c.4 (0,32)
            T_batch_matmul_NN.local = ...
    for j.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1030	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 24)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,48)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,6)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1031	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 48)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for k.1 (0,2)
        for b_c.3 (0,12)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,12)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1032	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 48)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,48)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,12)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,12)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1033	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 24)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,24)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,6)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1034	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 131328)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 80)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  vthread b.1@i.1@j.1@ (0,6)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,48)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,4)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,4)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1035	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 24)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,24)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,3)
          for j_c.3 (0,2)
            for b_c.4 (0,2)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1036	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 63) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 24)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,24)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,6)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1037	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (b_inner * 16384)) + ((((int)blockIdx.x) & 511) * 32)) + (((int)threadIdx.x) * 4)) + j_inner) + 24)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for b_c.3 (0,2)
          for b_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1038	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN[(((((b_inner * 16384) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.x) * 4)) + j_inner) + 131096)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 176)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for b_c.3 (0,2)
          for b_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1039	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 128)) + ((((int)blockIdx.x) & 31) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner) + 3072)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1040	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6) + (b_inner * 16384)) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 4)) + j_inner) + 24)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,6)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,6)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,6)
            p0.shared = ...
        for b_c.3 (0,2)
          for b_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1041	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 128)) + ((((int)blockIdx.x) & 31) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner) + 3072)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1042	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6) + (b_inner * 16384)) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 24)] = T_batch_matmul_NN_local[(((b_inner * 2) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          p0.shared = ...
        for b_c.3 (0,2)
          for b_c.4 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1043	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)blockIdx.x) & 63) >> 3) * 2048)) + (((((int)threadIdx.x) & 255) >> 4) * 128)) + ((((int)blockIdx.x) & 7) * 16)) + (((int)threadIdx.x) & 15))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    for k.0 (0,8)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1044	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 2047) >> 6) * 512)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 257)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1045	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1046	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + j_inner) + 2052)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 4)) + j_inner) + 1152)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:6.58, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,96)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,384)
        p0.shared = ...
      for b_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for b_c.4 (0,3)
              for i_c.4 (0,16)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1047	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[((((b_inner * 16384) + (i_inner * 128)) + j_inner) + 147456)] = T_batch_matmul_NN_local[((((b_inner * 16384) + (i_inner * 128)) + j_inner) + 147456)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  for k.0 (0,16)
    for ax0@ax1@ax2@.0.0 (0,6144)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,2048)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for k.1 (0,4)
      for b_c.3 (0,3)
        for i_c.3 (0,4)
          for j_c.3 (0,64)
            for i_c.4 (0,32)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
  for b.3 (0,3)
    for i.3 (0,128)
      for j.3 (0,128)
        T_batch_matmul_NN = ...

==================================================
No: 1048	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + j_inner) + 2052)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 4)) + j_inner) + 1152)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:6.47, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,384)
        p0.shared = ...
      for b_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for b_c.4 (0,3)
              for i_c.4 (0,16)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1049	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
4)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + j_inner) + 516)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 288)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.87, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,96)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        p0.shared = ...
      for b_c.3 (0,2)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for b_c.4 (0,3)
              for i_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1050	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
12) + ((((int)threadIdx.x) >> 1) * 128)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 304)] = T_batch_matmul_NN_local[(j_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            p0.shared = ...
        for j_c.3 (0,4)
          for k.2 (0,4)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1051	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1052	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1053	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1054	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)threadIdx.x) >> 4) * 16384)) + (((((int)blockIdx.x) & 127) >> 1) * 256)) + ((((int)blockIdx.x) & 1) * 64)) + (((int)threadIdx.x) & 15)) + 176)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for ax0@ax1@ax2@.0.0 (0,256)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,16)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1055	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 512) + ((int)threadIdx.x)) + 384)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((int)blockIdx.x) * 512) + ((int)threadIdx.x)) + 448)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1056	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
Idx.x) >> 4) * 16384)) + (((((int)blockIdx.x) & 127) >> 4) * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((int)blockIdx.x) & 15) * 8)) + 7)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647300.88)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1057	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* 512) + ((int)threadIdx.x)) + 384)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((int)blockIdx.x) * 512) + ((int)threadIdx.x)) + 448)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1058	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)threadIdx.x) >> 1) * 16384) + ((((int)blockIdx.x) >> 6) * 512)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 257)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,24)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p0.shared = ...
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1059	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)threadIdx.x) >> 4) * 16384)) + (((((int)blockIdx.x) & 127) >> 1) * 256)) + ((((int)blockIdx.x) & 1) * 64)) + (((int)threadIdx.x) & 15)) + 176)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,2)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1060	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1061	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        for k.1 (0,4)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1062	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) * 1024) + ((((int)threadIdx.x) >> 6) * 128)) + (((int)threadIdx.x) & 63)) + 832)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1063	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
Idx.x) & 127) >> 2) * 512)) + (((((int)threadIdx.x) & 63) >> 5) * 128)) + ((((int)blockIdx.x) & 3) * 32)) + (((int)threadIdx.x) & 31)) + 33024)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.49, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,8)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1064	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1065	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6) * 256)) + (((int)threadIdx.x) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 81921)] = T_batch_matmul_NN_local[((((((int)threadIdx.x) + 1) >> 1) + 11) - ((int)threadIdx.x))];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,12)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1066	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 2047) >> 6) * 512)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 257)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.51)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1067	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1068	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)blockIdx.x) >> 10) * 65536) + ((((int)threadIdx.x) >> 4) * 16384)) + ((((int)blockIdx.x) & 1023) * 16)) + (((int)threadIdx.x) & 15))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    for k.0 (0,8)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1069	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dIdx.x) >> 1) * 16384)) + (((((int)blockIdx.x) & 4095) >> 6) * 256)) + ((((int)threadIdx.x) & 1) * 128)) + ((((int)blockIdx.x) & 63) * 2)) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,12)
      for k.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,12)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1070	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x) >> 2) * 2048) + ((((int)threadIdx.x) >> 3) * 128)) + ((((int)blockIdx.x) & 3) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 1024
    for ax0@ax1@ax2@.0.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,8)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p0.shared = ...
    for k.1 (0,4)
      for j_c.3 (0,4)
        for k.2 (0,16)
          T_batch_matmul_NN.local = ...
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1071	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)threadIdx.x) & 63) >> 2) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,32)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1072	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 3) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        p0.shared = ...
      for j_c.3 (0,2)
        for k.2 (0,4)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1073	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)threadIdx.x) & 63) >> 2) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1074	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 3) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,2)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,8)
          for i_c.4 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1075	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
4) * 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 3) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    for k.0 (0,4)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
      for k.1 (0,16)
        for j_c.3 (0,2)
          for i_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1076	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)threadIdx.x) & 63) >> 2) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,2)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,16)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1077	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)threadIdx.x) & 63) >> 2) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,32)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1078	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 3) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.16, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        p0.shared = ...
      for k.1 (0,4)
        for j_c.3 (0,2)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1079	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)threadIdx.x) & 63) >> 2) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,16)
        for k.2 (0,4)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1080	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
4) * 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 3) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    for k.0 (0,4)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
      for k.1 (0,16)
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1081	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 3) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.16, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
        p0.shared = ...
      for k.2 (0,4)
        for i_c.4 (0,2)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1082	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 5) * 512)) + (i_inner * 128)) + ((((int)blockIdx.x) & 1) * 64)) + (((int)threadIdx.x) & 31)) + 16416)] = T_batch_matmul_NN_local[(i_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,32)
        for k.2 (0,2)
          for i_c.4 (0,4)
            T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1083	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)threadIdx.x) & 63) >> 2) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 15) * 8)) + (((int)threadIdx.x) & 3)) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.2 (0,64)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1084	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)blockIdx.x) & 3) * 4096)) + ((((int)threadIdx.x) >> 7) * 256)) + (i_inner * 128)) + (((int)threadIdx.x) & 127)) + 51200)] = T_batch_matmul_NN_local[(i_inner + 14)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
          p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1085	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 3) * 256)) + (i_inner * 128)) + ((((int)blockIdx.x) & 7) * 16)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p0.shared = ...
      for k.2 (0,4)
        for i_c.4 (0,2)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1086	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ (i_inner * 128)) + ((((int)threadIdx.x) & 3) * 32)) + j_inner) + 8192)] = T_batch_matmul_NN_local[((((b_inner * 128) + (i_inner * 32)) + j_inner) + 768)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for b_c.3 (0,6)
        for i_c.3 (0,4)
          for j_c.3 (0,4)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,4)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 1087	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + ((((int)blockIdx.x) & 1) * 64)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 64) + (i_inner * 4)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    for k.0 (0,64)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for b_c.3 (0,2)
        for i_c.3 (0,16)
          for j_c.3 (0,4)
            T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,16)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1088	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8)) + ((((int)blockIdx.x) & 3) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 8) + (i_inner * 2)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647303.52)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,4)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,4)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 11.33 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1089	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er + 160)];
    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + ((((int)threadIdx.x) & 7) * 16)) + j_inner) + 49792)] = T_batch_matmul_NN_local[(j_inner + 176)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,3072)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p0.shared = ...
      for j_c.3 (0,4)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1090	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 5) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.47, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p0.shared = ...
      for k.1 (0,8)
        for j_c.3 (0,2)
          for i_c.4 (0,8)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1091	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er + 160)];
    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + ((((int)threadIdx.x) & 7) * 16)) + j_inner) + 49792)] = T_batch_matmul_NN_local[(j_inner + 176)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,3072)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        p0.shared = ...
      for j_c.3 (0,4)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1092	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er + 160)];
    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + ((((int)threadIdx.x) & 7) * 16)) + j_inner) + 49792)] = T_batch_matmul_NN_local[(j_inner + 176)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,3072)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for j_c.3 (0,4)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1093	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er) + 73728)] = T_batch_matmul_NN_local[(j_inner + 72)];
    T_batch_matmul_NN[(((((int)threadIdx.x) * 12) + j_inner) + 86016)] = T_batch_matmul_NN_local[(j_inner + 84)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,1024)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
        p0.shared = ...
      for j_c.3 (0,6)
        for j_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for j.3 (0,12)
      T_batch_matmul_NN = ...

==================================================
No: 1094	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 5) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 31) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.42, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,8)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1095	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)threadIdx.x) / 384) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) % 384))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.1 (0,4)
        for i_c.3 (0,2)
          for k.2 (0,6)
            for i_c.4 (0,32)
              T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1096	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)threadIdx.x) / 384) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) % 384)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,6)
            for i_c.4 (0,32)
              T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1097	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er + 160)];
    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + ((((int)threadIdx.x) & 7) * 16)) + j_inner) + 49792)] = T_batch_matmul_NN_local[(j_inner + 176)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,3072)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for j_c.3 (0,4)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1098	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[2];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 12) * 768) + (((int)blockIdx.x) * 12)) + (((int)threadIdx.x) % 12)) + 73728)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1099	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[2];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 12) * 768) + (((int)blockIdx.x) * 12)) + (((int)threadIdx.x) % 12)) + 73728)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1100	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[2];
  T_batch_matmul_NN[(((((((int)threadIdx.x) / 12) * 768) + (((int)blockIdx.x) * 12)) + (((int)threadIdx.x) % 12)) + 73728)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1101	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1102	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) % 12) * 4)) + j_inner) + 336)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1103	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1104	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x))] = T_batch_matmul_NN_local[i_inner];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)threadIdx.x)) + 384)] = T_batch_matmul_NN_local[(i_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,1536)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p0.shared = ...
      for i_c.3 (0,4)
        for k.2 (0,2)
          for i_c.4 (0,32)
            T_batch_matmul_NN.local = ...
    for i.3 (0,128)
      T_batch_matmul_NN = ...

==================================================
No: 1105	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[((((((((int)threadIdx.x) >> 4) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) & 15)) + 112)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,1536)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for k.2 (0,2)
          for i_c.4 (0,8)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1106	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN[((((((((int)blockIdx.x) / 12) * 49152) + (i_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((int)threadIdx.x)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
        for i_c.3 (0,8)
          T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1107	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
12) * 49152) + ((((int)threadIdx.x) >> 6) * 6144)) + (i_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + (((int)threadIdx.x) & 63))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.4 (0,8)
          T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1108	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + ((((int)threadIdx.x) % 12) * 4)) + j_inner) + 336)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for i_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1109	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1110	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)threadIdx.x) >> 7) * 24576)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) & 127)) + 128)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,4)
            for k.2 (0,4)
              for i_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1111	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[((((((((int)threadIdx.x) / 12) * 768) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) % 12) * 8)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
        for j_c.3 (0,8)
          T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1112	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
76) + (i_inner * 768)) + (((int)blockIdx.x) * 64)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 32)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,32)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1113	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN[((((((((int)threadIdx.x) >> 4) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) & 15)) + 112)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,512)
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for k.1 (0,3)
          for k.2 (0,2)
            for i_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1114	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 4)) + j_inner) + 1536)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,3072)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1115	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1116	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_local[(i_inner + 128)];
    T_batch_matmul_NN[((((((int)blockIdx.x) * 49152) + (i_inner * 768)) + ((int)threadIdx.x)) + 576)] = T_batch_matmul_NN_local[(i_inner + 192)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,32)
              T_batch_matmul_NN.local = ...
      for i.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1117	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)threadIdx.x) / 384) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) % 384)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,6)
            for i_c.4 (0,32)
              T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1118	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 2)) + j_inner) + 1536)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 4)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1119	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) / 192) * 6144)) + (i_inner * 768)) + ((((int)threadIdx.x) % 192) * 2)) + j_inner) + 24960)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.12, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1120	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((int)threadIdx.x) >> 6) * 12288)) + (i_inner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + (((int)threadIdx.x) & 63)) + 192)] = T_batch_matmul_NN_local[(i_inner + 48)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647324.20)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,4)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,16)
        T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1121	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) >> 1) * 6144) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 6) * 64)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        p0.shared = ...
      for j_c.3 (0,2)
        for j_c.4 (0,32)
          T_batch_matmul_NN.local = ...
    for j.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1122	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 6) * 49152) + ((((int)threadIdx.x) / 12) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.2 (0,8)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1123	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1124	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 6) * 49152) + ((((int)threadIdx.x) / 12) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1125	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[((((((((int)threadIdx.x) >> 4) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) & 15)) + 112)] = T_batch_matmul_NN_local[(i_inner + 224)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,3072)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for i_c.4 (0,32)
          T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1126	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x))] = T_batch_matmul_NN_local[i_inner];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)threadIdx.x)) + 384)] = T_batch_matmul_NN_local[(i_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p0.shared = ...
      for i_c.3 (0,4)
        for i_c.4 (0,32)
          T_batch_matmul_NN.local = ...
    for i.3 (0,128)
      T_batch_matmul_NN = ...

==================================================
No: 1127	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((int)threadIdx.x))] = T_batch_matmul_NN_local[i_inner];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)threadIdx.x)) + 384)] = T_batch_matmul_NN_local[(i_inner + 128)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,1536)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,4)
        for k.2 (0,2)
          for i_c.4 (0,32)
            T_batch_matmul_NN.local = ...
    for i.3 (0,128)
      T_batch_matmul_NN = ...

==================================================
No: 1128	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.36, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,8)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1129	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[((((((((int)threadIdx.x) / 12) * 768) + (((int)blockIdx.x) * 96)) + ((((int)threadIdx.x) % 12) * 8)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for j_c.3 (0,8)
          T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1130	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 24704)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,3072)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1131	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((((int)threadIdx.x) / 384) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 384)) + (((int)threadIdx.x) % 384)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,768)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,6)
            for i_c.4 (0,32)
              T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1132	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 48) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for j_c.3 (0,4)
        for i_c.4 (0,2)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1133	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 4)) + j_inner) + 1536)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,3072)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1134	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[((((((((int)threadIdx.x) >> 7) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.x) & 127)) + 128)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,4)
            for k.2 (0,4)
              for i_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1135	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er + 160)];
    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 3) * 768) + ((((int)threadIdx.x) & 7) * 16)) + j_inner) + 49792)] = T_batch_matmul_NN_local[(j_inner + 176)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,12)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for j_c.3 (0,4)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1136	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N[((((((((int)threadIdx.x) >> 4) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) & 15)) + 112)] = T_batch_matmul_NN_local[(i_inner + 224)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
        for k.2 (0,2)
          for i_c.4 (0,32)
            T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1137	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + (((int)threadIdx.x) * 2)) + j_inner) + 1536)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 4)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,3072)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1138	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 768)) + (((int)blockIdx.x) * 256)) + ((((int)threadIdx.x) & 63) * 2)) + j_inner) + 128)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 64)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          vectorize ax0@ax1@ax2@.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,16)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1139	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
>> 2) * 49152) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) % 12) * 16)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p0.shared = ...
      for j_c.3 (0,4)
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for j.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1140	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) & 1) * 384)) + ((((int)threadIdx.x) % 48) * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,3072)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,4)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1141	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 128) + ((int)threadIdx.x)) + 96)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((int)blockIdx.x) * 128) + ((int)threadIdx.x)) + 112)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,1536)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1142	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)blockIdx.x) >> 6) * 49152) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 6)) + 24582)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.13, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,22)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,32)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1143	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) >> 6) * 49152) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      for k.2 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1144	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN[(((((((int)blockIdx.x) / 48) * 12288) + ((((int)threadIdx.x) >> 4) * 768)) + ((((int)blockIdx.x) % 48) * 16)) + (((int)threadIdx.x) & 15))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,6)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1145	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((((int)blockIdx.x) >> 6) * 49152) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 6)) + 24582)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      for k.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,22)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,16)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1146	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N_local[2];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 12288) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + 6145)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1147	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((((int)blockIdx.x) >> 5) * 24576) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 31) * 24)) + (((int)threadIdx.x) % 12)) + 12)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
        for k.1 (0,3)
          for k.2 (0,8)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1148	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((((int)blockIdx.x) >> 6) * 12288) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 6)) + 6150)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,6)
          for k.2 (0,32)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1149	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(((int)blockIdx.x) >> 6) * 49152) + ((((int)threadIdx.x) / 12) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + (((int)threadIdx.x) % 12)) + 24576)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for k.1 (0,3)
          for k.2 (0,8)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1150	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
1) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) % 96) * 8)) + ((((int)threadIdx.x) & 1) * 4)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.12, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,96)
        for i_c.3 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1151	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) % 6) * 16)) + j_inner) + 24672)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,16)
            for k.2 (0,8)
              for i_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 1152	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 96)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647326.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,32)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 5.06 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1153	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 1536) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1154	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 3)] * p1_shared[3]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1155	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,384)
      vectorize ax0@ax1@ax2@.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1156	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1157	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1158	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0]));
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[1] * p1_shared[1]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,2)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1159	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 3)] * p1_shared[3]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1160	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1161	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 8) * 1536) + ((((int)blockIdx.x) & 255) * 3)) + ((int)threadIdx.x)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,3)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1162	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 1)] * p1_shared[1]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1163	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[0] + (p0_shared[1] * p1_shared[(((int)threadIdx.x) + 4)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p0.shared = ...
      for k.2 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1164	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[6]));
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[7] * p1_shared[7]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,96)
    for ax0@ax1@ax2@.0.0 (0,2)
      vectorize ax0@ax1@ax2@.1 (0,6)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,3)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for k.1 (0,8)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1165	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1166	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[0];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 8) * 1536) + ((((int)blockIdx.x) & 255) * 3)) + ((int)threadIdx.x)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,3)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,3)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1167	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[6]));
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[7] * p1_shared[7]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,96)
    for ax0@ax1@ax2@.0.0 (0,8)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,8)
      p0.shared = ...
    for k.1 (0,8)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1168	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[6]));
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[7] * p1_shared[7]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,96)
    for ax0@ax1@ax2@.0.0 (0,8)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,3)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for k.1 (0,8)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1169	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1170	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[0] + (p0_shared[3] * p1_shared[(((int)threadIdx.x) + 6)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1171	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1172	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 3)] * p1_shared[3]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1173	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1174	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[0] + (p0_shared[2] * p1_shared[(((int)threadIdx.x) + 4)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,256)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,3)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1175	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 1536) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,256)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for k.1 (0,3)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1176	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1177	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1178	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 1536) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1179	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1180	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 1)] * p1_shared[1]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1181	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 6144) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1182	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((((((int)blockIdx.x) / 24) * 1536) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) % 24) * 32)) + (((int)threadIdx.x) & 7)) + 24)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
        for k.2 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1183	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1184	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_shared[23]));
  }
  T_batch_matmul_NN[(((int)blockIdx.x) * 2)] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647347.29)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        p0.shared = ...
      for k.1 (0,12)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1185	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_matmul_NN_local[6];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 192) * 6144) + ((((int)blockIdx.x) % 192) * 4)) + ((int)threadIdx.x)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647349.75)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          vectorize ax0@ax1@ax2@.1 (0,6)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1186	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0] + (p0_shared[11] * p1_shared[(((int)threadIdx.x) + 44)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,64)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,16)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,12)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1187	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l_NN_local[0] + (p0_shared[15] * p1_shared[(((int)threadIdx.x) + 60)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,48)
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,8)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1188	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1189	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,384)
      vectorize ax0@ax1@ax2@.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1190	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1191	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_shared[23]));
  }
  T_batch_matmul_NN[(((int)blockIdx.x) * 2)] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,24)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for k.1 (0,12)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1192	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1193	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN_local[3];
  T_batch_matmul_NN[((((int)blockIdx.x) * 6) + 4)] = T_batch_matmul_NN_local[4];
  T_batch_matmul_NN[((((int)blockIdx.x) * 6) + 5)] = T_batch_matmul_NN_local[5];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  vthread b.1@i.1@j.1@ (0,6)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,48)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1194	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((int)blockIdx.x) / 384) * 12288) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1)) + 9216)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1195	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[4];
  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 7) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) & 127) * 6)) + 5)] = T_batch_matmul_NN_local[5];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,96)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p0.shared = ...
        for k.1 (0,8)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1196	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[2];
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768)) + 18432)] = T_batch_matmul_NN_local[3];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1197	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1198	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1199	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1200	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_shared[23]));
  }
  T_batch_matmul_NN[(((int)blockIdx.x) * 2)] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,24)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        p0.shared = ...
      for k.1 (0,12)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1201	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,24)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1202	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1203	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 192) * 1536) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,4)
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1204	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1205	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_shared[23]));
  }
  T_batch_matmul_NN[(((int)blockIdx.x) * 2)] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + 1)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,24)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for k.1 (0,12)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1206	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 6144) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1207	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1208	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 192) * 1536) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1209	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,32)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1210	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 1)] * p1_shared[1]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1211	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,11)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for k.1 (0,8)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1212	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      for k.2 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1213	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 7)] * p1_shared[7]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1214	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner * 768)) + ((((int)blockIdx.x) % 12) * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 3072)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 4)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,2)
            for k.2 (0,12)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1215	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,256)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for k.1 (0,3)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1216	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 384) * 3072) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647349.76)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

Time elapsed for measurement: 4.96 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.T.T.T
==================================================
No: 1217	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) >> 2) * 16384) + ((((int)blockIdx.x) & 3) * 2048)) + (i_inner * 64)) + j_inner) + 9728)] = T_batch_matmul_NN_local[(((i_inner * 64) + j_inner) + 3584)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.15, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,1024)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,512)
        p0.shared = ...
      for j_c.3 (0,64)
        for k.2 (0,8)
          for i_c.4 (0,8)
            T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1218	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
lockIdx.x) >> 9) * 49152) + (b_inner * 8192)) + ((((int)blockIdx.x) & 511) * 16)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((b_inner * 16) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,192)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,4)
          for b_c.4 (0,3)
            T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 1219	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,3)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,3072)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,384)
        p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,2)
          for i_c.3 (0,2)
            for b_c.4 (0,2)
              for i_c.4 (0,4)
                for j_c.4 (0,64)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,4)
      for i.3 (0,8)
        for j.3 (0,64)
          T_batch_matmul_NN = ...

==================================================
No: 1220	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
lockIdx.x) >> 9) * 49152) + (b_inner * 8192)) + ((((int)blockIdx.x) & 511) * 16)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((b_inner * 16) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,384)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,4)
          for b_c.4 (0,3)
            T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 1221	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN[((((b_inner * 8192) + (((int)blockIdx.x) * 512)) + (i_inner * 64)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 512) + (i_inner * 64)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:9.82, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,32)
    for ax0@ax1@ax2@.0.0 (0,3072)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,384)
      p0.shared = ...
    for k.1 (0,4)
      for b_c.3 (0,12)
        for i_c.3 (0,2)
          for i_c.4 (0,4)
            for j_c.4 (0,64)
              T_batch_matmul_NN.local = ...
  for b.3 (0,12)
    for i.3 (0,8)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1222	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
lockIdx.x) >> 9) * 49152) + (b_inner * 8192)) + ((((int)blockIdx.x) & 511) * 16)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((b_inner * 16) + j_inner) + 48)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,192)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        p0.shared = ...
      for j_c.3 (0,16)
        for k.2 (0,4)
          for b_c.4 (0,3)
            T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 1223	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) >> 1) * 49152) + ((((int)blockIdx.x) & 1) * 4096)) + (i_inner * 64)) + j_inner) + 43008)] = T_batch_matmul_NN_local[(((i_inner * 64) + j_inner) + 22528)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,12)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,3072)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3072)
        p0.shared = ...
      for k.1 (0,4)
        for i_c.3 (0,4)
          for k.2 (0,2)
            for i_c.4 (0,8)
              for j_c.4 (0,64)
                T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1224	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,32)
    for ax0@ax1@ax2@.0.0 (0,3072)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,384)
      p0.shared = ...
    for k.1 (0,4)
      for b_c.3 (0,6)
        for i_c.3 (0,2)
          for b_c.4 (0,2)
            for i_c.4 (0,4)
              for j_c.4 (0,64)
                T_batch_matmul_NN.local = ...
  for b.3 (0,12)
    for i.3 (0,8)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1225	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
h_matmul_NN[((((b_inner * 8192) + (i_inner * 64)) + j_inner) + 77824)] = T_batch_matmul_NN_local[((((b_inner * 4096) + (i_inner * 64)) + j_inner) + 86016)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:4.07, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,32)
    for ax0@ax1@ax2@.0.0 (0,3072)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,6144)
      p0.shared = ...
    for b_c.3 (0,3)
      for j_c.3 (0,2)
        for k.2 (0,4)
          for i_c.4 (0,64)
            for j_c.4 (0,32)
              T_batch_matmul_NN.local = ...
  for b.3 (0,3)
    for i.3 (0,64)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1226	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x) & 3) * 2048)) + (((((int)threadIdx.x) & 255) >> 4) * 128)) + (i_inner * 64)) + (((int)threadIdx.x) & 15)) + 48)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1227	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 1024)) + (((((int)threadIdx.x) & 127) >> 4) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 15))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.39, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,4)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.3 (0,2)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1228	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x) & 3) * 2048)) + (((((int)threadIdx.x) & 255) >> 4) * 128)) + (i_inner * 64)) + (((int)threadIdx.x) & 15)) + 48)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,1024)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,1024)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1229	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x) >> 4) * 256)) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 16) + (i_inner * 4)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.41, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,8)
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for j_c.3 (0,2)
          for k.2 (0,4)
            for b_c.4 (0,4)
              for i_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,4)
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1230	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
+ ((((int)threadIdx.x) >> 4) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 1) * 32)) + (((int)threadIdx.x) & 15)) + 16400)] = T_batch_matmul_NN_local[(i_inner + 10)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,6)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,2)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1231	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 128) + (i_inner * 4)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,2)
        for b_c.3 (0,2)
          for i_c.3 (0,32)
            for j_c.3 (0,4)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,32)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1232	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
& 63) >> 3) * 256)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 32) + (i_inner * 8)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.36, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,192)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
          p0.shared = ...
      for k.1 (0,4)
        for i_c.3 (0,4)
          for j_c.3 (0,4)
            for k.2 (0,2)
              for b_c.4 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,4)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1233	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 1) * 4096)) + ((((int)threadIdx.x) >> 2) * 64)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 2096)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 168)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,3)
            for j_c.3 (0,2)
              for k.2 (0,2)
                for b_c.4 (0,2)
                  for j_c.4 (0,2)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1234	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6)) + (i_inner * 64)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 1568)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 8)) + j_inner) + 672)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.48, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,3)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for k.2 (0,8)
                for i_c.4 (0,2)
                  for j_c.4 (0,4)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,4)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 1235	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
96)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 32)] = T_batch_matmul_NN_local[((((b_inner * 256) + (i_inner * 4)) + j_inner) + 768)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,8)
          for b_c.3 (0,3)
            for i_c.3 (0,2)
              for j_c.3 (0,4)
                for i_c.4 (0,32)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,64)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1236	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 3072)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 8)) + j_inner) + 144)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,8)
          for b_c.3 (0,3)
            for i_c.3 (0,2)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,2)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 1237	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 1072)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 2)) + j_inner) + 224)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.41, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,32)
          for b_c.3 (0,2)
            for j_c.3 (0,2)
              for i_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,8)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1238	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) & 15) * 512)) + (i_inner * 64)) + (((int)blockIdx.x) * 16)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 128) + (i_inner * 16)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for b_c.4 (0,2)
                for i_c.4 (0,4)
                  for j_c.4 (0,8)
                    T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,8)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 1239	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) & 15) * 256)) + (i_inner * 64)) + ((((int)blockIdx.x) & 7) * 8)) + j_inner) + 6)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 24)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for k.2 (0,4)
                for i_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1240	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 15) >> 1) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 16)] = T_batch_matmul_NN_local[(((b_inner * 8) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for j_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1241	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
threadIdx.x b.2@i.2@j.2@ (0,32)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,32)
    for ax0@ax1@ax2@.0.0 (0,96)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        p0.shared = ...
    for k.1 (0,4)
      for b_c.3 (0,3)
        for i_c.3 (0,2)
          for b_c.4 (0,4)
            for i_c.4 (0,2)
              for j_c.4 (0,64)
                T_batch_matmul_NN.local = ...
  for b.3 (0,12)
    for i.3 (0,4)
      for j.3 (0,64)
        T_batch_matmul_NN = ...

==================================================
No: 1242	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
& 31) >> 3) * 128)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 16) + (i_inner * 8)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,6)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for k.2 (0,2)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,2)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1243	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 3072)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 8)) + j_inner) + 144)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.44, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,3)
            for i_c.3 (0,2)
              for j_c.4 (0,8)
                T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,2)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 1244	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
Idx.x) & 15) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 15) * 4)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 8) + (i_inner * 4)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p0.shared = ...
      for k.1 (0,2)
        for k.2 (0,4)
          for b_c.4 (0,2)
            for i_c.4 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1245	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
4)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 16) + (i_inner * 4)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  threadIdx.x b.2@i.2@j.2@ (0,128)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          p0.shared = ...
      for k.1 (0,2)
        for b_c.3 (0,3)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for i_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for i.3 (0,4)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1246	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 1) * 4)) + j_inner) + 1048)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 336)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.50, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,16)
          for j_c.3 (0,2)
            for b_c.4 (0,3)
              for i_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,4)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1247	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 2048)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 4)) + j_inner) + 24)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,4)
            for k.2 (0,16)
              for b_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,2)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1248	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
i_inner * 64)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 49536)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647386.73)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for k.2 (0,2)
              for j_c.4 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.T.T
==================================================
No: 1249	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
48)) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 32)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 2)) + j_inner) + 128)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:2.45, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for k.2 (0,4)
                for i_c.4 (0,8)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,32)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1250	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) >> 2) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 1552)] = T_batch_matmul_NN_local[(((b_inner * 4) + j_inner) + 56)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,48)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.4 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1251	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x) >> 3) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) & 7)) + 8)] = T_batch_matmul_NN_local[(((b_inner * 2) + i_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
          vectorize ax0@ax1@ax2@.1 (0,8)
            p0.shared = ...
        for b_c.3 (0,6)
          for i_c.3 (0,2)
            for k.2 (0,4)
              T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1252	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
Idx.x) >> 2) * 64)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 8)] = T_batch_matmul_NN_local[(((b_inner * 2) + j_inner) + 6)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,2)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,64)
          for b_c.3 (0,3)
            for j_c.3 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1253	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) & 15) >> 2) * 512)) + (i_inner * 64)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 56)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 112)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.53, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,192)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,192)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.4 (0,8)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1254	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 32768)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 32)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for i_c.3 (0,4)
              for j_c.3 (0,2)
                for k.2 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1255	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + ((((int)blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 64) + (i_inner * 2)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.45, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,4)
        for b_c.3 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for i_c.4 (0,16)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,32)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1256	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
28)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 2096)] = T_batch_matmul_NN_local[((((b_inner * 4) + (i_inner * 2)) + j_inner) + 112)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.65, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,384)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
            p0.shared = ...
        for k.1 (0,8)
          for j_c.3 (0,2)
            for b_c.4 (0,4)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,2)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1257	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,4)
          for b_c.3 (0,2)
            for i_c.3 (0,16)
              for j_c.3 (0,8)
                for b_c.4 (0,3)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,16)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 1258	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
er * 8192)) + ((((int)threadIdx.x) & 31) * 64)) + ((((int)blockIdx.x) & 1) * 32)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(((b_inner * 32) + j_inner) + 192)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for j_c.3 (0,16)
          for b_c.4 (0,2)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for j.3 (0,32)
          T_batch_matmul_NN = ...

==================================================
No: 1259	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)threadIdx.x) >> 5) * 512)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner) + 4096)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 16)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for i_c.4 (0,8)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1260	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for k.2 (0,2)
                for i_c.4 (0,4)
                  for j_c.4 (0,8)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,16)
          for j.3 (0,16)
            T_batch_matmul_NN = ...

==================================================
No: 1261	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8192)) + (((int)threadIdx.x) * 128)) + (i_inner * 64)) + j_inner) + 4128)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 32)) + j_inner) + 768)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,2)
            for k.2 (0,8)
              for b_c.4 (0,2)
                for i_c.4 (0,2)
                  for j_c.4 (0,32)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,2)
          for j.3 (0,32)
            T_batch_matmul_NN = ...

==================================================
No: 1262	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
128)) + (i_inner * 64)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner) + 48)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 8)) + j_inner) + 288)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,128)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,128)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,3)
            for i_c.3 (0,2)
              for j_c.3 (0,8)
                for b_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,2)
          for j.3 (0,8)
            T_batch_matmul_NN = ...

==================================================
No: 1263	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
48)) + (i_inner * 64)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner) + 56)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 2)) + j_inner) + 2688)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,4)
        for i_c.3 (0,32)
          for b_c.4 (0,6)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
    for b.3 (0,6)
      for i.3 (0,32)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1264	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 15) * 512)) + (i_inner * 64)) + (((int)threadIdx.x) * 2)) + j_inner) + 256)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 48)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for b_c.3 (0,3)
            for j_c.3 (0,2)
              for k.2 (0,2)
                for b_c.4 (0,2)
                  for i_c.4 (0,4)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,6)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1265	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
lockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 3072)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 96)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:1.91, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for b_c.4 (0,2)
                for i_c.4 (0,2)
                  for j_c.4 (0,2)
                    T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1266	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 16416)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 2)) + j_inner) + 384)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.2 (0,2)
          for b_c.4 (0,2)
            for i_c.4 (0,32)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,32)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1267	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
15) >> 3) * 4096)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 512) + (i_inner * 8)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for b_c.3 (0,2)
        for i_c.3 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,8)
              for i_c.4 (0,32)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,64)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1268	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) * 1024)) + (((((int)threadIdx.x) & 63) >> 3) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 7) * 8)) + (((int)threadIdx.x) & 7))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,4)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.3 (0,2)
          for k.2 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1269	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)threadIdx.x) & 15) * 128)) + (i_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for j_c.3 (0,16)
          for k.2 (0,8)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,16)
          T_batch_matmul_NN = ...

==================================================
No: 1270	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 16)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 4)) + j_inner) + 192)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,3)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for k.2 (0,8)
                for i_c.4 (0,8)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,16)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1271	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
threadIdx.x) & 127) >> 3) * 128)) + (i_inner * 64)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 48)] = T_batch_matmul_NN_local[(((i_inner * 2) + j_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.16, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,512)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for j_c.3 (0,2)
              for k.2 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,2)
          T_batch_matmul_NN = ...

==================================================
No: 1272	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4096) + ((((int)threadIdx.x) >> 5) * 512)) + (i_inner * 64)) + ((((int)threadIdx.x) & 31) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24)
  threadIdx.x b.2@i.2@j.2@ (0,256)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,4)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
          p0.shared = ...
      for k.1 (0,16)
        for j_c.3 (0,2)
          for k.2 (0,2)
            for i_c.4 (0,8)
              T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1273	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 6144)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 4)) + j_inner) + 384)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for k.2 (0,16)
              for b_c.4 (0,4)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,4)
        for i.3 (0,8)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1274	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) & 1) * 32)) + ((((int)threadIdx.x) & 7) * 4)) + j_inner) + 65536)] = T_batch_matmul_NN_local[((((b_inner * 16) + (i_inner * 4)) + j_inner) + 64)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for b_c.3 (0,2)
          for i_c.3 (0,4)
            for j_c.3 (0,4)
              for k.2 (0,4)
                T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,4)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1275	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t)blockIdx.x) * 16)) + ((((int)threadIdx.x) & 3) * 4)) + j_inner) + 73728)] = T_batch_matmul_NN_local[((((b_inner * 32) + (i_inner * 4)) + j_inner) + 288)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for i_c.3 (0,8)
          for j_c.3 (0,4)
            for k.2 (0,2)
              for b_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for b.3 (0,3)
        for i.3 (0,8)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1276	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt)threadIdx.x) >> 2) * 128) + (i_inner * 64)) + ((((int)threadIdx.x) & 3) * 16)) + j_inner) + 73728)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 96)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,4)
  threadIdx.x b.2@i.2@j.2@ (0,768)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,128)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,768)
          p0.shared = ...
      for i_c.3 (0,2)
        for j_c.3 (0,4)
          for j_c.4 (0,4)
            T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 1277	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2)) + (i_inner * 64)) + ((((int)threadIdx.x) & 3) * 8)) + j_inner) + 6176)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 8)) + j_inner) + 896)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:3.06, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
          p0.shared = ...
      for i_c.3 (0,4)
        for j_c.3 (0,4)
          for b_c.4 (0,2)
            for i_c.4 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for b.3 (0,2)
      for i.3 (0,8)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

==================================================
No: 1278	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 15) * 2)) + j_inner) + 4096)] = T_batch_matmul_NN_local[((((b_inner * 8) + (i_inner * 2)) + j_inner) + 96)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.14, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,256)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,256)
            p0.shared = ...
        for k.1 (0,4)
          for i_c.3 (0,4)
            for j_c.3 (0,2)
              for b_c.4 (0,12)
                T_batch_matmul_NN.local = ...
      for b.3 (0,12)
        for i.3 (0,4)
          for j.3 (0,2)
            T_batch_matmul_NN = ...

==================================================
No: 1279	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + (i_inner * 64)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 49152)] = T_batch_matmul_NN_local[((((b_inner * 64) + (i_inner * 4)) + j_inner) + 128)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,96)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
            p0.shared = ...
        for k.1 (0,8)
          for b_c.3 (0,2)
            for i_c.3 (0,8)
              for i_c.4 (0,2)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
      for b.3 (0,2)
        for i.3 (0,16)
          for j.3 (0,4)
            T_batch_matmul_NN = ...

==================================================
No: 1280	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)) + ((((int)blockIdx.x) & 3) * 16)) + ((((int)threadIdx.x) & 1) * 8)) + j_inner)] = T_batch_matmul_NN_local[(((b_inner * 128) + (i_inner * 8)) + j_inner)];
      }
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647403.55)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,8)
        for i_c.3 (0,8)
          for j_c.3 (0,2)
            for b_c.4 (0,3)
              for i_c.4 (0,2)
                for j_c.4 (0,4)
                  T_batch_matmul_NN.local = ...
    for b.3 (0,3)
      for i.3 (0,16)
        for j.3 (0,8)
          T_batch_matmul_NN = ...

Time elapsed for measurement: 34.05 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1281	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) + 1)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  T_softmax_maxelem[((int)blockIdx.x)] = ((volatile float*)red_buf0)[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,768)
  threadIdx.x i0@i1@i2@.1 (0,2)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,4096)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,48)
    T_broadcast_to = ...

==================================================
No: 1282	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 1)) >> 6)])) / T_softmax_expsum[(((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 1)) >> 6)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,32768)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,6)
    T_broadcast_to = ...

==================================================
No: 1283	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,65536)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,3)
    T_broadcast_to = ...

==================================================
No: 1284	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) + 1)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  T_softmax_maxelem[((int)blockIdx.x)] = ((volatile float*)red_buf0)[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,8192)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,24)
    T_broadcast_to = ...

==================================================
No: 1285	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,8192)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,24)
    T_broadcast_to = ...

==================================================
No: 1286	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) + 1)]);
    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;
  }
  __syncthreads();
  T_softmax_maxelem[((int)blockIdx.x)] = ((volatile float*)red_buf0)[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,4096)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,48)
    T_broadcast_to = ...

==================================================
No: 1287	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
24) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,64)
  threadIdx.x i0@i1@i2@.1 (0,24)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1288	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1289	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1290	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1291	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1292	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1293	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1294	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1295	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1296	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1297	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
24) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,64)
  threadIdx.x i0@i1@i2@.1 (0,24)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1298	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1299	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1300	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1301	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1302	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1303	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1304	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1305	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 6) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,256)
  threadIdx.x i0@i1@i2@.1 (0,6)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1306	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647429.07)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,64)
    threadIdx.x k.1 (0,2)
      T_softmax_expsum = ...
  for ax3.0 (0,64)
    threadIdx.x ax3.1 (0,2)
      T_broadcast_to = ...

==================================================
No: 1307	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) >> 4)])) / T_softmax_expsum[(((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) >> 4)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
T_softmax_expsum auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,8192)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,24)
    T_broadcast_to = ...

==================================================
No: 1308	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.11, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1309	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1310	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1311	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1312	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.10, Tstamp:1688647429.07)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1313	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1314	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1315	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.35, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1316	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1317	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1318	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,16)
    threadIdx.x k.1 (0,8)
      T_softmax_expsum = ...
  for ax3.0 (0,16)
    threadIdx.x ax3.1 (0,8)
      T_broadcast_to = ...

==================================================
No: 1319	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.41, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1320	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1321	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1322	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1323	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1324	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.40, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1325	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1326	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1327	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.38, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1328	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1329	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1330	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
, -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[((((int)blockIdx.x) * 128) + k)] - T_softmax_maxelem[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,4096)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,48)
    T_broadcast_to = ...

==================================================
No: 1331	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1332	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 1024
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1333	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1334	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 1024
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1335	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
48) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,32)
  threadIdx.x i0@i1@i2@.1 (0,48)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1336	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1337	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
48) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x i0@i1@i2@.0 (0,32)
  threadIdx.x i0@i1@i2@.1 (0,48)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1338	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 512
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1339	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 8192) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,24)
  threadIdx.x i0@i1@i2@.1 (0,64)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1340	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
dx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) >> 4)])) / T_softmax_expsum[(((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) >> 4)]);
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@ (0,1536)
  for k.0 (0,2)
    threadIdx.x k.1 (0,64)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,1536)
  for k (0,128)
    T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,8192)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,24)
    T_broadcast_to = ...

==================================================
No: 1341	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.15, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,384)
  threadIdx.x i0@i1@i2@.1 (0,4)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,32)
    threadIdx.x k.1 (0,4)
      T_softmax_expsum = ...
  for ax3.0 (0,32)
    threadIdx.x ax3.1 (0,4)
      T_broadcast_to = ...

==================================================
No: 1342	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 16
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_expsum = ...
blockIdx.x ax0@ax1@ax2@ax3@.0 (0,6144)
  threadIdx.x ax0@ax1@ax2@ax3@.1 (0,32)
    T_broadcast_to = ...

==================================================
No: 1343	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
0e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (p0[(((((int)blockIdx.x) * 128) + (ax3_outer * 32)) + ((int)threadIdx.x))] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647431.74)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 512
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_maxelem = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

==================================================
No: 1344	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], p0[(((((int)blockIdx.x) * 4096) + (((int)threadIdx.x) * 128)) + k)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647431.74)
==================================================
Placeholder: p0
T_softmax_maxelem auto_unroll: 64
blockIdx.x i0@i1@i2@.0 (0,48)
  threadIdx.x i0@i1@i2@.1 (0,32)
    for k (0,128)
      T_softmax_maxelem = ...
blockIdx.x ax0@ax1@ax2@ (0,1536)
  T_softmax_expsum auto_unroll: 64
  for k.0 (0,4)
    threadIdx.x k.1 (0,32)
      T_softmax_expsum = ...
  for ax3.0 (0,4)
    threadIdx.x ax3.1 (0,32)
      T_broadcast_to = ...

Time elapsed for measurement: 5.68 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1345	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1346	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647438.27)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1347	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1348	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1349	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1350	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1351	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1352	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1353	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1354	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1355	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647438.27)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1356	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1357	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1358	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647438.27)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1359	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1360	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1361	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1362	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1363	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.27)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1364	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.27)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1365	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1366	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1367	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1368	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1369	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1370	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1371	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1372	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1373	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1374	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1375	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1376	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647438.28)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1377	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1378	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1379	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1380	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1381	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1382	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1383	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1384	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1385	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1386	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1387	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1388	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1389	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1390	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1391	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1392	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1393	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1394	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1395	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1396	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1397	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1398	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1399	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1400	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1401	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1402	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1403	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1404	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1405	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1406	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1407	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1408	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647440.69)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

Time elapsed for measurement: 4.87 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1409	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ads();
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[0] * p1_shared[0]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    p1.shared = ...
    p0.shared = ...
    T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1410	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0]));
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[1] * p1_shared[1]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 16
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,2)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1411	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ch_matmul_NN_local[0] + (p0_shared[0] * p1_shared[((int)threadIdx.x)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1412	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[2]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,3)
      p1.shared = ...
    p0.shared = ...
    for j_c.4 (0,3)
      T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1413	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[2]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,3)
      p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,3)
      T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1414	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
kIdx.x) * 6) + ((int)threadIdx.x)) + 2)] = T_batch_matmul_NN_local[1];
  T_batch_matmul_NN[(((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) + 4)] = T_batch_matmul_NN_local[2];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
        for k.2 (0,2)
          T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1415	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1416	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN_local[0];
  T_batch_matmul_NN[((((int)blockIdx.x) * 3) + 1)] = T_batch_matmul_NN_local[1];
  T_batch_matmul_NN[((((int)blockIdx.x) * 3) + 2)] = T_batch_matmul_NN_local[2];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  vthread b.1@i.1@j.1@ (0,3)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,6)
        p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1417	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ocal[0] = (T_batch_matmul_NN_local[0] + (p0_shared[k_outer_inner] * p1_shared[k_outer_inner]));
    }
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,2)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1418	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 24)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 21504)] = T_batch_matmul_NN_local[(i_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,384)
      vectorize ax0@ax1@ax2@.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,11)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for k.1 (0,2)
        for i_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1419	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,384)
      vectorize ax0@ax1@ax2@.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for k.1 (0,2)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1420	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1421	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + ((((int)threadIdx.x) & 1) * 6)) + j_inner) + 21504)] = T_batch_matmul_NN_local[(j_inner + 42)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for j_c.4 (0,6)
          T_batch_matmul_NN.local = ...
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 1422	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.2 (0,2)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1423	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + ((((int)threadIdx.x) & 1) * 6)) + j_inner) + 21504)] = T_batch_matmul_NN_local[(j_inner + 42)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,8)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
            p0.shared = ...
        for j_c.4 (0,6)
          T_batch_matmul_NN.local = ...
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 1424	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[(((((((int)blockIdx.x) / 192) * 1536) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for k.1 (0,4)
        T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1425	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1426	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1427	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 12288)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,16)
        T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1428	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 12288)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for i_c.3 (0,16)
        T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1429	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_NN_local[(i_inner + 4)];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 1536) + (i_inner * 768)) + ((int)blockIdx.x)) + 73728)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,16)
            p0.shared = ...
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1430	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt i_inner = 0; i_inner < 8; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 6144) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p0.shared = ...
      for i_c.3 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1431	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1432	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) % 768)) + 4608)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (((int)blockIdx.x) % 768)) + 5376)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1433	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,8)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1434	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
68) + ((int)blockIdx.x)) + 73728)] = T_batch_matmul_NN_local[6];
  T_batch_matmul_NN[(((((int)threadIdx.x) * 768) + ((int)blockIdx.x)) + 86016)] = T_batch_matmul_NN_local[7];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
        T_batch_matmul_NN.local = ...
      T_batch_matmul_NN = ...

==================================================
No: 1435	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1436	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
h_matmul_NN_local[0] + (p0_shared[0] * p1_shared[((int)threadIdx.x)]));
  }
  T_batch_matmul_NN[((((int)blockIdx.x) * 96) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,96)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,96)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1437	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1438	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1439	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1440	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647461.00)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1441	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1442	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1443	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t j_inner = 0; j_inner < 384; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 384)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.48, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,384)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,384)
        T_batch_matmul_NN.local = ...
    for j.3 (0,384)
      T_batch_matmul_NN = ...

==================================================
No: 1444	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1445	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1446	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1447	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) >> 4) * 1536) + ((((int)threadIdx.x) / 24) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 24) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
        p0.shared = ...
      for k.2 (0,6)
        for j_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1448	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ul_NN[((((((((int)blockIdx.x) >> 2) * 3072) + (i_inner * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((int)threadIdx.x)) + 144)] = T_batch_matmul_NN_local[(i_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for k.1 (0,8)
          for i_c.3 (0,2)
            for k.2 (0,3)
              for i_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1449	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
8) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) & 31) * 24)) + ((((int)threadIdx.x) & 3) * 6)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(j_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,24)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,2)
            for k.2 (0,16)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for j.3 (0,6)
        T_batch_matmul_NN = ...

==================================================
No: 1450	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 96) * 49152) + ((((int)threadIdx.x) >> 3) * 12288)) + (i_inner * 768)) + ((((int)blockIdx.x) % 96) * 8)) + (((int)threadIdx.x) & 7))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,16)
        for k.2 (0,3)
          for i_c.4 (0,16)
            T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1451	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 768)) + ((((int)blockIdx.x) & 63) * 12)) + ((((int)threadIdx.x) & 1) * 6)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.60, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,8)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,3)
              for k.2 (0,48)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 1452	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 3072) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          p0.shared = ...
      for k.1 (0,24)
        for j_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1453	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) & 7) * 8)) + j_inner) + 128)] = T_batch_matmul_NN_local[(j_inner + 16)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1454	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN_local[(j_inner + 24)];
    T_batch_matmul_NN[((((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 4)) + j_inner) + 672)] = T_batch_matmul_NN_local[(j_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,24)
      T_batch_matmul_NN.local auto_unroll: 16
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,128)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
        for k.1 (0,2)
          for k.2 (0,2)
            for j_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1455	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NN[((((((((int)blockIdx.x) / 48) * 24576) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 48) * 16)) + j_inner) + 8)] = T_batch_matmul_NN_local[(j_inner + 8)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,192)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      for k.0 (0,12)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for j_c.3 (0,8)
            for k.2 (0,32)
              T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1456	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 / 192) * 12288) + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) % 192) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,192)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.2 (0,4)
        for j_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1457	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x) >> 6) * 6144) + ((((int)threadIdx.x) >> 2) * 768)) + ((((int)blockIdx.x) & 63) * 12)) + ((((int)threadIdx.x) & 3) * 3)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for k.1 (0,12)
        for k.2 (0,2)
          for j_c.4 (0,3)
            T_batch_matmul_NN.local = ...
    for j.3 (0,3)
      T_batch_matmul_NN = ...

==================================================
No: 1458	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) >> 1) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) & 1) * 24)) + j_inner) + 6144)] = T_batch_matmul_NN_local[(j_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.34, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,48)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,4)
          for j_c.3 (0,8)
            for k.2 (0,4)
              for j_c.4 (0,3)
                T_batch_matmul_NN.local = ...
      for j.3 (0,24)
        T_batch_matmul_NN = ...

==================================================
No: 1459	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + ((((int)threadIdx.x) / 12) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) & 31) * 24)) + (((int)threadIdx.x) % 12)) + 12)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.16, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,4)
        for ax0@ax1@ax2@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
            p0.shared = ...
        for k.1 (0,16)
          for k.2 (0,12)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1460	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atmul_NN[((((((((int)blockIdx.x) >> 4) * 1536) + ((((int)blockIdx.x) & 15) * 48)) + (((int)threadIdx.x) * 2)) + j_inner) + 768)] = T_batch_matmul_NN_local[(j_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1024)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,24)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,24)
            p0.shared = ...
        for k.1 (0,4)
          for k.2 (0,6)
            for j_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1461	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 3072) + (i_inner * 768)) + ((((int)blockIdx.x) % 384) * 2)) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,24)
        for i_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1462	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) {
    T_batch_matmul_NN[(((((((int)threadIdx.x) / 3) * 768) + (((int)blockIdx.x) * 6)) + ((((int)threadIdx.x) % 3) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,384)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,16)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,384)
          p0.shared = ...
      for k.1 (0,6)
        for k.2 (0,8)
          for j_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1463	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 3072) + (i_inner * 768)) + ((((int)blockIdx.x) % 384) * 2)) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,32)
      for ax0@ax1@ax2@.0.0 (0,12)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,24)
        for i_c.4 (0,4)
          T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1464	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[(((((((int)blockIdx.x) / 12) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 12) * 64)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.53, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,96)
      for ax0@ax1@ax2@.0.0 (0,86)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for k.1 (0,8)
        for j_c.3 (0,64)
          T_batch_matmul_NN.local = ...
    for j.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1465	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
((((int)threadIdx.x) >> 2) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 3)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 14)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.33, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,384)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,16)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,16)
        for ax0@ax1@ax2@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
            p0.shared = ...
        for k.1 (0,24)
          for k.2 (0,2)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1466	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
NN[((((((((int)blockIdx.x) / 192) * 12288) + (i_inner * 768)) + ((((int)blockIdx.x) % 192) * 4)) + ((int)threadIdx.x)) + 6146)] = T_batch_matmul_NN_local[(i_inner + 24)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,192)
        for ax0@ax1@ax2@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,4)
            for i_c.4 (0,4)
              T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1467	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[(((((((int)blockIdx.x) >> 4) * 6144) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,16)
      for ax0@ax1@ax2@.0.0 (0,288)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,48)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          p0.shared = ...
      for k.1 (0,24)
        for j_c.3 (0,48)
          for k.2 (0,2)
            T_batch_matmul_NN.local = ...
    for j.3 (0,48)
      T_batch_matmul_NN = ...

==================================================
No: 1468	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
144) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) & 7) * 2)) + j_inner) + 32)] = T_batch_matmul_NN_local[(j_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,2)
            for k.2 (0,4)
              T_batch_matmul_NN.local = ...
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1469	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
   T_batch_matmul_NN[(((((((int)blockIdx.x) / 24) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 24) * 32)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.15, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,192)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for k.1 (0,4)
        for j_c.4 (0,32)
          T_batch_matmul_NN.local = ...
    for j.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1470	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
inner * 768)) + ((((int)blockIdx.x) & 63) * 12)) + ((((int)threadIdx.x) & 1) * 6)) + j_inner) + 24576)] = T_batch_matmul_NN_local[(((i_inner * 6) + j_inner) + 12)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,32)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,64)
        for ax0@ax1@ax2@.0.0 (0,5)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
            p0.shared = ...
        for k.1 (0,2)
          for i_c.3 (0,2)
            for j_c.3 (0,3)
              for k.2 (0,6)
                for j_c.4 (0,2)
                  T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,6)
          T_batch_matmul_NN = ...

==================================================
No: 1471	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
6144) + ((((int)threadIdx.x) / 6) * 768)) + ((((int)blockIdx.x) & 15) * 48)) + ((((int)threadIdx.x) % 6) * 4)) + j_inner) + 24)] = T_batch_matmul_NN_local[(j_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,256)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,48)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p0.shared = ...
        for j_c.4 (0,4)
          T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1472	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((((int)threadIdx.x) >> 1) * 3072)) + (i_inner * 768)) + ((((int)blockIdx.x) % 192) * 4)) + (((int)threadIdx.x) & 1)) + 6146)] = T_batch_matmul_NN_local[(i_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.48, Tstamp:1688647463.82)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,4)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,12)
        for ax0@ax1@ax2@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            vectorize ax0@ax1@ax2@.1 (0,2)
              p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,64)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
            vectorize ax0@ax1@ax2@.1 (0,4)
              p0.shared = ...
        for i_c.3 (0,2)
          for k.2 (0,64)
            for i_c.4 (0,2)
              T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

Time elapsed for measurement: 5.31 s
Get 58 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1473	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1474	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1475	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1476	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1477	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1478	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1479	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1480	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1481	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1482	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1483	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1484	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1485	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1486	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1487	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1488	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1489	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1490	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1491	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1492	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1493	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1494	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 8) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1495	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1496	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 4) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1497	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1498	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1499	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1500	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1501	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1502	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 32) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1503	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1504	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 64) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647470.73)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1505	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ockIdx.x) * 16) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1506	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1507	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
)blockIdx.x) * 2) + ((int)threadIdx.x))]) * (p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)] - p1[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1508	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 1509	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 1510	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 1511	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1512	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 1513	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1514	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1515	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1516	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1517	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1518	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1519	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1520	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1521	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1522	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1523	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1524	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1525	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1526	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1527	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1528	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1529	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1530	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
= (T_multiply_red[((int)blockIdx.x)] + ((p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)]) * (p0[((((int)blockIdx.x) * 768) + k2)] - p1[((int)blockIdx.x)])));
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647473.22)
==================================================
Placeholder: p0, p1
T_multiply_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    T_multiply_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

Time elapsed for measurement: 5.01 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1531	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for i_c.3 (0,2)
      T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    T_batch_matmul_NN = ...

==================================================
No: 1532	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 1536) + (i_inner * 768)) + ((((int)blockIdx.x) % 384) * 2)) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1533	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1534	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ads();
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[0] * p1_shared[0]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,768)
    p1.shared = ...
    p0.shared = ...
    T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1535	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ads();
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[0] * p1_shared[0]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  for k.0 (0,768)
    p1.shared = ...
    p0.shared = ...
    T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1536	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 48)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p0.shared = ...
      for i_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1537	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[2]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    vectorize ax0@ax1@ax2@.1 (0,3)
      p1.shared = ...
    p0.shared = ...
    for j_c.4 (0,3)
      T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1538	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 12288)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.4 (0,16)
        T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1539	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NN_local[i_inner];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 24576) + (i_inner * 768)) + ((int)blockIdx.x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,48)
              p0.shared = ...
        for i_c.3 (0,32)
          T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1540	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[2]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    vectorize ax0@ax1@ax2@.1 (0,3)
      p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,3)
      T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1541	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
p1_shared[767]));
  }
  for (int j_inner = 0; j_inner < 768; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 768) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,256)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,768)
      T_batch_matmul_NN.local = ...
  for j.3 (0,768)
    T_batch_matmul_NN = ...

==================================================
No: 1542	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,11)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.4 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1543	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1544	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
or (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 4)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.4 (0,4)
        T_batch_matmul_NN.local = ...
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1545	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner < 16; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,6)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for i_c.4 (0,16)
      T_batch_matmul_NN.local = ...
  for i.3 (0,16)
    T_batch_matmul_NN = ...

==================================================
No: 1546	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner < 32; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,11)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for i_c.3 (0,32)
      T_batch_matmul_NN.local = ...
  for i.3 (0,32)
    T_batch_matmul_NN = ...

==================================================
No: 1547	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t j_inner = 0; j_inner < 384; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 384)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,128)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,384)
        T_batch_matmul_NN.local = ...
    for j.3 (0,384)
      T_batch_matmul_NN = ...

==================================================
No: 1548	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
p1_shared[767]));
  }
  for (int j_inner = 0; j_inner < 768; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 768) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,768)
      p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,768)
      T_batch_matmul_NN.local = ...
  for j.3 (0,768)
    T_batch_matmul_NN = ...

==================================================
No: 1549	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 48)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,11)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for i_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1550	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 6144)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1551	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 12288)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for i_c.4 (0,16)
        T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1552	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1553	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t j_inner = 0; j_inner < 384; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 384)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,384)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,384)
        T_batch_matmul_NN.local = ...
    for j.3 (0,384)
      T_batch_matmul_NN = ...

==================================================
No: 1554	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[2]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,2)
      vectorize ax0@ax1@ax2@.1 (0,2)
        p1.shared = ...
    p0.shared = ...
    for j_c.4 (0,3)
      T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1555	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * p1_shared[11]));
  }
  for (int j_inner = 0; j_inner < 12; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 12) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,8192)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,12)
      p1.shared = ...
    p0.shared = ...
    for j_c.4 (0,12)
      T_batch_matmul_NN.local = ...
  for j.3 (0,12)
    T_batch_matmul_NN = ...

==================================================
No: 1556	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 64; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 49152) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.08, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
      for i_c.3 (0,64)
        T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1557	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l[i_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 3072)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for i_c.3 (0,4)
        T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1558	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1559	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
N_local[(i_inner + 48)];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 6144) + (i_inner * 768)) + ((int)blockIdx.x)) + 86016)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,8)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,11)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,6)
              p0.shared = ...
        for i_c.4 (0,8)
          T_batch_matmul_NN.local = ...
      for i.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1560	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1561	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1562	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647493.68)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1563	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1564	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1565	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x.x)] * p1_shared[0]));
  }
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)threadIdx.x) * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.28, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1566	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner + 4)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 4608)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,4)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1567	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 64; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 49152) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.4 (0,64)
        T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1568	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,768)
      p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1569	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l[i_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 3072)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      for i_c.4 (0,4)
        T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1570	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) % 768))] = T_batch_matmul_NN_local[0];
  T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (((int)blockIdx.x) % 768)) + 768)] = T_batch_matmul_NN_local[1];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  vthread b.1@i.1@j.1@ (0,2)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        p0.shared = ...
      T_batch_matmul_NN.local = ...
    T_batch_matmul_NN = ...

==================================================
No: 1571	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 48)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for i_c.3 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1572	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 24)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 21504)] = T_batch_matmul_NN_local[(i_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        p0.shared = ...
      for i_c.4 (0,4)
        T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1573	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 12288)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.3 (0,16)
        T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1574	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 48)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.3 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1575	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
  T_batch_matmul_NN[(((((((int)blockIdx.x) / 384) * 1536) + (((int)threadIdx.x) * 768)) + ((((int)blockIdx.x) % 384) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for j.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1576	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,384)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,48)
            p0.shared = ...
      for k.1 (0,2)
        for i_c.3 (0,32)
          T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1577	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,2)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,4)
      p0.shared = ...
    for k.1 (0,2)
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    T_batch_matmul_NN = ...

==================================================
No: 1578	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,384)
    vectorize ax0@ax1@ax2@.1 (0,2)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,2)
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
    for k.1 (0,2)
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    T_batch_matmul_NN = ...

==================================================
No: 1579	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[(((((((int)blockIdx.x) / 192) * 1536) + (i_inner * 768)) + ((((int)blockIdx.x) % 192) * 4)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,4)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for i_c.3 (0,2)
      for j_c.3 (0,4)
        T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1580	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[1] * p1_shared[5]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,6)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      for j_c.3 (0,3)
        T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1581	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[1] * p1_shared[7]));
  }
  for (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 4) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,8)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,2)
      p0.shared = ...
    for j_c.3 (0,4)
      for k.2 (0,2)
        T_batch_matmul_NN.local = ...
  for j.3 (0,4)
    T_batch_matmul_NN = ...

==================================================
No: 1582	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[1] * p1_shared[7]));
  }
  for (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 4) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  T_batch_matmul_NN.local auto_unroll: 16
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,8)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      for j_c.3 (0,4)
        T_batch_matmul_NN.local = ...
  for j.3 (0,4)
    T_batch_matmul_NN = ...

==================================================
No: 1583	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 j_inner = 0; j_inner < 768; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,64)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,384)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,768)
        T_batch_matmul_NN.local = ...
    for j.3 (0,768)
      T_batch_matmul_NN = ...

==================================================
No: 1584	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + j_inner) + 8)] = T_batch_matmul_NN_local[(j_inner + 8)];
    T_batch_matmul_NN[(((((int)blockIdx.x) * 24) + j_inner) + 16)] = T_batch_matmul_NN_local[(j_inner + 16)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,3)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
      for k.1 (0,2)
        for j_c.4 (0,8)
          T_batch_matmul_NN.local = ...
    for j.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1585	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 384) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + ((((int)blockIdx.x) % 384) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.4 (0,2)
        for j_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      for j.3 (0,2)
        T_batch_matmul_NN = ...

==================================================
No: 1586	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[1] * p1_shared[5]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,6)
      p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      for j_c.3 (0,3)
        T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1587	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
mul_NN[(((((((int)blockIdx.x) / 192) * 1536) + (i_inner * 768)) + ((((int)blockIdx.x) % 192) * 4)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 4) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    vectorize ax0@ax1@ax2@.1 (0,4)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for i_c.3 (0,2)
      for j_c.3 (0,4)
        T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1588	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + j_inner) + 8)] = T_batch_matmul_NN_local[(j_inner + 8)];
    T_batch_matmul_NN[(((((int)blockIdx.x) * 24) + j_inner) + 16)] = T_batch_matmul_NN_local[(j_inner + 16)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,3)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,384)
      for ax0@ax1@ax2@.0.0 (0,48)
        p1.shared = ...
      vectorize ax0@ax1@ax2@.1 (0,2)
        p0.shared = ...
      for k.1 (0,2)
        for j_c.4 (0,8)
          T_batch_matmul_NN.local = ...
    for j.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1589	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
matmul_NN_local[(j_inner + 8)];
    T_batch_matmul_NN[((((((int)blockIdx.x) * 48) + (((int)threadIdx.x) * 8)) + j_inner) + 32)] = T_batch_matmul_NN_local[(j_inner + 16)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,2048)
  vthread b.1@i.1@j.1@ (0,3)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,384)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
        for k.2 (0,2)
          for j_c.4 (0,8)
            T_batch_matmul_NN.local = ...
      for j.3 (0,8)
        T_batch_matmul_NN = ...

==================================================
No: 1590	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NN[(((((((int)blockIdx.x) >> 8) * 1536) + (i_inner * 768)) + ((((int)blockIdx.x) & 255) * 3)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 3) + j_inner)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    vectorize ax0@ax1@ax2@.1 (0,3)
      p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for i_c.3 (0,2)
      for j_c.4 (0,3)
        T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    for j.3 (0,3)
      T_batch_matmul_NN = ...

==================================================
No: 1591	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[1] * p1_shared[7]));
  }
  for (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 4) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  T_batch_matmul_NN.local auto_unroll: 16
  for k.0 (0,384)
    for ax0@ax1@ax2@.0.0 (0,3)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for k.1 (0,2)
      for j_c.3 (0,4)
        T_batch_matmul_NN.local = ...
  for j.3 (0,4)
    T_batch_matmul_NN = ...

==================================================
No: 1592	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
hreadIdx.x) >> 1) * 6144) + (i_inner * 768)) + ((((int)threadIdx.x) & 1) * 192)) + j_inner) + 384)] = T_batch_matmul_NN_local[(((i_inner * 192) + j_inner) + 1536)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.44, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,2)
  threadIdx.x b.2@i.2@j.2@ (0,32)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,128)
      for ax0@ax1@ax2@.0.0 (0,144)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,24)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,32)
          p0.shared = ...
      for i_c.3 (0,8)
        for j_c.3 (0,6)
          for k.2 (0,6)
            for j_c.4 (0,32)
              T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      for j.3 (0,192)
        T_batch_matmul_NN = ...

==================================================
No: 1593	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
eadIdx.x) / 24) * 12288) + (i_inner * 768)) + ((((int)threadIdx.x) % 24) * 16)) + j_inner) + 74112)] = T_batch_matmul_NN_local[(((i_inner * 16) + j_inner) + 1792)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
vthread b.1@i.1@j.1@ (0,8)
  threadIdx.x b.2@i.2@j.2@ (0,48)
    for k.0 (0,64)
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,48)
          p0.shared = ...
      for k.1 (0,6)
        for i_c.3 (0,16)
          for j_c.3 (0,8)
            for k.2 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      for j.3 (0,16)
        T_batch_matmul_NN = ...

==================================================
No: 1594	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + ((((int)threadIdx.x) >> 4) * 768)) + ((((int)blockIdx.x) % 3) * 256)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 192)] = T_batch_matmul_NN_local[(j_inner + 12)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647496.34)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,4)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,128)
        for ax0@ax1@ax2@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
          p0.shared = ...
        for k.1 (0,3)
          for j_c.3 (0,2)
            for k.2 (0,2)
              for j_c.4 (0,2)
                T_batch_matmul_NN.local = ...
      for j.3 (0,4)
        T_batch_matmul_NN = ...

Time elapsed for measurement: 5.19 s
Get 58 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1595	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1596	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1597	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647502.93)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1598	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1599	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647502.93)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1600	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1601	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1602	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1603	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1604	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1605	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1606	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1607	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1608	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1609	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 32) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 24576) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1610	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1611	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1612	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1613	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1614	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1615	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1616	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1617	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1618	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1619	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 4) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 3072) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1620	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1621	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1622	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1623	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1624	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1625	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647502.93)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1626	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647502.93)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1627	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.29, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1628	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 8) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 6144) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1629	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1630	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1631	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1632	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1633	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1634	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1635	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1636	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1637	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1638	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1639	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1640	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1641	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1642	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.17, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,4)
  threadIdx.x ax0@ax1@ax2@.1 (0,32)
    T_divide = ...

==================================================
No: 1643	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

==================================================
No: 1644	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 64) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 49152) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 16
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1645	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,16)
  threadIdx.x ax0@ax1@ax2@.1 (0,8)
    T_divide = ...

==================================================
No: 1646	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 1024
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1647	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,32)
  threadIdx.x ax0@ax1@ax2@.1 (0,4)
    T_divide = ...

==================================================
No: 1648	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 64
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1649	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
int)blockIdx.x)] = 0.000000e+00f;
  for (int k2 = 0; k2 < 768; ++k2) {
    p0_red[((int)blockIdx.x)] = (p0_red[((int)blockIdx.x)] + p0[((((int)blockIdx.x) * 768) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.01, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  for k2 (0,768)
    p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,128)
  T_divide = ...

==================================================
No: 1650	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    T_divide = ...

==================================================
No: 1651	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
blockIdx.x) * 2) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 1536) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
blockIdx.x ax0@ax1@ax2@.0 (0,64)
  threadIdx.x ax0@ax1@ax2@.1 (0,2)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    T_divide = ...

==================================================
No: 1652	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ckIdx.x) * 16) + ((int)threadIdx.x))] = (p0_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + p0[(((((int)blockIdx.x) * 12288) + (((int)threadIdx.x) * 768)) + k2)]);
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647505.39)
==================================================
Placeholder: p0
p0_red auto_unroll: 512
blockIdx.x ax0@ax1@ax2@.0 (0,8)
  threadIdx.x ax0@ax1@ax2@.1 (0,16)
    for k2 (0,768)
      p0_red = ...
blockIdx.x ax0@ax1@ax2@.0 (0,2)
  threadIdx.x ax0@ax1@ax2@.1 (0,64)
    T_divide = ...

Time elapsed for measurement: 4.96 s
Get 64 programs to measure:
.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1653	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ads();
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[0] * p1_shared[0]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    p0.shared = ...
    T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1654	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
ads();
    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (p0_shared[0] * p1_shared[0]));
  }
  T_batch_matmul_NN[((int)blockIdx.x)] = T_batch_matmul_NN_local[0];
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,98304)
  T_batch_matmul_NN.local auto_unroll: 16
  for k.0 (0,768)
    p1.shared = ...
    p0.shared = ...
    T_batch_matmul_NN.local = ...
  T_batch_matmul_NN = ...

==================================================
No: 1655	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t j_inner = 0; j_inner < 384; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 384)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.43, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,192)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,384)
        T_batch_matmul_NN.local = ...
    for j.3 (0,384)
      T_batch_matmul_NN = ...

==================================================
No: 1656	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for i_c.4 (0,2)
      T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    T_batch_matmul_NN = ...

==================================================
No: 1657	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 12288)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,6)
            p0.shared = ...
      for i_c.3 (0,16)
        T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1658	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
p1_shared[767]));
  }
  for (int j_inner = 0; j_inner < 768; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 768) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.50, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,768)
      p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,24)
      for j_c.4 (0,32)
        T_batch_matmul_NN.local = ...
  for j.3 (0,768)
    T_batch_matmul_NN = ...

==================================================
No: 1659	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1660	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,24)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1661	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 16; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 12288) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for i_c.3 (0,8)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1662	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
or (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 6) + (((int)threadIdx.x) * 3)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,3)
        T_batch_matmul_NN.local = ...
    for j.3 (0,3)
      T_batch_matmul_NN = ...

==================================================
No: 1663	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,24)
            p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1664	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt i_inner = 0; i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 1536) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1665	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[((((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 3072)) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 6144)] = T_batch_matmul_NN_local[(i_inner + 4)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for i_c.4 (0,4)
          T_batch_matmul_NN.local = ...
      for i.3 (0,4)
        T_batch_matmul_NN = ...

==================================================
No: 1666	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 64; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 49152) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.4 (0,64)
        T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1667	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 64)];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)blockIdx.x)) + 73728)] = T_batch_matmul_NN_local[(i_inner + 96)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1668	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 4; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 3072) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.19, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,768)
    p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,4)
      p0.shared = ...
    for i_c.4 (0,4)
      T_batch_matmul_NN.local = ...
  for i.3 (0,4)
    T_batch_matmul_NN = ...

==================================================
No: 1669	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 64; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 49152) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for i_c.4 (0,64)
        T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1670	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 8; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,2)
      vectorize ax0@ax1@ax2@.1 (0,4)
        p0.shared = ...
    for i_c.3 (0,8)
      T_batch_matmul_NN.local = ...
  for i.3 (0,8)
    T_batch_matmul_NN = ...

==================================================
No: 1671	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner + 4)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 4608)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1672	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
) + j_inner) + 8)] = T_batch_matmul_NN_local[(j_inner + 8)];
    T_batch_matmul_NN[(((((int)blockIdx.x) * 24) + j_inner) + 16)] = T_batch_matmul_NN_local[(j_inner + 16)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,4096)
  vthread b.1@i.1@j.1@ (0,3)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,24)
        p1.shared = ...
      p0.shared = ...
      for j_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for j.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1673	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)blockIdx.x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 64)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p0.shared = ...
      for i_c.3 (0,64)
        T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1674	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + 1)]));
    }
  }
  for (int j_inner = 0; j_inner < 768; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 768) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.09, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,256)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,384)
      for j_c.4 (0,2)
        T_batch_matmul_NN.local = ...
  for j.3 (0,768)
    T_batch_matmul_NN = ...

==================================================
No: 1675	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 64; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 49152) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,3)
            p0.shared = ...
      for i_c.4 (0,64)
        T_batch_matmul_NN.local = ...
    for i.3 (0,64)
      T_batch_matmul_NN = ...

==================================================
No: 1676	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
l[i_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 1536)] = T_batch_matmul_NN_local[(i_inner + 2)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,4)
        p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1677	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner < 16; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 12288) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,6)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p0.shared = ...
    for i_c.4 (0,16)
      T_batch_matmul_NN.local = ...
  for i.3 (0,16)
    T_batch_matmul_NN = ...

==================================================
No: 1678	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nner + 4)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 6144) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 4608)] = T_batch_matmul_NN_local[(i_inner + 6)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1679	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
nt i_inner = 0; i_inner < 8; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 6144) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,16)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for i_c.3 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1680	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 64)];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)blockIdx.x)) + 73728)] = T_batch_matmul_NN_local[(i_inner + 96)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.06, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,128)
        p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1681	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 48)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,24)
          p0.shared = ...
      for i_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1682	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,22)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1683	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (((int)threadIdx.x) * 6144)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        vectorize ax0@ax1@ax2@.1 (0,12)
          p0.shared = ...
      for i_c.3 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1684	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
2) + 1)]));
    }
  }
  for (int j_inner = 0; j_inner < 768; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 768) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.07, Tstamp:1688647525.53)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,192)
      vectorize ax0@ax1@ax2@.1 (0,4)
        p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,384)
      for j_c.4 (0,2)
        T_batch_matmul_NN.local = ...
  for j.3 (0,768)
    T_batch_matmul_NN = ...

.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E.E
==================================================
No: 1685	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 48)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 43008)] = T_batch_matmul_NN_local[(i_inner + 56)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,64)
        p0.shared = ...
      for i_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1686	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.32, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1687	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
or (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 4)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.4 (0,4)
        T_batch_matmul_NN.local = ...
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1688	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 64)];
    T_batch_matmul_NN[(((i_inner * 768) + ((int)blockIdx.x)) + 73728)] = T_batch_matmul_NN_local[(i_inner + 96)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,16)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1689	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 + 24)];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 21504)] = T_batch_matmul_NN_local[(i_inner + 28)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.25, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,3072)
  vthread b.1@i.1@j.1@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,6)
        vectorize ax0@ax1@ax2@.1 (0,6)
          p0.shared = ...
      for i_c.4 (0,4)
        T_batch_matmul_NN.local = ...
    for i.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1690	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
* p1_shared[0]));
  }
  for (int i_inner = 0; i_inner < 128; ++i_inner) {
    T_batch_matmul_NN[((i_inner * 768) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.27, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,3)
      vectorize ax0@ax1@ax2@.1 (0,48)
        p0.shared = ...
    for i_c.3 (0,128)
      T_batch_matmul_NN.local = ...
  for i.3 (0,128)
    T_batch_matmul_NN = ...

==================================================
No: 1691	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NN_local[i_inner];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 24576) + (i_inner * 768)) + ((int)blockIdx.x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 1024
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,48)
              p0.shared = ...
        for i_c.3 (0,32)
          T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1692	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[3]));
  }
  for (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 4) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  T_batch_matmul_NN.local auto_unroll: 16
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,2)
      vectorize ax0@ax1@ax2@.1 (0,3)
        p1.shared = ...
    p0.shared = ...
    for j_c.3 (0,4)
      T_batch_matmul_NN.local = ...
  for j.3 (0,4)
    T_batch_matmul_NN = ...

==================================================
No: 1693	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 3)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.26, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,48)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p0.shared = ...
      for j_c.4 (0,3)
        T_batch_matmul_NN.local = ...
    for j.3 (0,3)
      T_batch_matmul_NN = ...

==================================================
No: 1694	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 3)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.24, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,512)
  threadIdx.x b.2@i.2@j.2@ (0,64)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
        p0.shared = ...
      for j_c.4 (0,3)
        T_batch_matmul_NN.local = ...
    for j.3 (0,3)
      T_batch_matmul_NN = ...

==================================================
No: 1695	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.22, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1696	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
or (int j_inner = 0; j_inner < 4; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 4)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,12288)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 16
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.4 (0,4)
        T_batch_matmul_NN.local = ...
    for j.3 (0,4)
      T_batch_matmul_NN = ...

==================================================
No: 1697	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
tmul_NN_local[i_inner];
    T_batch_matmul_NN[((((((int)threadIdx.x) * 24576) + (i_inner * 768)) + ((int)blockIdx.x)) + 49152)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.23, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,2)
      T_batch_matmul_NN.local auto_unroll: 64
      for k.0 (0,768)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,22)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
            vectorize ax0@ax1@ax2@.1 (0,3)
              p0.shared = ...
        for i_c.3 (0,32)
          T_batch_matmul_NN.local = ...
      for i.3 (0,32)
        T_batch_matmul_NN = ...

==================================================
No: 1698	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.18, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  T_batch_matmul_NN.local auto_unroll: 64
  for k.0 (0,768)
    p1.shared = ...
    vectorize ax0@ax1@ax2@.1 (0,2)
      p0.shared = ...
    for i_c.4 (0,2)
      T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    T_batch_matmul_NN = ...

==================================================
No: 1699	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.20, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1700	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
t j_inner = 0; j_inner < 384; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 768) + (((int)threadIdx.x) * 384)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.30, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,128)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,96)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          vectorize ax0@ax1@ax2@.1 (0,4)
            p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.3 (0,384)
        T_batch_matmul_NN.local = ...
    for j.3 (0,384)
      T_batch_matmul_NN = ...

==================================================
No: 1701	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 * p1_shared[63]));
  }
  for (int j_inner = 0; j_inner < 64; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 64) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  T_batch_matmul_NN.local auto_unroll: 512
  for k.0 (0,768)
    for ax0@ax1@ax2@.0.0 (0,3)
      vectorize ax0@ax1@ax2@.1 (0,24)
        p1.shared = ...
    p0.shared = ...
    for j_c.4 (0,64)
      T_batch_matmul_NN.local = ...
  for j.3 (0,64)
    T_batch_matmul_NN = ...

==================================================
No: 1702	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner < 2; ++i_inner) {
    T_batch_matmul_NN[((((((int)blockIdx.x) / 768) * 1536) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,49152)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    p1.shared = ...
    for ax0@ax1@ax2@.0.0 (0,2)
      p0.shared = ...
    for i_c.4 (0,2)
      T_batch_matmul_NN.local = ...
  for i.3 (0,2)
    T_batch_matmul_NN = ...

==================================================
No: 1703	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1704	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 32; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 24576) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,4)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,16)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,4)
          vectorize ax0@ax1@ax2@.1 (0,2)
            p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1705	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for i_c.4 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1706	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1707	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
atch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (((int)threadIdx.x) * 24576)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647528.07)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,32)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for i_c.4 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1708	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
[0] * p1_shared[2]));
  }
  for (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.02, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,32768)
  T_batch_matmul_NN.local auto_unroll: 1024
  for k.0 (0,768)
    vectorize ax0@ax1@ax2@.1 (0,3)
      p1.shared = ...
    p0.shared = ...
    for j_c.4 (0,3)
      T_batch_matmul_NN.local = ...
  for j.3 (0,3)
    T_batch_matmul_NN = ...

==================================================
No: 1709	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 6144)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p0.shared = ...
      for i_c.4 (0,8)
        T_batch_matmul_NN.local = ...
    for i.3 (0,8)
      T_batch_matmul_NN = ...

==================================================
No: 1710	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
or (int j_inner = 0; j_inner < 3; ++j_inner) {
    T_batch_matmul_NN[(((((int)blockIdx.x) * 6) + (((int)threadIdx.x) * 3)) + j_inner)] = T_batch_matmul_NN_local[j_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,16384)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      for ax0@ax1@ax2@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
          p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p0.shared = ...
      for j_c.4 (0,3)
        T_batch_matmul_NN.local = ...
    for j.3 (0,3)
      T_batch_matmul_NN = ...

==================================================
No: 1711	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 3072) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,24576)
  threadIdx.x b.2@i.2@j.2@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 512
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,2)
        vectorize ax0@ax1@ax2@.1 (0,4)
          p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

==================================================
No: 1712	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
 i_inner = 0; i_inner < 16; ++i_inner) {
    T_batch_matmul_NN[(((((int)threadIdx.x) * 12288) + (i_inner * 768)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.04, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,768)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
          vectorize ax0@ax1@ax2@.1 (0,12)
            p0.shared = ...
      for i_c.3 (0,8)
        for i_c.4 (0,2)
          T_batch_matmul_NN.local = ...
    for i.3 (0,16)
      T_batch_matmul_NN = ...

==================================================
No: 1713	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_inner];
    T_batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 49152) + (i_inner * 768)) + (((int)blockIdx.x) % 768)) + 24576)] = T_batch_matmul_NN_local[(i_inner + 32)];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.05, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,1536)
  vthread b.1@i.1@j.1@ (0,2)
    T_batch_matmul_NN.local auto_unroll: 64
    for k.0 (0,768)
      p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,3)
        vectorize ax0@ax1@ax2@.1 (0,24)
          p0.shared = ...
      for i_c.3 (0,32)
        T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1714	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
_batch_matmul_NN[(((((((int)threadIdx.x) >> 7) * 24576) + (i_inner * 768)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.x) & 127))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.31, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6)
  threadIdx.x b.2@i.2@j.2@ (0,512)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,24)
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p1.shared = ...
      for ax0@ax1@ax2@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@.0.1 (0,512)
          p0.shared = ...
      for k.1 (0,32)
        for i_c.3 (0,16)
          for i_c.4 (0,2)
            T_batch_matmul_NN.local = ...
    for i.3 (0,32)
      T_batch_matmul_NN = ...

==================================================
No: 1715	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
(i_inner * 768)) + ((((int)blockIdx.x) % 6) * 128)) + ((((int)threadIdx.x) & 15) * 4)) + j_inner) + 64)] = T_batch_matmul_NN_local[(((i_inner * 4) + j_inner) + 8)];
    }
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.21, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,96)
  vthread b.1@i.1@j.1@ (0,2)
    threadIdx.x b.2@i.2@j.2@ (0,64)
      T_batch_matmul_NN.local auto_unroll: 512
      for k.0 (0,32)
        for ax0@ax1@ax2@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p1.shared = ...
        for ax0@ax1@ax2@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@.0.1 (0,64)
            p0.shared = ...
        for k.1 (0,6)
          for i_c.3 (0,2)
            for j_c.3 (0,4)
              for k.2 (0,4)
                T_batch_matmul_NN.local = ...
      for i.3 (0,2)
        for j.3 (0,4)
          T_batch_matmul_NN = ...

==================================================
No: 1716	GFLOPS: 0.00 / 0.00	results: MeasureResult(error_type:CompileHostError, error_msg:Traceback (most recent call last):
  File "/home/canesche/git/tvm/python/tvm/auto_scheduler/measure.py", line 633, in _local_build_worker
    func = build_module.build(sch, args, target=task.target)
  File "/home/canesche/git/tvm/python/tvm/driver/build_mo
...
batch_matmul_NN[(((((((int)blockIdx.x) / 768) * 12288) + (((int)threadIdx.x) * 1536)) + (i_inner * 768)) + (((int)blockIdx.x) % 768))] = T_batch_matmul_NN_local[i_inner];
  }
}


Compilation error:
nvcc fatal   : Unsupported gpu architecture 'compute_86'

, all_cost:0.03, Tstamp:1688647528.08)
==================================================
Placeholder: p0, p1
blockIdx.x b.0@i.0@j.0@ (0,6144)
  threadIdx.x b.2@i.2@j.2@ (0,8)
    T_batch_matmul_NN.local auto_unroll: 1024
    for k.0 (0,768)
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        p1.shared = ...
      threadIdx.x ax0@ax1@ax2@.0.1 (0,8)
        vectorize ax0@ax1@ax2@.1 (0,3)
          p0.shared = ...
      for i_c.3 (0,2)
        T_batch_matmul_NN.local = ...
    for i.3 (0,2)
      T_batch_matmul_NN = ...

Time elapsed for measurement: 5.07 s
Get 0 programs to measure:
Time elapsed for measurement: 0.00 s
Get 64 programs to measure:
.E